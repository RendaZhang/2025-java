<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [Stage 2 Week 5 Day 2 - K8s åŸºç¡€å¯¹è±¡ï¼ˆNS/SA/Config/Secret/Deployment/Serviceï¼‰](#stage-2-week-5-day-2---k8s-%E5%9F%BA%E7%A1%80%E5%AF%B9%E8%B1%A1nssaconfigsecretdeploymentservice)
  - [ç›®æ ‡](#%E7%9B%AE%E6%A0%87)
  - [Step 1/5 - è§„èŒƒåŒ– `k8s/base`ï¼ˆå¹¶ä»¥ digest é”å®šé•œåƒï¼‰](#step-15---%E8%A7%84%E8%8C%83%E5%8C%96-k8sbase%E5%B9%B6%E4%BB%A5-digest-%E9%94%81%E5%AE%9A%E9%95%9C%E5%83%8F)
    - [ç›®å½•ä¸å˜é‡](#%E7%9B%AE%E5%BD%95%E4%B8%8E%E5%8F%98%E9%87%8F)
    - [Namespace + ServiceAccountï¼ˆé¢„ç•™ IRSA æ³¨è§£ä½ï¼‰](#namespace--serviceaccount%E9%A2%84%E7%95%99-irsa-%E6%B3%A8%E8%A7%A3%E4%BD%8D)
    - [ConfigMapï¼ˆæŒ‰éœ€ç»™åº”ç”¨æ³¨å…¥éæ•æ„Ÿé…ç½®ï¼‰](#configmap%E6%8C%89%E9%9C%80%E7%BB%99%E5%BA%94%E7%94%A8%E6%B3%A8%E5%85%A5%E9%9D%9E%E6%95%8F%E6%84%9F%E9%85%8D%E7%BD%AE)
    - [Deployment + Serviceï¼ˆä»¥ **digest** é”é•œåƒï¼‰](#deployment--service%E4%BB%A5-digest-%E9%94%81%E9%95%9C%E5%83%8F)
    - [æ¸²æŸ“æ¨¡æ¿å’Œåº”ç”¨](#%E6%B8%B2%E6%9F%93%E6%A8%A1%E6%9D%BF%E5%92%8C%E5%BA%94%E7%94%A8)
      - [æ¸²æŸ“æ¨¡æ¿](#%E6%B8%B2%E6%9F%93%E6%A8%A1%E6%9D%BF)
      - [åº”ç”¨å¹¶è¦†ç›–æ˜¨å¤©çš„ä¸´æ—¶èµ„æº](#%E5%BA%94%E7%94%A8%E5%B9%B6%E8%A6%86%E7%9B%96%E6%98%A8%E5%A4%A9%E7%9A%84%E4%B8%B4%E6%97%B6%E8%B5%84%E6%BA%90)
    - [å¿«é€Ÿå†’çƒŸï¼ˆé›†ç¾¤å†…ï¼‰](#%E5%BF%AB%E9%80%9F%E5%86%92%E7%83%9F%E9%9B%86%E7%BE%A4%E5%86%85)
  - [Step 2/5 - IRSA ç”¨ Terraformï¼ŒALB Controller ç”¨ Helmï¼ˆè¿› `post-recreate.sh`ï¼‰](#step-25---irsa-%E7%94%A8-terraformalb-controller-%E7%94%A8-helm%E8%BF%9B-post-recreatesh)
    - [Terraformï¼šåˆ›å»º IRSAï¼ˆRole + Policy + SA æ³¨è§£ï¼‰](#terraform%E5%88%9B%E5%BB%BA-irsarole--policy--sa-%E6%B3%A8%E8%A7%A3)
      - [å‡†å¤‡å®˜æ–¹ç­–ç•¥ï¼ˆæ”¾æ–‡ä»¶ï¼Œä¾¿äºä»¥åå‡çº§æ›¿æ¢ï¼‰](#%E5%87%86%E5%A4%87%E5%AE%98%E6%96%B9%E7%AD%96%E7%95%A5%E6%94%BE%E6%96%87%E4%BB%B6%E4%BE%BF%E4%BA%8E%E4%BB%A5%E5%90%8E%E5%8D%87%E7%BA%A7%E6%9B%BF%E6%8D%A2)
      - [HCL ä»£ç ï¼ˆIRSA + SA æ³¨è§£ï¼‰](#hcl-%E4%BB%A3%E7%A0%81irsa--sa-%E6%B3%A8%E8%A7%A3)
      - [ä½¿ç”¨ Terraform æ‰§è¡Œå˜æ›´ï¼š](#%E4%BD%BF%E7%94%A8-terraform-%E6%89%A7%E8%A1%8C%E5%8F%98%E6%9B%B4)
    - [Helm å®‰è£…/å‡çº§ + CRDsï¼šæ›´æ–° `post-recreate.sh`](#helm-%E5%AE%89%E8%A3%85%E5%8D%87%E7%BA%A7--crds%E6%9B%B4%E6%96%B0-post-recreatesh)
    - [éªŒè¯](#%E9%AA%8C%E8%AF%81)
  - [Step 3/5 â€” åˆ›å»º Ingressï¼ˆç”Ÿæˆ ALBï¼‰+ å…¬ç½‘éªŒè¯ + å†™å…¥ `post-recreate.sh`](#step-35--%E5%88%9B%E5%BB%BA-ingress%E7%94%9F%E6%88%90-alb-%E5%85%AC%E7%BD%91%E9%AA%8C%E8%AF%81--%E5%86%99%E5%85%A5-post-recreatesh)
    - [é¢„æ£€æŸ¥ï¼ˆå­ç½‘æ ‡ç­¾æ˜¯å¦ OKï¼‰](#%E9%A2%84%E6%A3%80%E6%9F%A5%E5%AD%90%E7%BD%91%E6%A0%87%E7%AD%BE%E6%98%AF%E5%90%A6-ok)
    - [å†™ Ingress æ¸…å•](#%E5%86%99-ingress-%E6%B8%85%E5%8D%95)
    - [ç­‰å¾… ALB åˆ†é…åœ°å€å¹¶å†’çƒŸéªŒè¯](#%E7%AD%89%E5%BE%85-alb-%E5%88%86%E9%85%8D%E5%9C%B0%E5%9D%80%E5%B9%B6%E5%86%92%E7%83%9F%E9%AA%8C%E8%AF%81)
    - [æŠŠ Ingress å‘å¸ƒä¸ç­‰å¾…å†™è¿› `post-recreate.sh`](#%E6%8A%8A-ingress-%E5%8F%91%E5%B8%83%E4%B8%8E%E7%AD%89%E5%BE%85%E5%86%99%E8%BF%9B-post-recreatesh)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

# Stage 2 Week 5 Day 2 - K8s åŸºç¡€å¯¹è±¡ï¼ˆNS/SA/Config/Secret/Deployment/Serviceï¼‰

## ç›®æ ‡

1. **å¤¯å® K8s åŸºç¡€å¯¹è±¡**
   - æŠŠåº”ç”¨åœ¨é›†ç¾¤å†…â€œç«™ç¨³â€çš„æœ€å°é›†åˆåšè§„èŒƒåŒ–ä¸è½åº“ï¼š**Namespace / ServiceAccountï¼ˆé¢„ç•™ IRSA æ³¨è§£ä½ï¼‰/ ConfigMap / Deployment / Service**ï¼Œ
   - å¹¶æŠŠ**readiness/liveness** æ¢é’ˆæ¥åˆ° Actuator å¥åº·æ£€æŸ¥ï¼›æ¸…å•è½å…¥ä»“åº“ `k8s/base/`ï¼Œä¾¿äºè„šæœ¬è‡ªåŠ¨åº”ç”¨ã€‚
2. **å¯¹å¤–æš´éœ²ä¸å¼¹æ€§ï¼ˆDay 3ï¼‰**
   - å®‰è£… **AWS Load Balancer Controller**ï¼Œç¼–å†™ **Ingress** ç”Ÿæˆå…¬ç½‘ **ALB**ï¼Œå¥åº·æ£€æŸ¥èµ° `/actuator/health/readiness`ï¼›
   - é¡ºæ‰‹åŠ ä¸€ä¸ª **HPAï¼ˆCPU=60%ï¼‰** åšæœ€å°æ‰©ç¼©æ¼”ç¤ºã€‚
3. **çº³å…¥æ¯æ—¥é‡å»º/é”€æ¯ä½“ç³»**
   - æ‰€æœ‰æ–°å¢å†…å®¹éƒ½è¦èƒ½éšç€ `make start-all / stop-all` å¾ªç¯é‡æ”¾ï¼š
     - Terraform ä»…è´Ÿè´£ IAM è§’è‰²ä¸ ServiceAccountï¼›
     - æ§åˆ¶å™¨æœ¬èº«é€šè¿‡ **post-recreate.sh** çš„ Helm å®‰è£…ã€‚

éªŒæ”¶æ¸…å•ï¼š

- `k8s/base/` ä¸‹**è§„èŒƒåŒ–**çš„ï¼š
  - `ns-sa.yaml`ã€`deploy-svc.yaml`ï¼ˆå«æ¢é’ˆ/é…ç½®æ³¨å…¥ï¼‰ï¼Œ
  - ä»¥åŠ `k8s/ingress.yaml`ã€`k8s/hpa.yaml`ã€‚
- **ALB DNS** å¯è®¿é—®ï¼ˆä¸»é¡µ/å¥åº·æ£€æŸ¥ï¼‰ä¸ä¸€æ¬¡ `kubectl describe hpa` è¾“å‡ºã€‚
- **post-recreate.sh**ï¼š
  - è¿½åŠ /è°ƒæ•´å®‰è£…ä¸å‘å¸ƒé€»è¾‘ï¼Œä½¿å®ƒèƒ½ **è‡ªåŠ¨æ›´æ–° Deployment çš„é•œåƒä¸º ECR digest**ã€ç­‰å¾… `rollout`ã€åœ¨é›†ç¾¤å†…åšå†’çƒŸï¼Œ
  - å¹¶å®‰è£…/å‡çº§ ALB Controllerã€‚
- æ–‡æ¡£ï¼šåœ¨ `äº‘åŸç”Ÿè®¡åˆ’` çš„å°èŠ‚è¡¥é½â€œåšä»€ä¹ˆ/å…³é”®å‘½ä»¤/äº§ç‰©â€ä¸é€€è·¯è®°å½•ã€‚

è¡¥å……ï¼š

- ä»Šæ—¥æ‰€æœ‰æ”¹åŠ¨åº”å½“ **å¯è¢«æ¯æ—¥é‡å»º** è‡ªåŠ¨å¤ç°ï¼ˆ`make start-all`ï¼‰ï¼Œå¹¶åœ¨æ¯æ—¥é”€æ¯æ—¶å®Œæ•´æ¸…ç†ï¼ˆ`make stop-all`ï¼‰ã€‚
- ç»§ç»­åšæŒ **ç”¨ ECR digest éƒ¨ç½²**ï¼Œé¿å… tag æ¼‚ç§»ï¼Œä½¿é‡å»ºåè¡Œä¸ºç¨³å®šã€ä¾¿äºå›æ»šã€‚

---

## Step 1/5 - è§„èŒƒåŒ– `k8s/base`ï¼ˆå¹¶ä»¥ digest é”å®šé•œåƒï¼‰

ç›®æ ‡ï¼šæŠŠæ˜¨å¤©çš„ä¸´æ—¶æ¸…å•é‡æ„ä¸ºå¯å¤ç”¨çš„ç›®å½•ç»“æ„ï¼Œå¹¶ç”¨ **ECR digest** å›ºå®šé•œåƒï¼Œä¾¿äºåœ¨ `post-recreate.sh` é‡Œä¸€é”®é‡æ”¾ã€‚

### ç›®å½•ä¸å˜é‡

```bash
WORK_DIR=/mnt/d/0Repositories/CloudNative
mkdir -p ${WORK_DIR}/task-api/k8s/base
cd ${WORK_DIR}/task-api

# ä½ çš„ç¯å¢ƒï¼ˆå¯ç›´æ¥å¤åˆ¶ï¼‰
export PROFILE=phase2-sso
export AWS_PROFILE=$PROFILE
export AWS_REGION=us-east-1
export CLUSTER=dev
export NS=svc-task
export APP=task-api
export ECR_REPO=task-api

# è´¦æˆ·ä¸é•œåƒ registry
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
export ACCOUNT_ID
export REMOTE="${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com/${ECR_REPO}"

# é•œåƒ digestï¼ˆä¼˜å…ˆç”¨æ˜¨å¤©è„šæœ¬æ–‡ä»¶ï¼Œå¦åˆ™å°±ç”¨ä½ æ˜¨å¤©ç»™æˆ‘çš„é‚£ä¸²ï¼‰
if [ -f scripts/.last_image ]; then source scripts/.last_image; fi
export DIGEST=${DIGEST:-sha256:927d20ca4cebedc14f81770e8e5e49259684723ba65b76e7c59f3003cc9a9741}
```

### Namespace + ServiceAccountï¼ˆé¢„ç•™ IRSA æ³¨è§£ä½ï¼‰

`k8s/base/ns-sa.yaml`

```yaml
apiVersion: v1
kind: Namespace
metadata:
  name: svc-task
  labels:
    app.kubernetes.io/part-of: task-platform
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: task-api
  namespace: svc-task
  labels:
    app.kubernetes.io/name: task-api
# é¢„ç•™ IRSA æ³¨è§£ä½ï¼ˆDay 4 å†å¡«ä¸Š role-arnï¼‰
#  annotations:
#    eks.amazonaws.com/role-arn: arn:aws:iam::<ACCOUNT_ID>:role/<ROLE_NAME>
```

> ç°åœ¨å…ˆä¸å¡« `role-arn`ï¼Œé¿å…æ— æ„ä¹‰çš„ 403ï¼›ç­‰æ˜å¤©åš IRSA æ—¶å†æ‰“å¼€ã€‚

### ConfigMapï¼ˆæŒ‰éœ€ç»™åº”ç”¨æ³¨å…¥éæ•æ„Ÿé…ç½®ï¼‰

`k8s/base/configmap.yaml`

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: task-api-config
  namespace: svc-task
data:
  APP_NAME: "task-api"
  WELCOME_MSG: "hello from ${AWS_REGION}"
```

### Deployment + Serviceï¼ˆä»¥ **digest** é”é•œåƒï¼‰

ä¸ºä¾¿äºè„šæœ¬æ³¨å…¥å˜é‡ï¼Œè¿™é‡Œå…ˆå†™æˆæ¨¡æ¿æ–‡ä»¶ï¼Œå†æ¸²æŸ“ã€‚

`k8s/base/deploy-svc.tmpl.yaml`

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ${APP}
  namespace: ${NS}
  labels: { app: ${APP} }
spec:
  replicas: 2
  revisionHistoryLimit: 3
  selector:
    matchLabels: { app: ${APP} }
  template:
    metadata:
      labels: { app: ${APP} }
    spec:
      serviceAccountName: ${APP}
      terminationGracePeriodSeconds: 20
      containers:
        - name: ${APP}
          image: ${REMOTE}@${DIGEST}
          imagePullPolicy: IfNotPresent
          ports:
            - name: http
              containerPort: 8080
          envFrom:
            - configMapRef:
                name: task-api-config
          resources:
            requests:
              cpu: "100m"
              memory: "128Mi"
            limits:
              cpu: "500m"
              memory: "512Mi"
          readinessProbe:
            httpGet: { path: /actuator/health/readiness, port: 8080 }
            initialDelaySeconds: 5
            periodSeconds: 10
            timeoutSeconds: 2
            failureThreshold: 3
          livenessProbe:
            httpGet: { path: /actuator/health/liveness, port: 8080 }
            initialDelaySeconds: 10
            periodSeconds: 10
            timeoutSeconds: 2
            failureThreshold: 3
---
apiVersion: v1
kind: Service
metadata:
  name: ${APP}
  namespace: ${NS}
  labels: { app: ${APP} }
spec:
  type: ClusterIP
  selector: { app: ${APP} }
  ports:
    - name: http
      port: 8080
      targetPort: 8080
```

### æ¸²æŸ“æ¨¡æ¿å’Œåº”ç”¨

#### æ¸²æŸ“æ¨¡æ¿

> macOS/Linux å¦‚æ—  `envsubst` ä¹Ÿå¯ç”¨ `sed`ï¼Œä¸¤ç§éƒ½ç»™å‡ºã€‚
> ä»»é€‰å…¶ä¸€æ‰§è¡Œå³å¯ã€‚

- **æ–¹å¼ Aï¼šenvsubst**
    ```bash
    cd ${WORK_DIR}/task-api
    envsubst < k8s/base/deploy-svc.tmpl.yaml > k8s/base/deploy-svc.yaml
    ```
- **æ–¹å¼ Bï¼šsed**
    ```bash
    cd ~/work/task-api
    sed -e "s|\${APP}|${APP}|g" \
        -e "s|\${NS}|${NS}|g" \
        -e "s|\${REMOTE}|${REMOTE}|g" \
        -e "s|\${DIGEST}|${DIGEST}|g" \
    k8s/base/deploy-svc.tmpl.yaml > k8s/base/deploy-svc.yaml
    ```

#### åº”ç”¨å¹¶è¦†ç›–æ˜¨å¤©çš„ä¸´æ—¶èµ„æº

é¢„è§ˆå·®å¼‚ï¼ˆå¯¹æ¯”æ˜¨å¤©ä½¿ç”¨ `k8s.yaml` éƒ¨ç½²çš„å·®å¼‚ï¼‰ï¼š

```bash
kubectl -n "$NS" diff -f k8s/base/ns-sa.yaml
# è¾“å‡ºï¼š
# diff -u -N /tmp/LIVE-2317581702/v1.Namespace..svc-task /tmp/MERGED-3486373042/v1.Namespace..svc-task
# --- /tmp/LIVE-2317581702/v1.Namespace..svc-task 2025-08-16 22:50:00.271411589 +0800
# +++ /tmp/MERGED-3486373042/v1.Namespace..svc-task       2025-08-16 22:50:00.271411589 +0800
# @@ -3,6 +3,7 @@
#  metadata:
#    creationTimestamp: "2025-08-16T14:39:42Z"
#    labels:
# +    app.kubernetes.io/part-of: task-platform
#      kubernetes.io/metadata.name: svc-task
#    name: svc-task
#    resourceVersion: "3314"
# diff -u -N /tmp/LIVE-2317581702/v1.ServiceAccount.svc-task.task-api /tmp/MERGED-3486373042/v1.ServiceAccount.svc-task.task-api
# --- /tmp/LIVE-2317581702/v1.ServiceAccount.svc-task.task-api    2025-08-16 22:50:01.131332891 +0800
# +++ /tmp/MERGED-3486373042/v1.ServiceAccount.svc-task.task-api  2025-08-16 22:50:01.131332891 +0800
# @@ -0,0 +1,9 @@
# +apiVersion: v1
# +kind: ServiceAccount
# +metadata:
# +  creationTimestamp: "2025-08-16T14:50:01Z"
# +  labels:
# +    app.kubernetes.io/name: task-api
# +  name: task-api
# +  namespace: svc-task
# +  uid: 19e1a327-2146-4208-a6b6-b31f2c542b55

kubectl -n "$NS" diff -f k8s/base/configmap.yaml
# è¾“å‡ºï¼š
# diff -u -N /tmp/LIVE-2707704724/v1.ConfigMap.svc-task.task-api-config /tmp/MERGED-3458038461/v1.ConfigMap.svc-task.task-api-config
# --- /tmp/LIVE-2707704724/v1.ConfigMap.svc-task.task-api-config  2025-08-16 22:51:38.948653919 +0800
# +++ /tmp/MERGED-3458038461/v1.ConfigMap.svc-task.task-api-config        2025-08-16 22:51:38.948653919 +0800
# @@ -0,0 +1,10 @@
# +apiVersion: v1
# +data:
# +  APP_NAME: task-api
# +  WELCOME_MSG: hello from ${AWS_REGION}
# +kind: ConfigMap
# +metadata:
# +  creationTimestamp: "2025-08-16T14:51:38Z"
# +  name: task-api-config
# +  namespace: svc-task
# +  uid: a8a278e6-894d-49f8-9ffb-f0cd8ed613a7

kubectl -n "$NS" diff -f k8s/base/deploy-svc.yaml
# è¾“å‡º
# diff -u -N /tmp/LIVE-173526345/apps.v1.Deployment.svc-task.task-api /tmp/MERGED-2415446233/apps.v1.Deployment.svc-task.task-api
# --- /tmp/LIVE-173526345/apps.v1.Deployment.svc-task.task-api    2025-08-16 22:51:57.986005507 +0800
# +++ /tmp/MERGED-2415446233/apps.v1.Deployment.svc-task.task-api 2025-08-16 22:51:57.986005507 +0800
# @@ -8,7 +8,9 @@
#      kubernetes.io/change-cause: kubectl set image deploy/task-api task-api=563149051155.dkr.ecr.us-east-1.amazonaws.com/task-api@sha256:927d20ca4cebedc14f81770e8e5e49259684723ba65b76e7c59f3003cc9a9741
#        --namespace=svc-task --record=true
#    creationTimestamp: "2025-08-16T14:40:33Z"
# -  generation: 2
# +  generation: 3
# +  labels:
# +    app: task-api
#    name: task-api
#    namespace: svc-task
#    resourceVersion: "3622"
# @@ -32,7 +34,10 @@
#          app: task-api
#      spec:
#        containers:
# -      - image: 563149051155.dkr.ecr.us-east-1.amazonaws.com/task-api@sha256:927d20ca4cebedc14f81770e8e5e49259684723ba65b76e7c59f3003cc9a9741
# +      - envFrom:
# +        - configMapRef:
# +            name: task-api-config
# +        image: 563149051155.dkr.ecr.us-east-1.amazonaws.com/task-api@sha256:927d20ca4cebedc14f81770e8e5e49259684723ba65b76e7c59f3003cc9a9741
#          imagePullPolicy: IfNotPresent
#          livenessProbe:
#            failureThreshold: 3
# @@ -72,7 +77,9 @@
#        restartPolicy: Always
#        schedulerName: default-scheduler
#        securityContext: {}
# -      terminationGracePeriodSeconds: 30
# +      serviceAccount: task-api
# +      serviceAccountName: task-api
# +      terminationGracePeriodSeconds: 20
#  status:
#    availableReplicas: 2
#    conditions:
# diff -u -N /tmp/LIVE-173526345/v1.Service.svc-task.task-api /tmp/MERGED-2415446233/v1.Service.svc-task.task-api
# --- /tmp/LIVE-173526345/v1.Service.svc-task.task-api    2025-08-16 22:51:58.725159956 +0800
# +++ /tmp/MERGED-2415446233/v1.Service.svc-task.task-api 2025-08-16 22:51:58.725159956 +0800
# @@ -5,6 +5,8 @@
#      kubectl.kubernetes.io/last-applied-configuration: |
#        {"apiVersion":"v1","kind":"Service","metadata":{"annotations":{},"name":"task-api","namespace":"svc-task"},"spec":{"ports":[{"name":"http","port":8080,"targetPort":8080}],"selector":{"app":"task-api"},"type":"ClusterIP"}}
#    creationTimestamp: "2025-08-16T14:40:34Z"
# +  labels:
# +    app: task-api
#    name: task-api
#    namespace: svc-task
#    resourceVersion: "3510"
```

- åŒåï¼ˆåŒ Kind / åŒ Namespaceï¼‰çš„ `kubectl apply` ä¼šåšå¢é‡ Patch å¹¶è§¦å‘ Deployment çš„æ»šåŠ¨æ›´æ–°ï¼Œä¸éœ€è¦å…ˆâ€œæš‚åœ/ä¸‹çº¿â€åŸå®ä¾‹ã€‚
- Kubernetes ä¼šæŒ‰ç­–ç•¥è¾¹ä¸Šçº¿æ–° Pod è¾¹ä¸‹çº¿æ—§ Podï¼ŒService ä¼šåªæŠŠ **å°±ç»ªï¼ˆreadiness=UPï¼‰** çš„æ–° Pod çº³å…¥è´Ÿè½½ï¼Œæ‰€ä»¥é€šå¸¸ä¸ä¼šä¸­æ–­ã€‚

åº”ç”¨å¹¶ç­‰å¾…æ»šåŠ¨å®Œæˆï¼š

```bash
# ç¡®ä¿ kubeconfig è·Ÿ aws åŒæ­¥
aws eks update-kubeconfig --name "$CLUSTER" --region "$AWS_REGION" --profile "$PROFILE"
# è¾“å‡ºï¼š
# Updated context arn:aws:eks:us-east-1:563149051155:cluster/dev in /home/renda/.kube/config

# åº”ç”¨
kubectl -n "$NS" apply -f k8s/base/ns-sa.yaml
# è¾“å‡ºï¼š
# namespace/svc-task configured
# serviceaccount/task-api created
kubectl -n "$NS" apply -f k8s/base/configmap.yaml
# è¾“å‡ºï¼š
# configmap/task-api-config created
kubectl -n "$NS" apply -f k8s/base/deploy-svc.yaml
# è¾“å‡ºï¼š
# deployment.apps/task-api configured
# service/task-api configured

# è§‚å¯Ÿæ»šåŠ¨
kubectl -n "$NS" rollout status deploy/"$APP" --timeout=180s
# è¾“å‡ºï¼š
# Waiting for deployment "task-api" rollout to finish: 1 out of 2 new replicas have been updated...
# Waiting for deployment "task-api" rollout to finish: 1 out of 2 new replicas have been updated...
# Waiting for deployment "task-api" rollout to finish: 1 out of 2 new replicas have been updated...
# Waiting for deployment "task-api" rollout to finish: 1 old replicas are pending termination...
# Waiting for deployment "task-api" rollout to finish: 1 old replicas are pending termination...
# deployment "task-api" successfully rolled out

kubectl -n "$NS" get deploy,svc -o wide
# è¾“å‡º
# NAME                       READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES                                                                                                                          SELECTOR
# deployment.apps/task-api   2/2     2            2           19m   task-api     563149051155.dkr.ecr.us-east-1.amazonaws.com/task-api@sha256:927d20ca4cebedc14f81770e8e5e49259684723ba65b76e7c59f3003cc9a9741   app=task-api

# NAME               TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE   SELECTOR
# service/task-api   ClusterIP   172.20.245.244   <none>        8080/TCP   19m   app=task-api
```

**é¢„æœŸï¼š**

`rollout status` æˆåŠŸï¼›`task-api` çš„ `IMAGE` å­—æ®µå±•ç¤ºä¸º `â€¦/${ECR_REPO}@sha256:â€¦`ï¼ˆdigest å½¢å¼ï¼‰ï¼Œ`READY`=2/2ã€‚

### å¿«é€Ÿå†’çƒŸï¼ˆé›†ç¾¤å†…ï¼‰

```bash
kubectl -n "$NS" port-forward svc/"$APP" 8080:8080 >/dev/null 2>&1 &
PF=$!; sleep 2
# è¾“å‡ºï¼š
# [1] 109916

curl -s "http://127.0.0.1:8080/api/hello?name=Renda"; echo
# è¾“å‡ºï¼š
# hello Renda

curl -s http://127.0.0.1:8080/actuator/health; echo
# è¾“å‡ºï¼š
# {"status":"UP","groups":["liveness","readiness"]}

ps aux | grep kubectl
# è¾“å‡ºï¼š
# renda     109916  0.0  0.1 1287760 48256 pts/0   Sl   23:05   0:00 kubectl -n svc-task port-forward svc/task-api 8080:8080

kill $PF >/dev/null 2>&1 || true
ps aux | grep kubectl
# è¾“å‡ºï¼š
# [1]+  Terminated              kubectl -n "$NS" port-forward svc/"$APP" 8080:8080 > /dev/null 2>&1
```

---

## Step 2/5 - IRSA ç”¨ Terraformï¼ŒALB Controller ç”¨ Helmï¼ˆè¿› `post-recreate.sh`ï¼‰

**ç›®æ ‡**ï¼š

1. ç”¨ Terraform åˆ›å»º **ALBC çš„ IRSA**ï¼ˆIAM Role + Policy + ç»‘å®šåˆ° `kube-system/aws-load-balancer-controller` SAï¼‰ï¼›
2. åœ¨ `post-recreate.sh` é‡Œç”¨ Helm å®‰è£…/å‡çº§ **AWS Load Balancer Controller**ï¼Œå¹¶**ç­‰å¾…å°±ç»ª**ï¼›
3. å›ºå®š Chart / é•œåƒç‰ˆæœ¬ + å¤„ç† **CRDs å‡çº§**ã€‚

### Terraformï¼šåˆ›å»º IRSAï¼ˆRole + Policy + SA æ³¨è§£ï¼‰

**ç›®å½•å»ºè®®**ï¼šæŠŠæ–°å¢çš„ HCL æ–‡ä»¶æ”¾åˆ° `infra/aws/modules/irsa_albc` ç›®å½•ä¸‹ã€‚

#### å‡†å¤‡å®˜æ–¹ç­–ç•¥ï¼ˆæ”¾æ–‡ä»¶ï¼Œä¾¿äºä»¥åå‡çº§æ›¿æ¢ï¼‰

`infra/aws/modules/irsa_albc/policy.json`

> å†…å®¹ä½¿ç”¨ AWS å®˜æ–¹æä¾›çš„ ALBC IAM Policy JSONï¼ˆä½“é‡è¾ƒå¤§ï¼Œè¿™é‡Œä¸ç²˜è´´ï¼‰ã€‚
> ç¬¬ä¸€æ¬¡å¯ä»¥æ‰‹åŠ¨ä¸‹è½½æ”¾å…¥è¯¥è·¯å¾„ï¼›åç»­å‡çº§åªéœ€æ›´æ–°è¿™ä¸ªæ–‡ä»¶å¹¶ `terraform apply`ã€‚

#### HCL ä»£ç ï¼ˆIRSA + SA æ³¨è§£ï¼‰

æ–°å¢ `infra/aws/modules/irsa_albc/main.tf` æ–‡ä»¶ï¼š

```hcl
// ---------------------------
// IRSA æ¨¡å—ï¼šä¸º Kubernetes ServiceAccount ç»‘å®š IAM è§’è‰²
// ç”¨äº AWS Load Balancer Controller è®¿é—® AWS API
// ---------------------------

resource "aws_iam_role" "aws_load_balancer_controller" {
  name        = var.name                                                            # IAM è§’è‰²åç§°
  description = "IRSA role for AWS Load Balancer Controller in ${var.cluster_name}" # è§’è‰²æè¿°
  assume_role_policy = jsonencode(
    {
      Version = "2012-10-17"
      Statement = [
        {
          Action = "sts:AssumeRoleWithWebIdentity"
          Effect = "Allow"
          Principal = {
            Federated = var.oidc_provider_arn # EKS OIDC Provider ARN
          }
          Condition = {
            StringEquals = {
              "${var.oidc_provider_url_without_https}:sub" = "system:serviceaccount:${var.namespace}:${var.service_account_name}"
            }
          }
        }
      ]
    }
  )

  lifecycle {
    create_before_destroy = true # å…ˆåˆ›å»ºæ–°è§’è‰²å†é”€æ¯æ—§è§’è‰²
  }
}

# åˆ›å»º AWS Load Balancer Controller IAM ç­–ç•¥
resource "aws_iam_policy" "albc" {
  name        = "${var.cluster_name}-AWSLoadBalancerControllerPolicy"
  description = "Policy for AWS Load Balancer Controller"

  # å®˜æ–¹æ¨èçš„æƒé™ç­–ç•¥
  policy = file("${path.module}/policy.json")
}

resource "aws_iam_role_policy_attachment" "albc_attach" {
  role       = aws_iam_role.aws_load_balancer_controller.name # å…³è”çš„ IAM è§’è‰²
  policy_arn = aws_iam_policy.albc.arn                        # IAM ç­–ç•¥ ARN

  depends_on = [
    aws_iam_role.aws_load_balancer_controller,
    aws_iam_policy.albc
  ]

  lifecycle {
    create_before_destroy = true
  }
}
```

æ–°å¢ `infra/aws/modules/irsa_albc/outputs.tf` æ–‡ä»¶ï¼š

```hcl
// è¾“å‡º AWS Load Balancer Controller æ‰€ä½¿ç”¨çš„ IAM è§’è‰² ARN
output "albc_role_arn" {
  description = "IAM Role ARN for the AWS Load Balancer Controller"
  value       = aws_iam_role.aws_load_balancer_controller.arn
}
```

æ–°å¢ `infra/aws/modules/irsa_albc/variables.tf` æ–‡ä»¶ï¼š

```hcl
// AWS Load Balancer Controller IRSA æ¨¡å—æ‰€éœ€çš„å˜é‡å®šä¹‰
variable "name" {
  description = "Name of the IRSA Role"
  type        = string
}

variable "cluster_name" {
  description = "Name of the EKS cluster"
  type        = string
}

variable "namespace" {
  description = "K8s Namespace of the ServiceAccount"
  type        = string
}

variable "service_account_name" {
  description = "Name of the ServiceAccount in Kubernetes"
  type        = string
}

variable "oidc_provider_arn" {
  description = "ARN of the OIDC provider for the EKS cluster"
  type        = string
}

variable "oidc_provider_url_without_https" {
  description = "OIDC provider URL (without https://)"
  type        = string
}
```

æ–°å¢ `infra/aws/modules/irsa_albc/versions.tf` æ–‡ä»¶ï¼š

```hcl
// æ¨¡å—ä½¿ç”¨çš„ Terraform åŠ provider ç‰ˆæœ¬
terraform {
  required_version = "~> 1.12"

  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 6.0"
    }
  }
}
```

æ›´æ–° `infra/aws/main.tf`ï¼š

```hcl
# æ–°å¢å¦‚ä¸‹å†…å®¹

module "irsa_albc" {
  source                          = "./modules/irsa_albc"                      # IRSA æ¨¡å—ï¼Œä¸º ALBC åˆ›å»ºè§’è‰²
  count                           = var.create_eks ? 1 : 0                     # ä»…åœ¨åˆ›å»º EKS æ—¶å¯ç”¨
  name                            = var.albc_irsa_role_name                    # ALBC IAM è§’è‰²åç§°
  namespace                       = var.albc_namespace                         # ALBC æ‰€åœ¨å‘½åç©ºé—´
  cluster_name                    = var.cluster_name                           # é›†ç¾¤åç§°
  service_account_name            = var.albc_service_account_name              # ALBC ServiceAccount åç§°
  oidc_provider_arn               = module.eks.oidc_provider_arn               # OIDC Provider ARN
  oidc_provider_url_without_https = module.eks.oidc_provider_url_without_https # OIDC URLï¼ˆæ—  httpsï¼‰
  depends_on                      = [module.eks]                               # ä¾èµ– EKS æ¨¡å—
}

resource "kubernetes_service_account" "aws_load_balancer_controller" {
  count = var.create_eks ? 1 : 0

  metadata {
    name      = var.albc_service_account_name
    namespace = var.albc_namespace
    annotations = {
      "eks.amazonaws.com/role-arn" = module.irsa_albc[0].albc_role_arn
    }
  }
}
```

æ›´æ–° `infra/aws/outputs.tf`ï¼š

```hcl
# æ–°å¢å¦‚ä¸‹å†…å®¹
output "albc_role_arn" {
  description = "AWS Load Balancer Controller ä½¿ç”¨çš„ IAM è§’è‰² ARN"
  value       = var.create_eks ? module.irsa_albc[0].albc_role_arn : null
}
```

æ›´æ–° `infra/aws/provider.tf`ï¼š

```hcl
# æ–°å¢å¦‚ä¸‹å†…å®¹
provider "kubernetes" {
  config_path = "~/.kube/config" # ä¸ helm å…±ç”¨ kubeconfig
}
```

æ›´æ–° `infra/aws/terraform.tfvars`ï¼š

```hcl
# æ–°å¢å¦‚ä¸‹å†…å®¹
# ALBC IRSA é…ç½®
albc_irsa_role_name       = "aws-load-balancer-controller" # ALBC IRSA è§’è‰²åç§°
albc_service_account_name = "aws-load-balancer-controller" # ALBC ServiceAccount åç§°
albc_namespace            = "kube-system"                  # ALBC æ‰€åœ¨å‘½åç©ºé—´
```

æ›´æ–° `infra/aws/variables.tf`ï¼š

```hcl
# æ–°å¢å¦‚ä¸‹å†…å®¹

variable "albc_irsa_role_name" {
  description = "Name of the IRSA role for AWS Load Balancer Controller"
  type        = string
  default     = "aws-load-balancer-controller"
}

variable "albc_service_account_name" {
  description = "Kubernetes ServiceAccount name for AWS Load Balancer Controller"
  type        = string
  default     = "aws-load-balancer-controller"
}

variable "albc_namespace" {
  description = "Namespace for AWS Load Balancer Controller ServiceAccount"
  type        = string
  default     = "kube-system"
}
```

æ›´æ–° `infra/aws/versions.tf`ï¼Œæ–°å¢å¦‚ä¸‹å†…å®¹ï¼š

```hcl
terraform {
  required_version = "~> 1.12" # Terraform CLI ç‰ˆæœ¬è¦æ±‚
  required_providers {
  ...
    # æ–°å¢ kubernetes
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.23"
    }
  ...
  }
}
```

#### ä½¿ç”¨ Terraform æ‰§è¡Œå˜æ›´ï¼š

```bash
cd ${WORK_DIR}

terraform -chdir=infra/aws init -reconfigure
terraform -chdir=infra/aws apply -auto-approve -input=false \
        -var="region=us-east-1" \
        -var="create_nat=true" \
        -var="create_alb=true" \
        -var="create_eks=true"
# è¾“å‡º
# Apply complete! Resources: 4 added, 0 changed, 0 destroyed.
# Outputs:
# alb_dns = "alb-demo-293119581.us-east-1.elb.amazonaws.com"
# albc_role_arn = "arn:aws:iam::563149051155:role/aws-load-balancer-controller"
# autoscaler_role_arn = "arn:aws:iam::563149051155:role/eks-cluster-autoscaler"
# private_subnet_ids = [
#   "subnet-0422bec13e7eec9e6",
#   "subnet-00630bdad3664ee18",
# ]
# public_subnet_ids = [
#   "subnet-066a65e68e06df5db",
#   "subnet-08ca22e6d15635564",
# ]
# vpc_id = "vpc-0b06ba5bfab99498b"
```

æŸ¥çœ‹æ–°å»ºçš„ IAM Roleï¼š

```bash
# ä½¿ç”¨å‘½ä»¤æ£€æŸ¥è§’è‰²çš„ ARN
aws iam list-roles \
  --query "Roles[?RoleName == 'aws-load-balancer-controller'].Arn" \
  --output text
# è¾“å‡ºï¼š
# arn:aws:iam::563149051155:role/aws-load-balancer-controller

# æ£€æŸ¥è§’è‰²è¢«æˆäºˆçš„æƒé™ç­–ç•¥
aws iam list-attached-role-policies --role-name aws-load-balancer-controller
# è¾“å‡ºï¼š
# {
#     "AttachedPolicies": [
#         {
#             "PolicyName": "dev-AWSLoadBalancerControllerPolicy",
#             "PolicyArn": "arn:aws:iam::563149051155:policy/dev-AWSLoadBalancerControllerPolicy"
#         }
#     ]
# }
```

æŸ¥çœ‹ ServiceAccount çš„è¯¦ç»†ä¿¡æ¯ï¼š

```bash
kubectl -n kube-system describe serviceaccount aws-load-balancer-controller
# è¾“å‡ºï¼š
# Name:                aws-load-balancer-controller
# Namespace:           kube-system
# Labels:              <none>
# Annotations:         eks.amazonaws.com/role-arn: arn:aws:iam::563149051155:role/aws-load-balancer-controller
# Image pull secrets:  <none>
# Mountable secrets:   <none>
# Tokens:              <none>
# Events:              <none>
```

å·²ç»ç¡®è®¤ï¼š

- ç”Ÿæˆäº† IAM Role `aws-load-balancer-controller` å¹¶é™„åŠ ç­–ç•¥ `dev-AWSLoadBalancerControllerPolicy`ï¼›
- `kube-system` ä¸‹å‡ºç°äº† SA `aws-load-balancer-controller`ï¼Œå¹¶å¸¦æœ‰ `eks.amazonaws.com/role-arn` æ³¨è§£ã€‚

### Helm å®‰è£…/å‡çº§ + CRDsï¼šæ›´æ–° `post-recreate.sh`

åœ¨ `scripts/post-recreate.sh` é‡Œæ–°å¢å¦‚ä¸‹ä»£ç ï¼š

```sh
...

# AWS Load Balancer Controller settings
ALBC_CHART_NAME="aws-load-balancer-controller"
ALBC_RELEASE_NAME=${ALBC_CHART_NAME}
ALBC_SERVICE_ACCOUNT_NAME=${ALBC_CHART_NAME}
ALBC_CHART_VERSION="1.8.1"
ALBC_IMAGE_TAG="v2.8.1"
ALBC_IMAGE_REPO="602401143452.dkr.ecr.${REGION}.amazonaws.com/amazon/aws-load-balancer-controller"
ALBC_HELM_REPO_NAME="eks"
ALBC_HELM_REPO_URL="https://aws.github.io/eks-charts"
POD_ALBC_LABEL="app.kubernetes.io/name=${ALBC_RELEASE_NAME}"

...

# æ£€æŸ¥ AWS Load Balancer Controller éƒ¨ç½²çŠ¶æ€
check_albc_status() {
  if ! kubectl -n $KUBE_DEFAULT_NAMESPACE get deployment $ALBC_RELEASE_NAME >/dev/null 2>&1; then
    echo "missing"
    return
  fi
  if kubectl -n $KUBE_DEFAULT_NAMESPACE get pod -l $POD_ALBC_LABEL \
      --no-headers 2>/dev/null | grep -v Running >/dev/null; then
    echo "unhealthy"
  else
    echo "healthy"
  fi
}

...

# å®‰è£…æˆ–å‡çº§ AWS Load Balancer Controller
install_albc_controller() {
  local status
  status=$(check_albc_status)
  case "$status" in
    healthy)
      log "âœ… AWS Load Balancer Controller å·²æ­£å¸¸è¿è¡Œ, æ‰§è¡Œ Helm å‡çº§ä»¥ç¡®ä¿ç‰ˆæœ¬ä¸€è‡´"
      ;;
    missing)
      log "âš™ï¸  æ£€æµ‹åˆ° AWS Load Balancer Controller æœªéƒ¨ç½², å¼€å§‹å®‰è£…"
      ;;
    unhealthy)
      log "âŒ æ£€æµ‹åˆ° AWS Load Balancer Controller çŠ¶æ€å¼‚å¸¸, åˆ é™¤æ—§ Pod åé‡æ–°éƒ¨ç½²"
      kubectl -n $KUBE_DEFAULT_NAMESPACE delete pod -l $POD_ALBC_LABEL --ignore-not-found
      ;;
    *)
      log "âš ï¸  æœªçŸ¥çš„ AWS Load Balancer Controller çŠ¶æ€, ç»§ç»­å°è¯•å®‰è£…"
      ;;
  esac

  if ! helm repo list | grep -q "^${ALBC_HELM_REPO_NAME}\b"; then
    log "ğŸ”§ æ·»åŠ  ${ALBC_HELM_REPO_NAME} Helm ä»“åº“"
    helm repo add ${ALBC_HELM_REPO_NAME} ${ALBC_HELM_REPO_URL}
  fi
  helm repo update

  log "ğŸ“¦ åº”ç”¨ AWS Load Balancer Controller CRDs (version ${ALBC_CHART_VERSION})"
  tmp_dir=$(mktemp -d)
  helm pull ${ALBC_HELM_REPO_NAME}/${ALBC_CHART_NAME} --version ${ALBC_CHART_VERSION} --untar -d "$tmp_dir"
  kubectl apply -f "$tmp_dir/${ALBC_CHART_NAME}/crds"
  rm -rf "$tmp_dir"

  VPC_ID=$(aws eks describe-cluster --name "$CLUSTER_NAME" --region "$REGION" --profile "$PROFILE" --query "cluster.resourcesVpcConfig.vpcId" --output text)

  log "ğŸš€ æ­£åœ¨é€šè¿‡ Helm å®‰è£…æˆ–å‡çº§ AWS Load Balancer Controller..."
  helm upgrade --install ${ALBC_RELEASE_NAME} ${ALBC_HELM_REPO_NAME}/${ALBC_CHART_NAME} \
    -n $KUBE_DEFAULT_NAMESPACE \
    --version ${ALBC_CHART_VERSION} \
    --set clusterName=$CLUSTER_NAME \
    --set region=$REGION \
    --set vpcId=$VPC_ID \
    --set serviceAccount.create=false \
    --set serviceAccount.name=${ALBC_SERVICE_ACCOUNT_NAME} \
    --set image.repository=${ALBC_IMAGE_REPO} \
    --set image.tag=${ALBC_IMAGE_TAG}

  log "ğŸ” ç­‰å¾… AWS Load Balancer Controller å°±ç»ª"
  kubectl -n $KUBE_DEFAULT_NAMESPACE rollout status deployment/${ALBC_RELEASE_NAME} --timeout=180s
  kubectl -n $KUBE_DEFAULT_NAMESPACE get pod -l $POD_ALBC_LABEL
}

...

# è¿›è¡ŒåŸºç¡€èµ„æºæ£€æŸ¥
perform_health_checks() {
  ...
  log "ğŸ” æ£€æŸ¥ AWS Load Balancer Controller éƒ¨ç½²çŠ¶æ€"
  albc_status=$(check_albc_status)
  log "AWS Load Balancer Controller status: $albc_status"
  ...
}

...

install_albc_controller
```

å·²ç»ç¡®ä¿ï¼š

* **ç‰ˆæœ¬å›ºå®š**ï¼š`ALBC_CHART_VERSION` ä¸ `ALBC_IMAGE_TAG` å›ºå®šï¼Œç¡®ä¿æ¯æ—¥é‡å»ºç»“æœä¸€è‡´ã€‚
* **CRDs å…ˆè¡Œ**ï¼š`kubectl apply -f "$tmp_dir/${ALBC_CHART_NAME}/crds"` è§£å†³å‡çº§åœºæ™¯ä¸‹ CRDs ä¸æ›´æ–°çš„é—®é¢˜ã€‚
* **ä¸åˆ›å»º SA**ï¼š`serviceAccount.create=false`ï¼Œé¿å…ä¸ Terraform ç®¡ç†çš„ SA å†²çªã€‚
* **å°±ç»ªç­‰å¾…**ï¼š`rollout status` ä¿éšœåç»­ Ingress åˆ›å»ºä¸â€œæ’å¢™â€ã€‚

### éªŒè¯

æ‰§è¡Œ `bash scripts/post-recreate.sh` å®Œæˆæ§åˆ¶å™¨å®‰è£…ã€‚

éªŒè¯ï¼š

```bash
kubectl -n kube-system rollout status deploy/aws-load-balancer-controller
kubectl -n kube-system get deploy aws-load-balancer-controller
kubectl -n kube-system get pod -l app.kubernetes.io/name=aws-load-balancer-controller
kubectl -n kube-system logs deploy/aws-load-balancer-controller | tail -n 100
```

éªŒè¯ç»“æœï¼š

Deployment å¯ç”¨å‰¯æœ¬å°±ç»ªï¼Œæ—¥å¿—ç»“å°¾æ— æŠ¥é”™ï¼ˆè‹¥æœ‰å­ç½‘æ ‡ç­¾/æƒé™é—®é¢˜ï¼Œæ—¥å¿—ä¼šå³æ—¶æç¤ºï¼‰ã€‚

```bash
$ kubectl -n kube-system logs deploy/aws-load-balancer-controller | grep error
Found 2 pods, using pod/aws-load-balancer-controller-8574d469c6-b4cr9
$ kubectl -n kube-system logs deploy/aws-load-balancer-controller | grep warning
Found 2 pods, using pod/aws-load-balancer-controller-8574d469c6-b4cr9
W0816 20:21:19.838633       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0816 20:21:19.840303       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0816 20:28:47.843007       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
W0816 20:34:04.845735       1 warnings.go:70] v1 Endpoints is deprecated in v1.33+; use discovery.k8s.io/v1 EndpointSlice
```

---

## Step 3/5 â€” åˆ›å»º Ingressï¼ˆç”Ÿæˆ ALBï¼‰+ å…¬ç½‘éªŒè¯ + å†™å…¥ `post-recreate.sh`

ç›®æ ‡ï¼š

ç”¨ **AWS Load Balancer Controller** ä¸º `task-api` ç”Ÿæˆå…¬ç½‘ **ALB**ï¼Œå¥åº·æ£€æŸ¥èµ° `/actuator/health/readiness`ï¼Œå¹¶æŠŠè¿™ä¸€æ­¥è‡ªåŠ¨åŒ–è¿›ä½ çš„æ¯æ—¥é‡å»ºè„šæœ¬ã€‚

### é¢„æ£€æŸ¥ï¼ˆå­ç½‘æ ‡ç­¾æ˜¯å¦ OKï¼‰

```bash
export PROFILE=phase2-sso
export AWS_PROFILE=$PROFILE
export AWS_REGION=us-east-1
export CLUSTER=dev

VPC_ID=$(aws eks describe-cluster --name "$CLUSTER" --region "$AWS_REGION" \
  --query 'cluster.resourcesVpcConfig.vpcId' --output text)

aws ec2 describe-subnets --filters "Name=vpc-id,Values=$VPC_ID" \
  --query 'Subnets[].{Id:SubnetId,Pub:MapPublicIpOnLaunch,Tags:Tags}' --output table

# çœ‹çœ‹å…¬æœ‰å­ç½‘æ˜¯å¦æœ‰ kubernetes.io/role/elb=1ã€ç§æœ‰å­ç½‘æ˜¯å¦æœ‰ kubernetes.io/role/internal-elb=1
# ä»¥åŠæ‰€æœ‰å‚ä¸å­ç½‘æœ‰ kubernetes.io/cluster/dev = shared/owned
```

### å†™ Ingress æ¸…å•

> æ–‡ä»¶ï¼š`${WORK_DIR}/task-api/k8s/ingress.yaml`

```yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: task-api
  namespace: svc-task
  annotations:
    alb.ingress.kubernetes.io/scheme: internet-facing      # å…¬ç½‘ ALBï¼›è‹¥èµ°å†…ç½‘æ”¹: internal
    alb.ingress.kubernetes.io/target-type: ip              # ç›´è¿ Podï¼ˆæ¨èï¼‰
    alb.ingress.kubernetes.io/healthcheck-path: /actuator/health/readiness
    alb.ingress.kubernetes.io/healthcheck-port: traffic-port
    alb.ingress.kubernetes.io/listen-ports: '[{"HTTP":80}]'
    # å¦‚éœ€ X-Forwarded-* ä¿ç•™ï¼š
    alb.ingress.kubernetes.io/load-balancer-attributes: routing.http.xff_header_processing.mode=preserve
spec:
  ingressClassName: alb
  rules:
    - http:
        paths:
          - path: /
            pathType: Prefix
            backend:
              service:
                name: task-api
                port:
                  number: 8080
```

åº”ç”¨ï¼š

```bash
kubectl apply -f "${WORK_DIR}/task-api/k8s/ingress.yaml"
# è¾“å‡ºï¼š
# ingress.networking.k8s.io/task-api created
kubectl -n svc-task get ingress task-api
# è¾“å‡ºï¼š
# NAME       CLASS   HOSTS   ADDRESS                                                                PORTS   AGE
# task-api   alb     *       k8s-svctask-taskapi-c91e97499e-281967989.us-east-1.elb.amazonaws.com   80      13s
```

### ç­‰å¾… ALB åˆ†é…åœ°å€å¹¶å†’çƒŸéªŒè¯

```bash
# ç­‰å¾… ALB DNS å‡ºç°
for i in {1..30}; do
  ALB_DNS=$(kubectl -n svc-task get ing task-api -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
  [[ -n "$ALB_DNS" ]] && echo "ALB_DNS=$ALB_DNS" && break
  echo "waiting ALB..."; sleep 10
done
[[ -z "$ALB_DNS" ]] && echo "ALB æœªå°±ç»ªï¼Œè¯·æ£€æŸ¥ Controller/å­ç½‘æ ‡ç­¾/æƒé™" && exit 1

# è¾“å‡ºï¼š
# ALB_DNS=k8s-svctask-taskapi-c91e97499e-281967989.us-east-1.elb.amazonaws.com

# å†’çƒŸ
curl -s "http://$ALB_DNS/api/hello?name=Renda"; echo
# è¾“å‡ºï¼š
# hello Renda
curl -s "http://$ALB_DNS/actuator/health"; echo
# è¾“å‡ºï¼š
# {"status":"UP","groups":["liveness","readiness"]}
curl -sI "http://$ALB_DNS/" | sed -n '1,10p'
# è¾“å‡ºï¼š
HTTP/1.1 404
Date: Sat, 16 Aug 2025 21:36:23 GMT
Content-Type: application/json
Connection: keep-alive
Vary: Origin
Vary: Access-Control-Request-Method
Vary: Access-Control-Request-Headers
```

**é¢„æœŸï¼š**

`/api/hello` è¿”å› `hello Renda`ï¼Œå¥åº·æ£€æŸ¥ `{"status":"UP"}`ï¼›`curl -I` é¦–è¡Œ `HTTP/1.1 200 OK` æˆ– `HTTP/1.1 404` æˆ– 302 å–å†³äºæ ¹è·¯å¾„æ˜¯å¦æœ‰èµ„æºï¼Œä½†é‡è¦çš„æ˜¯å¯è¾¾ï¼‰ã€‚

### æŠŠ Ingress å‘å¸ƒä¸ç­‰å¾…å†™è¿› `post-recreate.sh`

> æ–°å¢ä»¥ä¸‹å‡½æ•°ä¸è°ƒç”¨ï¼š

```bash
# ---- Ingress for task-api ----
ING_FILE="${ROOT_DIR}/task-api/k8s/ingress.yaml"

# éƒ¨ç½² taskapi ingress
deploy_taskapi_ingress() {
  set -euo pipefail
  local outdir="${SCRIPT_DIR}/.out"; mkdir -p "$outdir"

  log "ğŸ“¦ Apply Ingress (${APP}) ..."
  # è‹¥æ— å˜æ›´å°±ä¸ applyï¼ˆ0=æ— å·®å¼‚ï¼Œ1=æœ‰å·®å¼‚ï¼Œ>1=å‡ºé”™ï¼‰
  if kubectl -n "$NS" diff -f "$ING_FILE" >/dev/null 2>&1; then
    log "â‰¡ No changes"
  else
    kubectl apply -f "$ING_FILE"
  fi

  # å¦‚æœå·²ç»æœ‰ ALBï¼Œå°±ç›´æ¥å¤ç”¨å¹¶è¿”å›
  local dns
  dns=$(kubectl -n "$NS" get ing "$APP" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
  if [[ -n "${dns}" ]]; then
    log "âœ… ALB ready: http://${dns}"
    echo "${dns}" > "${outdir}/alb_${APP}_dns"
    return 0
  fi

  log "â³ Waiting for ALB to be provisioned ..."
  local t=0; local timeout=600
  while [[ $t -lt $timeout ]]; do
    dns=$(kubectl -n "$NS" get ing "$APP" -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
    [[ -n "${dns}" ]] && break
    sleep 5; t=$((t+5))
  done
  [[ -z "${dns}" ]] && { log "âŒ Timeout waiting ALB"; return 1; }

  log "âœ… ALB ready: http://${dns}"
  echo "${dns}" > "${outdir}/alb_${APP}_dns"

  log "ğŸ§ª Smoke"
  curl -s "http://${dns}/api/hello?name=Renda" | sed -n '1p'
  curl -s "http://${dns}/actuator/health" | sed -n '1p'
}

# åœ¨è„šæœ¬ä¸»æµç¨‹åˆé€‚ä½ç½®è°ƒç”¨ï¼ˆåœ¨ ALBC å®‰è£…å®Œæˆä¹‹åï¼‰
deploy_taskapi_ingress
```

> æé†’ï¼šè‹¥åç»­è¦å¯ç”¨ HTTPSï¼Œåªéœ€åœ¨ Ingress ä¸ŠåŠ è¯ä¹¦ ARNï¼š
>
> `alb.ingress.kubernetes.io/certificate-arn: arn:aws:acm:us-east-1:<ACCOUNT_ID>:certificate/<ID>`
>
> å¹¶æŠŠ `listen-ports` æ”¹ä¸º `[{"HTTP":80},{"HTTPS":443}]`ï¼Œå†åœ¨ `spec.tls` ä¸­å£°æ˜ä¸»æœºåã€‚

---
