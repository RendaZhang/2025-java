<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [高频问题库](#%E9%AB%98%E9%A2%91%E9%97%AE%E9%A2%98%E5%BA%93)
  - [API Design & Reliability](#api-design--reliability)
    - [契约清晰：资源建模 & 语义化接口（Contract Clarity）](#%E5%A5%91%E7%BA%A6%E6%B8%85%E6%99%B0%E8%B5%84%E6%BA%90%E5%BB%BA%E6%A8%A1--%E8%AF%AD%E4%B9%89%E5%8C%96%E6%8E%A5%E5%8F%A3contract-clarity)
    - [版本化策略：URI vs Header；向后兼容与下线流程](#%E7%89%88%E6%9C%AC%E5%8C%96%E7%AD%96%E7%95%A5uri-vs-header%E5%90%91%E5%90%8E%E5%85%BC%E5%AE%B9%E4%B8%8E%E4%B8%8B%E7%BA%BF%E6%B5%81%E7%A8%8B)
    - [鉴权与授权：JWT/OIDC、最小权限、Token 续期与旋转](#%E9%89%B4%E6%9D%83%E4%B8%8E%E6%8E%88%E6%9D%83jwtoidc%E6%9C%80%E5%B0%8F%E6%9D%83%E9%99%90token-%E7%BB%AD%E6%9C%9F%E4%B8%8E%E6%97%8B%E8%BD%AC)
    - [幂等性：幂等键、PUT vs POST、重试安全](#%E5%B9%82%E7%AD%89%E6%80%A7%E5%B9%82%E7%AD%89%E9%94%AEput-vs-post%E9%87%8D%E8%AF%95%E5%AE%89%E5%85%A8)
    - [限流-重试-熔断：客户端与服务端协同；退避策略；降级与快速失败](#%E9%99%90%E6%B5%81-%E9%87%8D%E8%AF%95-%E7%86%94%E6%96%AD%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%8D%8F%E5%90%8C%E9%80%80%E9%81%BF%E7%AD%96%E7%95%A5%E9%99%8D%E7%BA%A7%E4%B8%8E%E5%BF%AB%E9%80%9F%E5%A4%B1%E8%B4%A5)
    - [错误码与可观察性：统一错误模型 / Trace-ID / 指标-日志-链路关联](#%E9%94%99%E8%AF%AF%E7%A0%81%E4%B8%8E%E5%8F%AF%E8%A7%82%E5%AF%9F%E6%80%A7%E7%BB%9F%E4%B8%80%E9%94%99%E8%AF%AF%E6%A8%A1%E5%9E%8B--trace-id--%E6%8C%87%E6%A0%87-%E6%97%A5%E5%BF%97-%E9%93%BE%E8%B7%AF%E5%85%B3%E8%81%94)
    - [灰度发布与回滚：逐步放量 / 健康检查 / 一键回滚](#%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E4%B8%8E%E5%9B%9E%E6%BB%9A%E9%80%90%E6%AD%A5%E6%94%BE%E9%87%8F--%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5--%E4%B8%80%E9%94%AE%E5%9B%9E%E6%BB%9A)
  - [DB & Cache](#db--cache)
    - [索引选型与失效（B+Tree、组合索引、覆盖索引、左前缀）](#%E7%B4%A2%E5%BC%95%E9%80%89%E5%9E%8B%E4%B8%8E%E5%A4%B1%E6%95%88btree%E7%BB%84%E5%90%88%E7%B4%A2%E5%BC%95%E8%A6%86%E7%9B%96%E7%B4%A2%E5%BC%95%E5%B7%A6%E5%89%8D%E7%BC%80)
    - [事务与隔离级别：RC / RR；MVCC 的一致性读 vs 当前读；如何避免幻读与超卖](#%E4%BA%8B%E5%8A%A1%E4%B8%8E%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%ABrc--rrmvcc-%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E8%AF%BB-vs-%E5%BD%93%E5%89%8D%E8%AF%BB%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E5%B9%BB%E8%AF%BB%E4%B8%8E%E8%B6%85%E5%8D%96)
    - [慢查询定位：执行计划、扫描行数、回表/下推、坏味道清单](#%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%AE%9A%E4%BD%8D%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%E6%89%AB%E6%8F%8F%E8%A1%8C%E6%95%B0%E5%9B%9E%E8%A1%A8%E4%B8%8B%E6%8E%A8%E5%9D%8F%E5%91%B3%E9%81%93%E6%B8%85%E5%8D%95)
    - [读写分离的坑：主从延迟、读旧值、强一致读 / 亲和策略](#%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E7%9A%84%E5%9D%91%E4%B8%BB%E4%BB%8E%E5%BB%B6%E8%BF%9F%E8%AF%BB%E6%97%A7%E5%80%BC%E5%BC%BA%E4%B8%80%E8%87%B4%E8%AF%BB--%E4%BA%B2%E5%92%8C%E7%AD%96%E7%95%A5)
    - [缓存一致性：Cache-Aside 双删顺序、消息通知/回源、热键与热点保护](#%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7cache-aside-%E5%8F%8C%E5%88%A0%E9%A1%BA%E5%BA%8F%E6%B6%88%E6%81%AF%E9%80%9A%E7%9F%A5%E5%9B%9E%E6%BA%90%E7%83%AD%E9%94%AE%E4%B8%8E%E7%83%AD%E7%82%B9%E4%BF%9D%E6%8A%A4)
    - [三座大山：穿透 / 击穿 / 雪崩（识别与治理清单）](#%E4%B8%89%E5%BA%A7%E5%A4%A7%E5%B1%B1%E7%A9%BF%E9%80%8F--%E5%87%BB%E7%A9%BF--%E9%9B%AA%E5%B4%A9%E8%AF%86%E5%88%AB%E4%B8%8E%E6%B2%BB%E7%90%86%E6%B8%85%E5%8D%95)
  - [Message and Consistency](#message-and-consistency)
    - [Outbox（事务外箱）& 本地事务边界](#outbox%E4%BA%8B%E5%8A%A1%E5%A4%96%E7%AE%B1-%E6%9C%AC%E5%9C%B0%E4%BA%8B%E5%8A%A1%E8%BE%B9%E7%95%8C)
    - [幂等消费与去重键：表/Redis 实战与失败补偿](#%E5%B9%82%E7%AD%89%E6%B6%88%E8%B4%B9%E4%B8%8E%E5%8E%BB%E9%87%8D%E9%94%AE%E8%A1%A8redis-%E5%AE%9E%E6%88%98%E4%B8%8E%E5%A4%B1%E8%B4%A5%E8%A1%A5%E5%81%BF)
    - [重试策略与“重试预算”：退避 + 抖动 + 限额；与幂等/熔断的协同](#%E9%87%8D%E8%AF%95%E7%AD%96%E7%95%A5%E4%B8%8E%E9%87%8D%E8%AF%95%E9%A2%84%E7%AE%97%E9%80%80%E9%81%BF--%E6%8A%96%E5%8A%A8--%E9%99%90%E9%A2%9D%E4%B8%8E%E5%B9%82%E7%AD%89%E7%86%94%E6%96%AD%E7%9A%84%E5%8D%8F%E5%90%8C)
    - [DLQ / 停车场与人工处置：可观察、可回放、可审计](#dlq--%E5%81%9C%E8%BD%A6%E5%9C%BA%E4%B8%8E%E4%BA%BA%E5%B7%A5%E5%A4%84%E7%BD%AE%E5%8F%AF%E8%A7%82%E5%AF%9F%E5%8F%AF%E5%9B%9E%E6%94%BE%E5%8F%AF%E5%AE%A1%E8%AE%A1)
    - [顺序性与分区键：按“聚合维度”保序，吞吐与热点的权衡](#%E9%A1%BA%E5%BA%8F%E6%80%A7%E4%B8%8E%E5%88%86%E5%8C%BA%E9%94%AE%E6%8C%89%E8%81%9A%E5%90%88%E7%BB%B4%E5%BA%A6%E4%BF%9D%E5%BA%8F%E5%90%9E%E5%90%90%E4%B8%8E%E7%83%AD%E7%82%B9%E7%9A%84%E6%9D%83%E8%A1%A1)
    - [Exactly-once 的工程化取舍：追求 “effectively-once” 而非执念 EOS](#exactly-once-%E7%9A%84%E5%B7%A5%E7%A8%8B%E5%8C%96%E5%8F%96%E8%88%8D%E8%BF%BD%E6%B1%82-effectively-once-%E8%80%8C%E9%9D%9E%E6%89%A7%E5%BF%B5-eos)
  - [Java Concurrency](#java-concurrency)
    - [内存模型 & 可见性：happens-before / `volatile` 的边界](#%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B--%E5%8F%AF%E8%A7%81%E6%80%A7happens-before--volatile-%E7%9A%84%E8%BE%B9%E7%95%8C)
    - [`synchronized` vs `ReentrantLock`：可中断 / 定时 / 公平 / 条件队列](#synchronized-vs-reentrantlock%E5%8F%AF%E4%B8%AD%E6%96%AD--%E5%AE%9A%E6%97%B6--%E5%85%AC%E5%B9%B3--%E6%9D%A1%E4%BB%B6%E9%98%9F%E5%88%97)
    - [`ThreadPoolExecutor` 七参数、队列取舍与拒绝策略](#threadpoolexecutor-%E4%B8%83%E5%8F%82%E6%95%B0%E9%98%9F%E5%88%97%E5%8F%96%E8%88%8D%E4%B8%8E%E6%8B%92%E7%BB%9D%E7%AD%96%E7%95%A5)
    - [`CompletableFuture` 任务编排：并行、超时、取消与自定义 Executor](#completablefuture-%E4%BB%BB%E5%8A%A1%E7%BC%96%E6%8E%92%E5%B9%B6%E8%A1%8C%E8%B6%85%E6%97%B6%E5%8F%96%E6%B6%88%E4%B8%8E%E8%87%AA%E5%AE%9A%E4%B9%89-executor)
    - [并发诊断与排障：死锁、线程池饱和、阻塞点定位，5 分钟 SOP](#%E5%B9%B6%E5%8F%91%E8%AF%8A%E6%96%AD%E4%B8%8E%E6%8E%92%E9%9A%9C%E6%AD%BB%E9%94%81%E7%BA%BF%E7%A8%8B%E6%B1%A0%E9%A5%B1%E5%92%8C%E9%98%BB%E5%A1%9E%E7%82%B9%E5%AE%9A%E4%BD%8D5-%E5%88%86%E9%92%9F-sop)
    - [突发流量 + 下游限速，线程池怎么“吸收不作死”？](#%E7%AA%81%E5%8F%91%E6%B5%81%E9%87%8F--%E4%B8%8B%E6%B8%B8%E9%99%90%E9%80%9F%E7%BA%BF%E7%A8%8B%E6%B1%A0%E6%80%8E%E4%B9%88%E5%90%B8%E6%94%B6%E4%B8%8D%E4%BD%9C%E6%AD%BB)
    - [`CompletableFuture` 并行编排要做到：fail-fast + 可取消 + 明确降级](#completablefuture-%E5%B9%B6%E8%A1%8C%E7%BC%96%E6%8E%92%E8%A6%81%E5%81%9A%E5%88%B0fail-fast--%E5%8F%AF%E5%8F%96%E6%B6%88--%E6%98%8E%E7%A1%AE%E9%99%8D%E7%BA%A7)
    - [锁竞争 / 死锁如何 5 分钟内定位并修复？](#%E9%94%81%E7%AB%9E%E4%BA%89--%E6%AD%BB%E9%94%81%E5%A6%82%E4%BD%95-5-%E5%88%86%E9%92%9F%E5%86%85%E5%AE%9A%E4%BD%8D%E5%B9%B6%E4%BF%AE%E5%A4%8D)
    - [线程池饱和 + 重试风暴，如何协同治理？](#%E7%BA%BF%E7%A8%8B%E6%B1%A0%E9%A5%B1%E5%92%8C--%E9%87%8D%E8%AF%95%E9%A3%8E%E6%9A%B4%E5%A6%82%E4%BD%95%E5%8D%8F%E5%90%8C%E6%B2%BB%E7%90%86)
    - [线程池监控与告警：看哪些指标？阈值怎么定？](#%E7%BA%BF%E7%A8%8B%E6%B1%A0%E7%9B%91%E6%8E%A7%E4%B8%8E%E5%91%8A%E8%AD%A6%E7%9C%8B%E5%93%AA%E4%BA%9B%E6%8C%87%E6%A0%87%E9%98%88%E5%80%BC%E6%80%8E%E4%B9%88%E5%AE%9A)
    - [把并发策略整合落地：一段可直接用于面试的完整口语回答](#%E6%8A%8A%E5%B9%B6%E5%8F%91%E7%AD%96%E7%95%A5%E6%95%B4%E5%90%88%E8%90%BD%E5%9C%B0%E4%B8%80%E6%AE%B5%E5%8F%AF%E7%9B%B4%E6%8E%A5%E7%94%A8%E4%BA%8E%E9%9D%A2%E8%AF%95%E7%9A%84%E5%AE%8C%E6%95%B4%E5%8F%A3%E8%AF%AD%E5%9B%9E%E7%AD%94)
  - [Observability and Release](#observability-and-release)
    - [日志 / 指标 / 追踪（OTel / Prom / Grafana）](#%E6%97%A5%E5%BF%97--%E6%8C%87%E6%A0%87--%E8%BF%BD%E8%B8%AAotel--prom--grafana)
    - [SLO / 告警阈值 / 误差预算](#slo--%E5%91%8A%E8%AD%A6%E9%98%88%E5%80%BC--%E8%AF%AF%E5%B7%AE%E9%A2%84%E7%AE%97)
    - [发布与回滚（灰度 / 蓝绿 / 金丝雀）](#%E5%8F%91%E5%B8%83%E4%B8%8E%E5%9B%9E%E6%BB%9A%E7%81%B0%E5%BA%A6--%E8%93%9D%E7%BB%BF--%E9%87%91%E4%B8%9D%E9%9B%80)
  - [Kubernetes / Cloud-Native](#kubernetes--cloud-native)
    - [Pod & Container 基础：Pod 为什么是最小调度单位；资源请求/限制；重启策略；日志规范](#pod--container-%E5%9F%BA%E7%A1%80pod-%E4%B8%BA%E4%BB%80%E4%B9%88%E6%98%AF%E6%9C%80%E5%B0%8F%E8%B0%83%E5%BA%A6%E5%8D%95%E4%BD%8D%E8%B5%84%E6%BA%90%E8%AF%B7%E6%B1%82%E9%99%90%E5%88%B6%E9%87%8D%E5%90%AF%E7%AD%96%E7%95%A5%E6%97%A5%E5%BF%97%E8%A7%84%E8%8C%83)
    - [Service / Ingress / Gateway：流量路径；超时/重试/黏性会话；常见 502/504 排查口径](#service--ingress--gateway%E6%B5%81%E9%87%8F%E8%B7%AF%E5%BE%84%E8%B6%85%E6%97%B6%E9%87%8D%E8%AF%95%E9%BB%8F%E6%80%A7%E4%BC%9A%E8%AF%9D%E5%B8%B8%E8%A7%81-502504-%E6%8E%92%E6%9F%A5%E5%8F%A3%E5%BE%84)
    - [Probes 与优雅下线：startup/readiness/liveness 的边界；`preStop` + `terminationGrace`；预热与缓存](#probes-%E4%B8%8E%E4%BC%98%E9%9B%85%E4%B8%8B%E7%BA%BFstartupreadinessliveness-%E7%9A%84%E8%BE%B9%E7%95%8Cprestop--terminationgrace%E9%A2%84%E7%83%AD%E4%B8%8E%E7%BC%93%E5%AD%98)
    - [HPA 与自动扩缩容：CPU/内存 vs 自定义指标；`stabilizationWindow`、`scaleDownPolicy` 抖动治理；冷启动](#hpa-%E4%B8%8E%E8%87%AA%E5%8A%A8%E6%89%A9%E7%BC%A9%E5%AE%B9cpu%E5%86%85%E5%AD%98-vs-%E8%87%AA%E5%AE%9A%E4%B9%89%E6%8C%87%E6%A0%87stabilizationwindowscaledownpolicy-%E6%8A%96%E5%8A%A8%E6%B2%BB%E7%90%86%E5%86%B7%E5%90%AF%E5%8A%A8)
    - [配置与发布安全：ConfigMap/Secret 版本化与回滚；镜像不可变标签；RollingUpdate 参数；PDB 与 `drain`](#%E9%85%8D%E7%BD%AE%E4%B8%8E%E5%8F%91%E5%B8%83%E5%AE%89%E5%85%A8configmapsecret-%E7%89%88%E6%9C%AC%E5%8C%96%E4%B8%8E%E5%9B%9E%E6%BB%9A%E9%95%9C%E5%83%8F%E4%B8%8D%E5%8F%AF%E5%8F%98%E6%A0%87%E7%AD%BErollingupdate-%E5%8F%82%E6%95%B0pdb-%E4%B8%8E-drain)
    - [最小权限与身份（RBAC / OIDC / IRSA）：ServiceAccount 绑定最小权限；云资源精细授权；密钥不落盘](#%E6%9C%80%E5%B0%8F%E6%9D%83%E9%99%90%E4%B8%8E%E8%BA%AB%E4%BB%BDrbac--oidc--irsaserviceaccount-%E7%BB%91%E5%AE%9A%E6%9C%80%E5%B0%8F%E6%9D%83%E9%99%90%E4%BA%91%E8%B5%84%E6%BA%90%E7%B2%BE%E7%BB%86%E6%8E%88%E6%9D%83%E5%AF%86%E9%92%A5%E4%B8%8D%E8%90%BD%E7%9B%98)
  - [Full-Stack](#full-stack)
    - [React/TypeScript 基础最小面（函数组件/Hook、受控 vs 非受控、常用 TS 工具类型、错误边界）](#reacttypescript-%E5%9F%BA%E7%A1%80%E6%9C%80%E5%B0%8F%E9%9D%A2%E5%87%BD%E6%95%B0%E7%BB%84%E4%BB%B6hook%E5%8F%97%E6%8E%A7-vs-%E9%9D%9E%E5%8F%97%E6%8E%A7%E5%B8%B8%E7%94%A8-ts-%E5%B7%A5%E5%85%B7%E7%B1%BB%E5%9E%8B%E9%94%99%E8%AF%AF%E8%BE%B9%E7%95%8C)
    - [路由与表单（React Router v6、嵌套路由/懒加载、表单校验与数据流）](#%E8%B7%AF%E7%94%B1%E4%B8%8E%E8%A1%A8%E5%8D%95react-router-v6%E5%B5%8C%E5%A5%97%E8%B7%AF%E7%94%B1%E6%87%92%E5%8A%A0%E8%BD%BD%E8%A1%A8%E5%8D%95%E6%A0%A1%E9%AA%8C%E4%B8%8E%E6%95%B0%E6%8D%AE%E6%B5%81)
    - [SSR / CSR / 选择性水合（取舍与指标：TTFB/TTI/CLS；岛屿架构要点）](#ssr--csr--%E9%80%89%E6%8B%A9%E6%80%A7%E6%B0%B4%E5%90%88%E5%8F%96%E8%88%8D%E4%B8%8E%E6%8C%87%E6%A0%87ttfbtticls%E5%B2%9B%E5%B1%BF%E6%9E%B6%E6%9E%84%E8%A6%81%E7%82%B9)
    - [CSP / 缓存策略（CSP `nonce/hash`、Cache-Control/ETag、CDN/Edge/浏览器多级缓存）](#csp--%E7%BC%93%E5%AD%98%E7%AD%96%E7%95%A5csp-noncehashcache-controletagcdnedge%E6%B5%8F%E8%A7%88%E5%99%A8%E5%A4%9A%E7%BA%A7%E7%BC%93%E5%AD%98)
    - [Sentry 埋点与错误上报（前后端统一 TraceID、Source Map、错误分级与去噪）](#sentry-%E5%9F%8B%E7%82%B9%E4%B8%8E%E9%94%99%E8%AF%AF%E4%B8%8A%E6%8A%A5%E5%89%8D%E5%90%8E%E7%AB%AF%E7%BB%9F%E4%B8%80-traceidsource-map%E9%94%99%E8%AF%AF%E5%88%86%E7%BA%A7%E4%B8%8E%E5%8E%BB%E5%99%AA)
    - [环境变量管理（构建期 vs 运行期、公开变量白名单、CI/CD 注入、敏感信息不入包）](#%E7%8E%AF%E5%A2%83%E5%8F%98%E9%87%8F%E7%AE%A1%E7%90%86%E6%9E%84%E5%BB%BA%E6%9C%9F-vs-%E8%BF%90%E8%A1%8C%E6%9C%9F%E5%85%AC%E5%BC%80%E5%8F%98%E9%87%8F%E7%99%BD%E5%90%8D%E5%8D%95cicd-%E6%B3%A8%E5%85%A5%E6%95%8F%E6%84%9F%E4%BF%A1%E6%81%AF%E4%B8%8D%E5%85%A5%E5%8C%85)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

# 高频问题库

- **Java**：集合/并发（synchronized、Lock、CAS、线程池）、JVM（内存结构、GC）、异常与最佳实践
- **Spring**：IOC/AOP、RestController、Actuator、配置管理、事务/连接池
- **微服务 & K8s**：Deployment/Service/Ingress/HPA/探针、无状态、滚动发布与回滚、ConfigMap/Secret
- **AWS & 云原生**：EKS NodeGroup vs Fargate、ALB、IRSA/OIDC、ECR、S3、CloudWatch、AMP、Grafana
- **DevOps**：CI/CD（GitHub Actions OIDC）、Trivy、回滚策略、IaC（Terraform 后端与锁）、最小权限
- **SRE**：SLI/SLO/错误预算、MTTR、Chaos、容量与成本权衡
- **行为/英文**：冲突处理、推动落地、失败复盘、跨团队协作、影响力（每题准备一条 STAR）

---

## API Design & Reliability

### 契约清晰：资源建模 & 语义化接口（Contract Clarity）

> **契约清晰（资源建模/语义化 URL/统一字段语义/错误模型/可观测 Trace-ID）**：对外坚持 Canonical Model + OpenAPI/JSON Schema 校验，分页/排序/时间/金额/ID 统一约定，返回体可观测可排障；渠道差异放在内部映射层，外部只做向后兼容的增量字段。

**面试官：**
“你在跨境电商/库存中台里，如何把**商品/库存**这类核心对象建模，并通过**清晰稳定的 API 契约**让前端、第三方渠道（Shopify/WooCommerce 等）都能稳用？如果上游平台字段各不相同、且业务量在活动期飙升，你会怎么设计接口与返回结构？”（你可以结合你在深圳市凡新科技 & Michaels 的经历来回答）

**你：**

“我会先做一个**Canonical Model（规范化域模型）**，然后把各平台的字段映射进来，API 对外只暴露**我们的一致语义**。例如把 `Product`、`Variant`、`StockItem` 拆清，`/products/{id}`、`/variants/{id}`、`/stocks?variantId=...&channel=...` 用**语义化 URL**和**查询参数**表达资源与过滤。之所以坚持对外契约稳定，是因为我们的服务在活动期会到 **80k–150k req/day，峰值 \~1.2k QPS**，而且要保持 P95 < 140ms，所以**任何破坏性变更都会造成放大效应**。这在我现在的工作环境里是常态（AWS 上 6 个 Spring Boot 微服务 + 自动扩容），因此我会把契约做成**可文档化、可校验**的，比如 OpenAPI + JSON Schema，前后端都能对齐检查。”

“以**库存**为例，我会规定：

- **ID 与类型**：所有 ID 统一用字符串（避免某些平台 `variant_id` 的长整型在 JS 客户端精度丢失）；金额统一**分为最小货币单位**（如分）、**货币代码**单独字段；时间全用 **RFC3339 UTC**。
- **分页与排序**：`page/size/sort` 统一格式；对大列表返回 `nextCursor` 以便前端/任务稳定翻页。
- **并发读写**：读 API 返回 `ETag/Last-Modified`，写 API 支持 `If-Match` 做并发控制；结合缓存（我们线上用 **Redis Cluster + Aurora 只读副本** 做读写分离），读路径可控、延迟稳定。”

“在 **Michaels** 做电商与 MakerPlace 的 API 时，我们也坚持把**登录与用户域**契约化，比如 **JWT 轮换 + OAuth2** 统一安全语义，接口文档清晰，移动端/前端对接成本低；同时在性能上通过**索引与响应结构优化**把接口延迟打下来，证明**契约清晰**有助于定位与优化。 ”

“错误返回我会统一一个**错误模型**：

```json
{ "code": "STOCK_NOT_FOUND", "message": "Stock item not found", "requestId": "trace-id", "details": { "variantId": "v123", "channel": "shopify" } }
```

配合**Trace-ID** 贯通到 CloudWatch / X-Ray / Grafana，这对我们线上**快速定位**很关键（我们有完善的可观测和零停机发布流程）。”

追问 1（场景深挖）

**面试官：**“上游新增了一个渠道特有字段，比如 `shopify_location_id`，但你不想污染对外契约，怎么处理？”

**你：**

“我不会把渠道细节渗透进公开模型，而是：

1. **内部映射层**吸收它（Connector DTO）；
2. 对外契约只在**业务确实需要**且跨渠道有共同语义时才**增量添加**字段（只做向后兼容的**可选字段**）；
3. 对必须透传的极少数字段，用 `extensions.*` 命名空间承载，并在 OpenAPI 标注**非核心**。这样不破坏现有调用方，也避免**破坏性变更**在高峰期放大。”（与我们在活动期高 QPS 的稳定性目标一致。）

追问 2（项目落地）

**面试官：**“你在凡新或 Michaels 有没有因为契约不清导致事故？后来怎么改的？”

**你：**

“有一次库存批量同步的响应里，**金额字段单位**没写清，导致一个下游任务把分当元，差点误触发大额补货。后来我们把**金额强制最小单位 + 货币代码**写进 Schema，并在 CI 里做**契约校验**与**示例响应校验**；同时在库存批同步流程里也加了**幂等键与步骤化编排**（我们用 **Lambda + SQS + Step Functions** 重构这条链路，整体耗时也从 ~25min 降到 ~7min）。”

### 版本化策略：URI vs Header；向后兼容与下线流程

> **版本化（URI 大版本 + Header 可选；向后兼容优先）**：非破坏性演进留在同大版本，破坏性才切 v2；提供兼容层与双写验证；Deprecation/Sunset 通知 + 分版本监控 + 强制下线日程，做到“可见、可控、可回滚”。

**面试官：**
“你在（深圳市凡新科技/麦克尔斯深圳）做商品与库存接口时，有一次业务要从‘单仓数量’升级到‘多仓分布库存’。这对返回结构是**破坏性变更**：原来 `stockQuantity` 是一个整数，现在要返回 `warehouses[]` 明细。你会怎么做**版本化**？是放在 URL 里 `/v2/stocks`，还是用 `Accept: application/vnd.xxx+json;v=2` 的 Header？旧客户端还在跑，你怎么做到**平滑迁移**和**按期下线**？”

**你：**

“对**外部/多团队依赖**的 API，我优先选 **URI 大版本**（`/api/v1/...` → `/api/v2/...`），因为它**可见性强**、文档和路由隔离清晰，前端/第三方也最好理解。对于**内部 BFF 或同域微服务**之间，我会保留 **Header 版本**（`Accept: ...;v=2`），用 Spring 的内容协商把同一条路径映射到不同的 `produces`。
落地上我会遵循这几条：

1. **非破坏性演进**（新增可选字段、增加新接口）只在**同大版本**里做，比如在 v1 返回里增加 `warehouses`（可选），同时保留 `stockQuantity`；
2. **破坏性变更**（字段语义变化、枚举收缩等）才启 **v2** 路径；
3. **双写/影子读** 验证：服务内部先把多仓逻辑双写到新表/新索引，线上对 v2 做**小流量灰度**，对比指标与告警；
4. **治理与下线**：对 v1 返回 **Deprecation/Sunset** 头（例如 `Sunset: <日期>`），在 API 网关或 Ingress/Grafana 里**按版本打点**，当 v1 调用量 < X% 持续 Y 天，就发最后通知并**切 410**；
5. 期间提供一个**兼容层**：v2 服务对老客户端仍可回填 `stockQuantity = sum(warehouses[].qty)`，让迁移有缓冲期。
   在凡新那边做促销高峰时，这种‘大版本在 URL，小迭代在 Schema’的策略更稳；在麦克尔斯那边，移动端同学更喜欢**明确的路径版本**，他们升级 App 时能直观看到 v2。整体目标是让‘**破坏性只发生在大版本切换**’，其它都是**增量可兼容**。”

追问 1（深挖迁移计划）

**面试官：**“如果大量老客户端一时半会升级不了，导致你迟迟不能下线 v1 怎么办？”

**你：**

“我们会把**兼容层**做成**可配置的**：

- 先在 v2 内部保留一层**适配器**把 `warehouses` 聚合成 `stockQuantity` 返回给 v1 客户；
- 在 API 网关对 v1/v2 的**QPS、错误率、延迟**做**分版本监控**，并在每次版本公告后给出**采纳率**；
- 设一个明确的**日程线**：例如 90 天后进入‘降级窗口’，老版本只做**安全修复**不加新特性；180 天后**强制下线**（返回 410 + 链接到迁移文档）。
  这样我们既不拖累新版本的演进，也给合作方足够时间。”

追问 2（工程实现）

**面试官：**“Spring Boot 里你怎么同时支持 URI 版本和 Header 版本？”

**你：**

“实际做法是**对外统一用 URI 大版本**，对内需要时再开 Header 协商：

- 控制器层：`/api/v1/...` 与 `/api/v2/...` 各有路由；
- 若同一路径用 Header：在 `@RequestMapping` 的 `produces` 里声明 `application/vnd.renda.stock+json;v=1/2`，并配置 `ContentNegotiationStrategy`；
- OpenAPI 文档分**两个 group**（v1/v2）生成 swagger，CI 里对两套 **JSON Schema** 做**契约校验**与**向后兼容检查**（新增字段只能是可选、禁止删除/改义）。
  配合灰度和回滚开关，风险可控。”

### 鉴权与授权：JWT/OIDC、最小权限、Token 续期与旋转

> **鉴权与授权（JWT/OIDC，最小权限，续期与旋转）**：统一 OIDC，短寿命 Access + 旋转 Refresh + 撤销表；按 `scope/aud` 做最小权限；服务间用客户端凭证/临时凭证；区分 401/403 并可观测（traceId/指标/告警）。

**面试官：**
“你在（深圳市凡新科技 / 麦克尔斯深圳）做订单与库存 API 时，前端（Web/小程序/APP）和三方渠道都要访问。你怎么做**统一登录与鉴权**？具体到 **JWT/OIDC** 的落地细节、**最小权限**的授权设计、以及**Access/Refresh** 的**续期与旋转**，你怎么权衡安全与可用性？”

**你：**

“我们把**身份认证**统一到 OIDC（例如 IDP：Cognito/Keycloak/公司自建），客户端用 **Auth Code + PKCE** 获取 **短时 Access Token（JWT）** 和 **较长 Refresh Token**。服务端（Spring Boot）作为 **OAuth2 Resource Server** 校验 JWT 的签名与过期，走 **JWKs** 自动拉取公钥并做**缓存**。
**授权**层面，我坚持**最小权限**：

- 面向外部调用，用 **scope** 粒度（`product:read`、`stock:write`），避免一刀切的 `admin`；
- 面向内部微服务，采用 **audience（aud）** 与 **资源级/操作级**组合（比如只能改“库存”但不能改“价格”），把权限做成**可配置策略**（如基于角色/属性的 ABAC）。
  **续期**我用‘短 Access + 可旋转 Refresh’：Access 约 5–15 分钟，Refresh 7–30 天，**刷新时旋转**（旧 Refresh 立即失效），并把 **jti（令牌唯一 ID）** 写进**黑名单/撤销表**（Redis/DB），防止被盗用。
  我们线上有**并发与多设备**，所以刷新接口设计成**幂等**，只承认‘最新签发’的 Refresh；如果同一 Refresh 被重复使用，我会触发**全账户 Refresh 封禁**并发告警。
  对**服务间调用**，我们禁用‘人为生成的长寿命 Token’，而走**客户端凭证流**或云原生临时凭证（比如 IRSA 访问云资源），降低泄漏风险。
  最后，把**401/403** 语义分清（未认证 vs 已认证但无权限），错误体里带 **traceId**，利于排障。”

追问 1（深挖安全细节）

**面试官：**“如果 JWT 泄露，或者我们要**强制登出**某个用户，怎么让**本来‘自包含’不可撤销**的 JWT 立即失效？”

**你：**

“我们有两层方案：

1. **短 TTL 的 Access** + **Gateway 层的撤销列表**：把需要立刻失效的 `jti` 放到 Redis，API Gateway 或全局过滤器先查撤销表，命中就拒绝；
2. **旋转 Refresh** + **一次性使用**：刷新时颁发新的 Refresh，并把旧的标记为‘已消费’，如果旧的再次出现就判定可疑并封禁。
   这两层能把‘自包含 Token 不可撤销’的问题控制在可接受窗口内（几分钟级）。此外我们开启 **kid（key id）轮换**，密钥换代时能平滑过渡。”

追问 2（可观测与故障演练）

**面试官：**“如何观察和演练这个体系？”

**你：**

“指标我们会分三类：

- **认证失败率**（签名/过期/撤销命中）、**刷新成功率**、**刷新重放**告警；
- **授权拒绝率**（403）按 `scope`/`aud` 分维度；
- **JWKs 拉取与缓存命中率**、**IDP 延迟**。
  我们有**失效演练**（把某用户/某应用加入撤销表；把某把密钥下线），确认 401 生效、刷新被拒并且告警到位。”

追问 3（项目落地）

**面试官：**“能结合你在凡新/麦克尔斯的经验说个具体例子吗？”

**你：**

“促销高峰时，**库存写入**要打得很紧，我们把 `stock:write` 单独成 scope，并给三方渠道一个**只读**的 `stock:read`。有次某脚本用错了客户端凭据，触发了**403**，我们通过错误体里的 `traceId` 很快定位到是**权限维度**不对，不是程序 Bug。
另外一次移动端升级后，出现**Refresh 重放**，我们通过**旋转 + jti 撤销**挡住了重放，并把这一模式加入**风控告警**。这些属于真实环境里‘安全与可用’的平衡：尽量短的 Access + 自动刷新，搭配清晰的权限边界。”

### 幂等性：幂等键、PUT vs POST、重试安全

> **幂等性（POST 幂等键、PUT/DELETE 自幂等、回调按事件 ID 去重）**：服务端原子占位（Redis SETNX 或 DB 唯一键）+ 响应快照复用；设置幂等窗口 TTL；与**指数退避**配合避免重试风暴；异步链路用 **Outbox + 消费端幂等** 保证最终一致。

**面试官：**
“促销高峰里，用户可能**连点两次下单**，第三方支付/库存回调也可能**重复推送**。你在（深圳市凡新科技 / 麦克尔斯深圳）如何保证**不会重复创建**订单/扣减库存？你具体怎么设计**幂等键**、**返回语义**，以及**与重试策略的配合**？”

**你：**

“我把问题分两层：**写接口幂等** + **事件/回调幂等**。

- **写接口（客户端→我们）**：POST 创建类接口要求客户端带 `Idempotency-Key`（或我们在 BFF 生成），幂等键 = `method + path + canonical(body) + user/tenant` 的哈希。服务端先做 **原子占位**（Redis `SETNX`/DB 唯一键），抢到占位才执行业务；执行完把**响应快照**缓存起来（含状态码、关键字段）。后续同键请求直接返回**同一份响应**（201 或 200），而不是再执行业务逻辑。
- **事件/回调（他们→我们 / 我们→下游）**：以**事件 ID**当幂等键，消费者端先查 `processed_events`（DB/Redis）是否见过，没见过才处理并**原子写入**‘已处理’标记；见过就直接 ACK。

  语义上我会遵守：

1. **PUT/DELETE** 本身具幂等；
2. **POST** 通过 `Idempotency-Key` 做到‘**功能幂等** + **响应幂等**’；
3. 返回如果命中幂等缓存，带一个 `Idempotent-Replay: true` 的响应头，方便排障。
   这套在凡新那边的订单与库存写路径都落了地；在麦克尔斯那边，支付回调我们就是用**回调事件 ID**做幂等键的。”

追问 1（工程细节）

**面试官：**“你怎么避免并发条件下的**双写**？Redis 会不会不可靠？”

**你：**

“占位一定要**原子**且**可恢复**：

- Redis 用 `SET key value NX EX=ttl`，抢到才继续；执行完成把**响应摘要**写回同 key，值里存状态与必要字段。
- 如果担心 Redis 丢数据或需要强一致，我会在 DB 里建一张 `idempotency` 表（`idempotency_key` 唯一索引），业务在**同一事务里**插入占位记录并处理。并发下只有一个事务能成功，其它事务收到**唯一键冲突**后转为读已存在的响应摘要。
- **TTL（幂等窗口）** 按业务风险定，比如创建订单 24h，库存写入 1–3h。窗口内重复请求都命中缓存；窗口外按新请求处理。”

**（伪代码，面试口述用）**

```java
// before controller
String key = hash(method, path, canonical(body), userId);
if (redis.setNx(key, "PROCESSING", ttl)) {
    Result r = handleBusiness();  // do create order / stock deduct in tx
    redis.set(key, serialize(r), ttl);   // cache response snapshot
    return r;                            // 201 Created
} else {
    return deserialize(redis.get(key));  // replay same response (201/200)
}
```

追问 2（重试与错误语义）

**面试官：**“如果下游超时了你会怎么重试？怎么避免**重试风暴**？”

**你：**

“我把**幂等**和**重试**绑在一起设计：

- 客户端/任务统一用**指数退避 + 抖动**（如 200ms、500ms、1.2s…，上限 5–7 次）；
- 后端在返回体里给出**可重试与否**：`429/503` 搭配 `Retry-After`，**可重试**；`4xx` 里非瞬时错误**不可重试**；
- 幂等键保证重试**不会产生副作用**；
- 对**写链路的异步下发**（比如出库通知）用**事务外箱（Transactional Outbox）+ 队列**，消费端也按事件 ID 幂等；如用队列的 FIFO + 去重（SQS FIFO/内容去重）进一步降重。”

追问 3（项目落地）

**面试官：**“给我一个你真实遇到的例子。”

**你：**

“凡新那边在大促高峰，有用户在慢网环境**连点两次下单**，以前会出现两张‘相同订单’，后来我们把 BFF 统一生成 `Idempotency-Key`，落到后端做**占位 + 响应快照**，第二次直接重放响应，问题就没了。
在麦克尔斯那边，**支付平台回调**会在网络抖动时**重复推送** 3–5 次，我们用回调的 `eventId` 做幂等键，消费者先查‘已处理表’，见过就**幂等 ACK**，**不会重复扣款/更新**。这两处上线后，重复写导致的工单几乎归零，告警也更干净。”

### 限流-重试-熔断：客户端与服务端协同；退避策略；降级与快速失败

> **限流-重试-熔断**：入口令牌桶 + 服务内并发舱壁；只对幂等请求做**指数退避+抖动**（10% 重试预算）；**P95×1.5** 量级超时；错误/超时率阈值触发熔断与半开探测；读链路缓存降级、写链路排队受理；全链路观测 `429/5xx/熔断/重试`。

**面试官：**
“在大促或库存同步高峰时，你们的下游（支付、三方渠道、库存引擎）会抖动。你在（深圳市凡新科技 / 麦克尔斯深圳）如何**限流**、**重试**、**熔断**，既保护下游又保证整体体验？说说**策略与参数**，以及在 Spring Boot/K8s 里怎么落地？”

**你：**

“我把它当成一套‘**压力控制闭环**’：**入口限流 → 调用重试/超时 → 熔断与降级 → 指标与自愈**。

- **限流（服务端）**：网关/Ingress 层做**令牌桶**（每租户/每 API），比如：`RPS=200，突发=400`；服务内部再做**并发量阈值**（`maxConcurrent=200`），超出立刻**快速失败**返回 `429`，带 `Retry-After`。促销期我们会给**关键写接口**更严格的限额，读接口放宽。
- **限流（客户端/BFF）**：BFF 自身也做**本地并发限制**，避免把抖动放大成**重试风暴**。对同一个用户或同一个商品操作，我们会合并/串行化。
- **重试**：只对**幂等**的请求做，且**超时/503/429**才重试；使用**指数退避 + 抖动**（如 200ms、500ms、1.2s、2.5s、…，最多 5 次），并设**重试预算**（例如每分钟请求的 ≤10% 可用于重试），防止二次放大。
- **超时**：外呼都设置**硬超时**（P95×1.5 左右起步），避免线程长时间占用；不同下游不同超时，禁止“一个超时管天下”。
- **熔断**：错误率或超时率超过阈值（如 50% 且请求数≥N），**打开熔断**一段时间（比如 30–60s），期间直接失败；然后**半开**探测少量请求，恢复后再关闭。
- **降级**：读链路返回**缓存/近似值**（例如价格/库存读走 Redis 缓存 + 过期容忍），写链路则**排队/延后**（Outbox + 队列）并返回**受理中**；对非关键接口直接**功能降级**（如去掉次要字段/统计）。
- **观测**：分**版本/租户/接口**统计 `RPS、并发、429/5xx、超时率、重试次数、熔断状态`，并把**限流命中率**、**重试预算**暴露到仪表盘；做到‘哪里在保护、保护了多少’一目了然。

在 EKS + Spring Boot 的落地：网关（NGINX Ingress/ALB/API Gateway）做**硬限流**；服务内用 **Resilience4j**（或等价组件）做**TimeLimiter/Retry/RateLimiter/CircuitBreaker/Bulkhead**；部署层面给关键服务加**HPA + PDB + 合理的资源限制**，让自动扩容和限流协同而不是对冲。”

追问 1（具体数值与权衡）

**面试官：**“能给我一组你实际会用的参数吗？怎么权衡‘保护下游’和‘用户体验’？”

**你：**

“拿**库存读**举例：

- 网关限流：每租户 `RPS 200，突发 400`；服务内并发 `maxConcurrent 200`；
- 超时：P95 大约 80ms，那我会先设 150–200ms 的客户端超时；
- 重试：指数退避 + 抖动，最多 4–5 次；预算 10%；
- 熔断：统计窗口 10s，错误/超时率 >50% 且调用数≥50，打开 30s；半开 10 个请求探测；
- 降级：命中熔断即读缓存（可过期 30–60s），返回‘库存大于 0/未知’这类**弱一致**信息，并提示页面做**轻提示**。
  促销时我们宁可**严格保护写链路**（下单/扣减），把失败暴露得更明显一些，也不要把下游打挂导致**全局雪崩**。读链路能容忍短期不准，写链路尽量排队或受理中。”

追问 2（如何避免重试风暴）

**面试官：**“如果某个依赖刚好抖了 2–3 分钟，所有客户端一起重试会不会压塌你们？”

**你：**

“我们做了三件事：

1. **重试预算**：全局限制重试占比 ≤10%，一旦达到预算，就**不再重试**直接快速失败；
2. **抖动**：退避时间随机化，避免同一时刻同步重试；
3. **排队/背压**：服务内并发满载时直接拒绝（429），并把 `Retry-After` 写清楚；异步链路用队列限速消费。
   此外，**观测层**有‘**异常重试速率**’的告警，看见就先把重试预算降下来，必要时临时调低入口 RPS。”

追问 3（项目落地&复盘）

**面试官：**“有没有真实的事故和你们的改进？”

**你：**

“凡新那边有次第三方库存端点在半夜抖了 5 分钟，早期我们没做重试预算，导致重试量把线程池打满。复盘后我们引入**预算 + 并发舱壁**（Bulkhead），并把**缓存降级**做成开关，一键打开后 P95 直接回落。
在麦克尔斯那边，MakerPlace 图片处理链路偶发慢，我们把**超时**从 3s 切到 1.5s 并加熔断，前端降级为**低清图占位**；随后把慢服务拆到**独立队列**限速消费，线上体验稳定了。”

### 错误码与可观察性：统一错误模型 / Trace-ID / 指标-日志-链路关联

> **错误码与可观察性**：统一错误模型（`code/message/traceId/details`）+ 明确 4xx/5xx 映射；W3C trace 贯穿响应体/日志/指标；结构化日志携带 MDC（traceId 等）；RED 指标联动 APM；采样对 5xx/高延迟强制保留；日志脱敏与告警基线到位。

**面试官：**
“促销高峰里，你们的下单接口间歇性报错。客服只拿到一条报障：‘结算失败，请稍后重试’。你怎么通过**统一错误码**和**可观测性**（日志、指标、分布式追踪）**迅速定位**是用户错误还是服务端问题？结合你在（深圳市凡新科技 / 麦克尔斯深圳）的实践讲讲。”

**你：**

“我会把‘错误→定位’做成**一跳直达**：

1. **统一错误模型**让前端/客服拿到**可报障信息**：

```json
{ "code":"PAYMENT_TIMEOUT", "message":"Payment timeout, please retry",
  "traceId":"8a3c60f7…", "hint":"retry after 3s",
  "details":{ "orderId":"o123", "gateway":"stripe" } }
```

2. **Trace-ID** 贯穿**响应体 + 日志 + 指标 + 链路追踪**：客服把 `traceId` 给我们，我们在日志平台/可观测平台（EKS 上的 OTel/ADOT→Prometheus/Grafana/CloudWatch/X-Ray）就能**直跳到那条请求**。
3. 指标采用 **RED 法**（Rate/Errors/Duration）：先看该接口 `5xx/4xx` 比例和 P95 是否异常，再通过 `traceId` 打开**分布式链路**定位慢点或失败点（比如外呼支付网关超时 vs 我们的校验 400）。
   在凡新的库存/订单链路，我们这样能把‘用户填错地址’（**4xx**）和‘支付网关波动’（**5xx/超时**）几分钟内区分清楚；在麦克尔斯的 MakerPlace，我们把 `traceId` 展示在移动端错误页里，客服直接抄给我们。”

追问 1（设计与规范）

**面试官：**“错误码你怎么分层？HTTP 状态码和业务码怎么配？”

**你：**

“**HTTP 只表达通用语义**，**业务码表达具体原因**：

- 4xx：用户侧/可预期错误，比如 `VALIDATION_FAILED`、`AUTH_REQUIRED`、`RATE_LIMITED`；
- 5xx：服务侧/依赖侧，比如 `UPSTREAM_TIMEOUT`、`DB_DEADLOCK`、`STOCK_INCONSISTENT`。
  **映射规则**固定：`400/401/403/404/409/422/429` 用于常见场景；`500/502/503/504` 对应服务端/依赖错误。错误体统一四段：`code/message/traceId/details`；**message 面向人类**（可本地化），`code` 面向程序，`details` 放**可排障字段**（不含敏感信息）。
  另外我们会规定：**相同幂等键的重放**返回同一业务码，并在响应头加 `Idempotent-Replay:true`，排障更清楚。”

追问 2（工程落地）

**面试官：**“在 Spring Boot / K8s 上，你怎么把 Trace-ID 贯穿并串起‘指标-日志-链路’三件事？”

**你：**

“落地分三步：

1. **链路追踪**：W3C `traceparent` 头透传（网关→BFF→微服务→下游）；未携带则在网关生成。后端用 **OpenTelemetry SDK** 自动注入 span。
2. **结构化日志**：日志全用 JSON，`traceId/spanId/tenant/userId` 写进 **MDC**，日志 Appender 自动带上：

```java
// in a filter
String traceId = currentTraceIdOrGenerate();
MDC.put("traceId", traceId);
// controller logs will carry it; also return in body/header
```

3. **指标关联**：在计时器/计数器（Micrometer）上打 `api=placeOrder, outcome=success|error, http_status=...` 等标签；Grafana 面板支持按 `traceId` 链接到 APM，形成**点开指标→跳到具体 trace→再看相关日志**的三连。
   采样方面：默认 **概率采样**（如 5–10%），对 `5xx`/高延迟请求**强制采样**，确保关键问题有完整 trace。”

追问 3（质量与风控）

**面试官：**“怎么避免把隐私或安全数据打到日志？告警怎么设？”

**你：**

“我们做了两件事：

- **日志脱敏/拦截**：统一的 `LogSanitizer` 过滤 `password/token/card/email` 等字段；**从不**把 Authorization、JWT、银行卡号写日志；错误体的 `details` 也做白名单。
- **告警基线**：
  - SLO：如下单成功率 99.5%，**误差预算**消耗≥5% 触发告警；
  - `5xx` 比例、`429` 命中率、P95 超过阈值持续 5 分钟；
  - **错误码分布**偏移（例如 `PAYMENT_TIMEOUT` 激增）触发调度，第一时间看依赖健康与熔断状态。
    这样既安全又可定位。”

追问 4（真实案例）

**面试官：**“讲个你遇到的真实定位案例。”

**你：**

“凡新一次活动夜里，`/checkout` 的 `5xx` 抬头。我们从 Grafana 上看到 `UPSTREAM_TIMEOUT` 占比升高，随手点进一个 trace，发现**支付网关调用 3s 超时**。同时日志里同一个 `traceId` 显示我们内部处理只有 40ms——很快就定位到是**外部依赖抖动**，切开关走**降级队列**并调低重试预算，几分钟内恢复。
麦克尔斯那边有次是**4xx**暴涨，错误码 `VALIDATION_FAILED`，trace 里显示是 `postalCode` 校验新规则上线，回滚后即恢复。统一错误码 + trace 贯穿真的省了太多时间。”

### 灰度发布与回滚：逐步放量 / 健康检查 / 一键回滚

> **灰度与回滚**：滚动/金丝雀/蓝绿按风险选型；金丝雀 1%→5%→25%→50%→100%，以 `5xx/4xx/P95/资源/错误码分布` 设守卫，失败自动回滚；探针 + preStop 优雅下线；DB 走 **expand→migrate→contract**，特性开关解耦交付与发布；发布即实验、指标闭环。

**面试官：**
“你在（深圳市凡新科技 / 麦克尔斯深圳）做库存与下单链路时，如何把**高风险改动**安全上线？比如新版本涉及**缓存策略调整 + 一个字段语义变化**，你怎么做**灰度策略**、**健康检查**、以及**快速回滚**？”

**你：**

“我的思路是‘**发布即实验**’：把每次上线当成带监控的实验，**小流量试水 → 指标达标再放量 → 不达标立即回滚**。

- **发布策略选择**
  - **滚动更新（K8s 默认）**：常规小改动；`maxUnavailable=0、maxSurge=25%`，确保无损切换。
  - **金丝雀（Canary）**：有**性能/缓存语义**风险时，用 1%→5%→25%→50%→100% 分阶段放量；按 **4xx/5xx、P95、错误预算燃尽率** 设**自动阻断/回滚**阈值。
  - **蓝绿（Blue/Green）**：涉及**大版本或依赖升级**时，用全量双环境；流量开关一键切回旧环境。
  - **特性开关（Feature Flag）**：把业务开关与**交付开关解耦**，先灰度代码，再灰度开关。
- **健康检查与就绪**
  - **Startup/Readiness/Liveness** 三探针分工明确：Startup 给冷启动、Readiness 挡流量、Liveness 防僵死；
  - **preStop 钩子 + 优雅下线**（如 `sleep 5`），配合 Ingress/ALB 目标组的健康阈值，避免**半关闭阶段**丢请求；
  - **只读路径**先验证（GET/查询），**写路径**在金丝雀阶段**缩小配额**并捆绑幂等等保护。
- **回滚**
  - **一键回滚**：Helm `rollback` / Argo Rollouts `abort` / 网关流量权重回拨；
  - **数据向后兼容**：DB 走 **expand→migrate→contract** 两阶段，确保旧版本还能工作；
  - **事后验证**：回滚后继续观察 10–30 分钟，确认指标归位。”

追问 1（数据库与契约同步）

**面试官：**“如果涉及数据库表结构变更，你怎么保证灰度与回滚不被数据库‘卡住’？”

**你：**

“我严格走**向后兼容的双阶段**：

1. **Expand**：先上线 v1.5，加**新列/新表**（可空）与**写入双写**，读路径仍用旧列；
2. **Migrate**：异步回填/对账，监控读取命中率与差异；
3. **Switch**：新版本开始读新列，但**旧列仍保持实时同步**；
4. **Contract**：确认稳定（通常一到两周）才删除旧列。
   这样**任何阶段都可回滚**到旧版本；契约层面只做**向后兼容**的新增字段，不做破坏性变更。”

追问 2（阈值设定与自动回滚）

**面试官：**“你会设置哪些‘自动回滚’阈值？”

**你：**

“按**金丝雀批次**设硬阈值，例如每 5 分钟窗口：

- **错误率**：`5xx > 1%` 或 `4xx（非校验）> 3%` 立即阻断；
- **延迟**：`P95 较基线上浮 > 30%` 阻断；
- **资源**：`CPU>80% & GC 暴涨` 或 `容器重启` 超阈值阻断；
- **错误码分布**：`PAYMENT_TIMEOUT`、`STOCK_INCONSISTENT` 异常抬头即阻断。
  阻断后：**自动回滚 + 触发告警 + 开启降级**（读走缓存、写排队）。这些阈值在凡新大促和麦克尔斯的 MakerPlace 我都踩过坑，后来固定为发布模板的一部分。”

追问 3（工程落地：K8s / Spring Boot）

**面试官：**“能给我一套落地做法吗？”

**你：**

“K8s 侧：

- Deployment：`rollingUpdate.maxSurge=25%、maxUnavailable=0`；配 PDB 保证有**最小可用副本**；
- 探针：`startupProbe` > `readinessProbe` > `livenessProbe`，超时阈值分开设；
- HPA：根据 `CPU/内存/自定义 QPS` 指标自动扩，但**不代替**限流；
- 渐进式交付：**Argo Rollouts/Flagger** 配 `setWeight` 与 `metric` 守卫，失败自动 `abort`；
- 网关：ALB/NGINX 做**权重路由**；金丝雀阶段把**写操作**路由比例更低。

应用侧：

- Spring Boot 加**启动预热**（连接池/缓存）与**优雅停机**；
- 加**版本与commit id**到 `/actuator/info`；
- 监控面板预置‘上线看板’：金丝雀 vs 基线对比（RPS/5xx/4xx/P95/CPU/重启数）。这样**发布即实验**就能闭环。”

追问 4（真实落地）

**面试官：**“说个你当时印象深刻的灰度/回滚案例。”

**你：**

“凡新有次把**库存写入缓存策略**改成‘写后淘汰’，金丝雀 5% 时 `STOCK_INCONSISTENT` 错误码抬头且 P95 拉长，我们立即**abort 回滚**，切回旧策略，随后排查发现是**并发回写与淘汰顺序**导致短暂不一致；修复后再灰度就稳了。
麦克尔斯那边，移动端依赖的**图片处理服务**升级了底层库，1% 金丝雀就发现**内存泄漏 + 容器重启**，我们一键回滚并把这条链路拆出**独立队列限速**，再上线就顺滑了。”

---

## DB & Cache

### 索引选型与失效（B+Tree、组合索引、覆盖索引、左前缀）

> **索引选型与失效**：等值在前、范围在后，`(user_id, status, created_at DESC)` 覆盖列表页；避免函数包列与隐式转换；选择度差用**组合索引**提升；热查询必要时**拆专用索引**，不用指望索引合并；用 `EXPLAIN` 盯 `rows/key/filesort`。

**面试官**

“你在（深圳市凡新科技 / 麦克尔斯深圳）做订单与库存查询时，最常见的一条查询长这样：
按 `user_id + status` 过滤，按 `created_at DESC` 排序，取最近 20 条。偶尔还会加上 `channel`（Shopify/WooCommerce）。你会怎么设计**索引**？在什么情况下**索引会失效**，你怎么避免？”

**你**

“我会直接上一个**组合索引**：`(user_id, status, created_at DESC)`。

- 这样 `WHERE user_id=? AND status=?` 命中**最左前缀**，`ORDER BY created_at DESC LIMIT 20` 能做到**顺序读**，几乎不用再做 filesort。
- 如果查询只返回列表页字段（比如 `order_id, total, created_at, status`），我会把这些字段也纳入**覆盖索引**（只要都在二级索引上，查询就**不回表**）。
- 如果经常用到 `channel` 维度，我会根据**查询比例**和**选择度**决定是否把索引改成 `(user_id, channel, status, created_at DESC)`，或者再开一条以 `channel` 为第二位的索引（避免一个索引承担所有模式）。
- 文本模糊搜索我不会指望 B+Tree，`LIKE '%kw%'` 这种我会用**倒排/全文索引**或**ES**，避免全表扫。”

“**可能失效的坑**我会提前规避：

- **函数/计算包住列**：`DATE(created_at)=?` 会让索引失效，我改成 `created_at >= '2025-09-01 00:00:00' AND < '2025-09-02 00:00:00'`。
- **隐式类型转换**：`user_id` 是 `BIGINT`，但参数传字符串，可能走不到索引，我会在 DAL 层**强类型**。
- **范围列放前面**：`WHERE created_at > ? AND status = ?`，如果把 `created_at` 放在索引前面，后面的列就用不上索引了；所以把**等值列在前，范围列靠后**。
- **选择度差**：`status` 只有 3 个值，单列索引没意义，要靠 `user_id + status` 的组合来提高选择度。
- **回表过多**：遇到‘扫 10 万行再回表’的情况，我会把列表页必要字段放进覆盖索引里，或者改成**先查主键再回表**的两段式。”

“上线前我会用 `EXPLAIN` 看 `type`、`rows`、`key` 和 `Using index/Using filesort`；`rows` 过大就说明选择度不行。慢查询里还会看**扫描行数**与**回表次数**，必要时调整索引顺序或再拆一个更贴近热查询的索引。”

追问 1（实战细节）

**面试官：**“如果同样的查询，偶尔要先按 `channel` 过滤再按 `user_id` 呢？你是加一个新的 `(channel, user_id, status, created_at)` 还是用索引合并？”

**你：**

“我一般会**加一条新索引**，不要指望索引合并（AND 合并常常不稳定且代价高）。判断标准是**这条访问模式的占比**是否值得一条新索引。如果比例不高，我会把这类查询**引导到报表库/异步计算**，避免在 OLTP 上堆太多索引。”

追问 2（排序与覆盖）

**面试官：**“`ORDER BY created_at DESC` 一定能用上索引排序吗？什么情况下会退化成 filesort？”

**你：**

“有两个常见退化点：

1. **排序列不在索引的连续前缀里**（或者方向不一致）；
2. **查询列不被索引覆盖**且回表顺序与排序不一致，优化器可能选择 filesort。
   所以我会把 `created_at DESC` 放在组合索引的最后一位，并尽量让列表页**覆盖索引**，这样基本避免 filesort。”

追问 3（真实案例）

**面试官：**“讲一个你线上遇到的‘索引失效’问题。”

**你：**
“有一次活动页报慢，我们发现 SQL 写了 `WHERE DATE(created_at)=CURDATE()`，直接把索引废了；改成半开区间后 P95 从 900ms 掉到 80ms。还有一次是 `user_id` 参数是字符串，发生了**隐式转换**，修掉参数类型后 `rows` 从十几万降到几百。”

### 事务与隔离级别：RC / RR；MVCC 的一致性读 vs 当前读；如何避免幻读与超卖

> **事务与隔离**：读走 **RC + 一致性读** 提并发；写用**条件更新**（`UPDATE … WHERE qty>=?`）或**当前读 + 明确锁**保正确；RR 防幻靠 Next-Key，但更易死锁；唯一约束/插入即占位 > 锁；控制**短事务、统一加锁顺序、失败可重试**。

**面试官**

“你在（深圳市凡新科技 / 麦克尔斯深圳）做下单与库存扣减时，既要抗高并发，又要保证**不超卖**、**不出现脏数据**。MySQL/InnoDB 的默认隔离级别是 **可重复读（RR）**，很多团队又把线上切到 **读已提交（RC）** 提升并发。你会怎么选？具体到 **MVCC 的一致性读** 和 **当前读（锁定读）** 语义，你怎么落地？”

**你（口语化回答示范）**

“我把读写分两类：

- **一致性读（snapshot read）**：普通 `SELECT`，用 MVCC 读历史快照。RR 下‘同一事务多次查到的是同一份视图’，避免**不可重复读**；RC 下‘每条语句读最新已提交’，更‘新鲜’，但可能前后两次结果不同。
- **当前读（locking read）**：`SELECT … FOR UPDATE/ FOR SHARE`、`UPDATE/DELETE`，会加锁。RR 下会用**间隙锁/Next-Key 锁**来防止范围内的**幻读**；RC 下默认不加间隙锁，更多用**记录锁**，并发更高，但范围查询更容易出现幻写/插入竞争。

**我的选择**：

- **读路径**（列表/详情/报表）多用 **RC + 一致性读**，减少锁冲突，实时性也更好；
- **写路径**（下单扣减/配额/优惠券核销）不用纠结隔离级别，直接用 **‘条件更新’原子写法**或**当前读 + 明确锁**来保证正确性。”

**避免超卖的两种写法**

1. **条件更新（推荐）**：

```sql
UPDATE stock
SET qty = qty - :n
WHERE sku = :sku AND qty >= :n;
-- 受影响行数 = 1 才算成功；=0 表示库存不足
```

- 优点：不需要先 `SELECT FOR UPDATE`，天然**原子**，在 RC/RR 下都安全，锁粒度小、冲突少。
- 适用于“数值扣减”类场景（库存、配额、次数）。

2. **当前读 + 再写**：

```sql
START TRANSACTION;
SELECT qty FROM stock WHERE sku=:sku FOR UPDATE; -- 锁住这行（RR 下还会锁间隙）
-- 校验与扣减
UPDATE stock SET qty = qty - :n WHERE sku=:sku;
COMMIT;
```

- 适合需要**读取多个字段**再决定写入的业务；
- RR 下能靠 Next-Key 防**幻写**，但更可能**死锁**，需要**重试机制**与**短事务**控制。

**常见坑与我的做法**

- **幻读/重复写**：RC 下范围条件的 `SELECT … FOR UPDATE` 不锁间隙，可能插入“新纪录”造成幻影 → 改成**条件更新**或**唯一约束 + INSERT IGNORE/ON DUPLICATE KEY** 防重复。
- **长事务**：RR 下长事务会让 MVCC 的 undo 链变长，吞吐变差、历史回收受阻 → 严控**事务边界**，把计算挪到事务外；UI 上避免“开事务等用户操作”。
- **死锁**：并发写相同/相邻键很常见 → 统一**加锁顺序**（按主键/索引顺序）、**缩短事务**、**减少回表**（覆盖索引），并在代码里**捕获死锁并指数退避重试**。
- **隐式类型/函数包列**导致索引失效 → 在写路径尤其要**强类型**和**区间写法**。

**面试官追问 1**

“如果要‘按条件占位’避免同一用户重复下单，你选 RR 还是 RC？”

**你**

“都可以，我更倾向**唯一约束**+**插入即占位**，规避隔离级别差异：

```sql
-- 唯一键 (user_id, order_uuid)
INSERT INTO orders(user_id, order_uuid, status, ...)
VALUES(:u, :oid, 'PENDING', ...)
ON DUPLICATE KEY UPDATE touched_at = NOW(); -- 幂等
```

或者对核销码/优惠券也是**唯一约束**来杜绝重复消费，比锁更干脆。”

**面试官追问 2**

“实际线上你怎么定位隔离级别相关的问题？”

**你**

“我会：

- 打开 `EXPLAIN` 看是否走到**覆盖索引**，减少锁范围；
- 用 `performance_schema`/慢日志看**等待事件**（锁等待/扫描行数）；
- 碰到死锁，抓 `SHOW ENGINE INNODB STATUS`，审计‘谁先锁了谁’→ 调整**加锁顺序**或改成**条件更新**；
- 对高争用表建**合理分区/热点拆分**，降低冲突；
- 定期巡检**长事务**、**历史版本长度**，看到异常就查出是哪个业务把事务拉得太长。”

**真实案例**

“凡新那边高峰期遇到过**优惠券重复核销**风控误判，我们一开始用 `SELECT … FOR UPDATE` 查是否核销过，再写入；在 RC 下因为不锁间隙，边界条件下出现并发插入比赛的窗口。后来改成**唯一索引 + 插入即占位**，问题消失。
在麦克尔斯那边，做**库存预留**时从 RR 改 RC 后，读写冲突少了一半，但我们把关键写全部改为**条件更新模式**，再配幂等与重试，稳定住了。”

### 慢查询定位：执行计划、扫描行数、回表/下推、坏味道清单

> **慢查询定位**：用慢日志与 `EXPLAIN ANALYZE` 定位“扫描多/回表多/排序慢/锁等待”；组合索引对齐过滤+排序并**覆盖**；谓词改写（半开区间、强类型、去函数/OR）；大分页用**seek**，Join 用**小表驱动**。

小清单

- `EXPLAIN ANALYZE` 看真实耗时；盯 `type/key/rows/Extra`
- 组合索引：等值在前、范围在后；排序方向一致；尽量覆盖索引
- 谓词改写：半开区间、强类型、去函数包列/避免前置 `%`
- 分页：大页改**seek**；Join：小表驱动大表
- 复盘：慢日志 + rows_examined + 统计信息更新

**面试官**

“你在（深圳市凡新科技 / 麦克尔斯深圳）遇到一个接口偶发 900ms+：按 `user_id + status` 过滤，`ORDER BY created_at DESC LIMIT 20`。请你**在线上**快速判断瓶颈在哪，并说说你会如何**定位 → 证实 → 修复**？”

**你**

“我有一套**五步法**，基本两三分钟就能把方向定下来：

**① 先用慢日志/性能面板确定是‘DB 内慢’，还是‘网络/应用层慢’**

- 看 `rows_examined`、`query_time`、`lock_time`；如果 `query_time` 高但 `rows_examined` 很低，多半是**锁/等待**；如果 `rows_examined` 很大，就是**扫描多**。

**② 上 `EXPLAIN`（MySQL 8 用 `EXPLAIN ANALYZE`）看真实代价**

- 关注 `type`（`ALL`/`range`/`ref`/`eq_ref` 越靠后越好）、`key` 是否命中正确索引、`rows` 估算值、以及 `Extra`：
  - `Using index`（覆盖）、`Using where`（回表筛）、`Using filesort`、`Using temporary`、`Using index condition`（ICP 下推）。
- `EXPLAIN ANALYZE`还能看到每步实际耗时与行数，能直接证伪“是不是排序/回表慢”。

**③ 检查‘坏味道’**

- `LIKE '%kw%'`、`DATE(created_at)=…`/对列做函数、隐式类型转换（字符串比 BIGINT）、`OR` 把索引打散、**范围列放在组合索引前**、`ORDER BY` 顺序/方向与索引不一致、`SELECT *` 导致**回表**、大 `OFFSET` 分页。

**④ 快速修法**（优先不改业务）

- **索引对齐**：按这个查询我会用 `(user_id, status, created_at DESC)` 并让列表页字段尽量**覆盖索引**；
- **改写谓词**：把 `DATE(created_at)` 换半开区间；把字符串参数转成强类型；
- **排序与分页**：若仍有 `filesort`，用**覆盖+方向一致**避免；超大分页改为**游标/seek**：`WHERE (created_at, id) < (?, ?) LIMIT 20`；
- **Join 顺序**：小表驱动大表，必要时加 hint；
- **统计信息**：更新表/索引统计，避免优化器走岔路。

**⑤ 复核与回归**

- 再跑 `EXPLAIN ANALYZE` 与压测，确认 P95/P99 回到目标；把修复写进‘索引设计规范’和‘SQL 代码评审清单’。”

追问 1（动手演示）

**面试官：**“就拿你这条 `user_id+status` 的查询，EXPLAIN 看到了 `Using filesort`，而且 `rows` 很大，你怎么落地修？”

**你：**

“我先看索引：

- 如果当前只有 `(user_id)`，我会改成**组合索引** `(user_id, status, created_at DESC)`；
- 列表页只用 `order_id, total, status, created_at`，我把这些字段也放进索引，形成**覆盖索引**；
- 再跑 `EXPLAIN ANALYZE`，目标是看到 `type=range/ref`、`Using index`，没有 `Using filesort`。
  必要时把 `LIMIT 20` 的大偏移分页改成**seek 分页**，配合 `(created_at DESC, id DESC)` 复合排序键。”

追问 2（索引失效的真实坑）

**面试官：**“说个你遇到的‘一行代码让索引失效’的例子。”

**你：**

“典型就是 `WHERE DATE(created_at)=CURDATE()`，或者把 `user_id` 当字符串传，直接**全表扫**。线上修法：立刻改半开区间/强类型，P95 直接从百毫秒量级掉回两位数。”

追问 3（回表与下推）

**面试官：**“怎么判断是**回表**拖慢还是**排序**拖慢？”

**你：**

“看 `EXPLAIN ANALYZE` 的每步耗时：

- 如果 `Using index condition` + `rows` 很大但 `table` 回主键花时高，是**回表**多 → 做覆盖索引/减少列；
- 如果 `Using filesort` 的耗时高，说明**排序**是瓶颈 → 调整索引顺序与方向，或改游标分页。”

追问 4（大表 join）

**面试官：**“两张大表 join 变慢呢？”

**你：**

“先确保**连接键都有索引**，让 `type` 到 `ref/eq_ref`；用**小表驱动大表**（或子查询先裁剪大表），避免 `ALL`；必要时做**中间结果落地**或用 **覆盖索引 + 主键回表**两段式。”

### 读写分离的坑：主从延迟、读旧值、强一致读 / 亲和策略

> **读写分离**：读旧值的解法 = **读亲和 + 主读回退 + 延迟感知**；配 `read_token`（GTID/位点）实现 **read-your-writes**，超时回主；延迟异常时关键读走主、非关键读走缓存并提高 TTL；有开关/权重剔除与监控闭环。

**面试官**

“你在（深圳市凡新科技 / 麦克尔斯深圳）做读写分离后，用户刚下单立刻点订单详情，却偶发‘查不到’或看到旧状态。你怎么**稳定地做到 read-after-write**？当复制延迟上来时你会怎么**降级**？”

**你**

“读写分离最大的坑就是**主从延迟**带来的‘读旧值’。我一般从三层下手：**路由策略、读一致性令牌、退化/降级**。”

1. **路由策略（亲和 + 回退）**
   - **读亲和（stickiness）**：同一用户/会话的读请求在短窗口内（如 60–120s）**固定到同一只从库**，减少随机命中落后副本的概率。
   - **主读回退**：写后**关键读**（订单详情、支付状态）直接读主库；或先读从库，若未命中/版本低则**自动回读主库**。
   - **延迟感知**：每只从库上报 `Seconds_Behind_Master` / 复制位点；超过阈值（如 >2s）就**暂时摘出读池**，或降低权重。
2. **读一致性令牌（read token / GTID bookmark）**
   - **写入时打戳**：拿到 **GTID**（或 binlog 位点）作为 `read_token` 填到响应/上下文（也可放 Redis）。
   - **读时阻塞 / 选择副本**：路由层要求副本执行进度 ≥ token；MySQL 8 可用 `WAIT_FOR_EXECUTED_GTID_SET(token, timeout)`；超时则**回主库**。
   - 这样能保证 **read-your-writes**：同一用户在写后的第一次关键读取一定看到自己的最新数据。
3. **退化与降级**
   - **功能退化**：当延迟异常时，把“非关键读”走缓存或展示“已受理，稍后刷新”；**关键读**强制走主库。
   - **缓存配合**：写路径**先写 DB 再删缓存**（或消息通知回源）；延迟高时**提高临时 TTL**，避免击穿把主压垮。
   - **开关与监控**：有‘**主读开关**’与‘**从库剔除**’开关；仪表盘盯 `延迟/主读比例/回主次数/P95`，超阈值自动切策略。”

“在凡新做活动高峰时，我们对**订单/支付**这类链路默认‘**写后首次读→主库**’，且带 **`read_token`**。在麦克尔斯，MakerPlace 的作品发布后立即查看也走同样逻辑；非关键读（列表/统计）则尽量留在副本，保证总体吞吐。”

追问 1（工程细节）

**面试官：**“你在 Java/Spring 中怎么实现 `read_token` 这一套？”

**你：**

“写请求返回时把 `read_token` 放到响应头/Body；BFF 持久化到用户会话。读请求前在路由拦截器里：

1. 如果携带 token → 先挑**进度足够**的从库；
2. 没有副本满足或超时 → **回主库**；
3. 成功后更新‘该用户→该副本’的亲和映射（短 TTL）。

同时在 MyBatis/JPA 的数据源路由里支持 `forcePrimary()` 标记，让关键接口显式声明强一致读。”

追问 2（复制延迟来源与治理）

**面试官：**“延迟经常是谁拖的？你怎么治？”

**你：**

“常见原因：**主库长事务/大批量写**、从库 I/O 慢、网络抖动、从库查询过重。治理：

- 严控**长事务**和大批量 DDL/DML，拆批次；
- 从库只做**只读**业务，不跑重查询；
- 监控位点滞后，自动**摘除落后副本**；
- 跨区/跨地域复制尽量前置**异地只读场景**，关键链路避免跨地域强一致要求。”

追问 3（真实案例）

**面试官：**“说个你线上遇到的‘读旧值’事故。”

**你：**

“凡新某次‘下单后立刻查看’返回了未支付状态，是因为随机打到**落后 3–4 秒**的从库。上线 **`read_token` + 主回退** 后，关键读不再随机；另外把延迟阈值从 2s 调到 1s，并把那只从库自动降权，问题就消失了。
在麦克尔斯，发布作品后列表没及时刷新，我们加了**列表读缓存 + 变更消息**去**主动失效**缓存，同时把**首次查看走主**，体验就稳定了。”

### 缓存一致性：Cache-Aside 双删顺序、消息通知/回源、热键与热点保护

> **缓存一致性**：写库→删缓存（必要时**延时双删**/CDC 消息）；读 miss 用**互斥回源** + **逻辑过期** + **TTL 抖动**；热键用**本地+远端两级缓存**、**限速重建**与**预热**；值带 **version/etag**，Lua 原子“新旧值比较”防回灌，消费者幂等保证最终一致。

**面试官**

“你在（深圳市凡新科技 / 麦克尔斯深圳）做商品与库存读多写少的场景：价格、库存、商品详情都会进 Redis。高峰期经常发生**脏读**或**雪崩式回源**。请你说明**写路径怎么保证一致性**、**读路径怎么防止击穿**，以及**热 Key 如何防‘缓存重建风暴’**？”

**你：**

“我把它拆成三块：**写路径的删序与可靠失效**、**读路径的回源互斥**、**热键保护**。”

1 - 写路径：先写库，再删缓存（必要时‘延时双删’/消息通知）

- **基本顺序（Cache-Aside）**：
  1. **写 DB** 成功；
  2. **删 Cache**（`DEL product:{id}`）；
  > 不先删再写，避免‘删了却写失败’导致**长期空洞**。
- **并发场景的竞态**（T1 更新、T2 读）：T1 写库后刚删缓存，T2 在删之前回源把**旧值**重新塞回缓存。
- **解法 A：延时双删**：
  - 写库 → 立即 `DEL` → **sleep 200–500ms** → 再 `DEL`（或通过**延时队列/MQ**二次失效）。
- **解法 B：消息通知/CDC**：
  - 写库后**可靠地**发布 `productUpdated(id, version)` 到 Kafka/Redis Stream；消费者订阅做 `DEL`/对比 `version` 决定是否覆盖。
  - 我们在凡新把商品价改、库存改都走 **Outbox + 消息**，重放安全、跨服务也能同步；在麦克尔斯，作品发布后用 **Redis Stream** 做失效广播，移动端立刻看到最新。
- **版本化防回灌**：缓存值里带 `version/etag`；写入方只在 `newVersion >= cachedVersion` 时覆盖（Lua 脚本原子判断）。

2 - 读路径：互斥回源 + 逻辑过期 + 抖动 TTL

- **互斥回源（单飞 / mutex）**：Cache miss 时对 `rebuild:{key}` 做 `SET NX PX` 拿**短锁**（如 1–3s），拿到锁的线程**唯一**回源 DB 并回填；其它线程直接读老值或短暂等待。
- **逻辑过期（stale-while-revalidate）**：缓存里存 `data + expireAt`。
  - 读到**过期但未严重过期**时：先返回**旧值**，后台异步刷新；
  - 严重过期或拿到互斥锁：同步刷新。
  - 这样高峰期不会所有请求都打到 DB。
- **TTL 抖动**：给 TTL 加随机（±10–20%），避免**同一时刻**大面积同时失效引发雪崩。
- **旁路回源保护**：对空值做**短 TTL 缓存**（例如 30–60s）防穿透；对“超大对象”分片缓存，避免单值过大回源慢。

3 -  热 Key 与热点保护

- **本地 + 远端两级缓存**：在 BFF/网关层用 **Caffeine** 做本地 50–200ms 的**极短 TTL**，命中率能挡掉一批抖动；远端 Redis 继续做主缓存。
- **热键互斥 + 限速重建**：
  - 热度检测（QPS、失败率）触发时，对该 key 设 **更长 TTL** 并且**只有一个重建者**（互斥锁 + 队列）；
  - 重建频率加**令牌桶**，每秒最多 N 次回源。
- **分片/哈希打散**：对计数类热键（pv/like）用**分片 key** 聚合，避免单槽被打爆。
- **预热**：发布/大促前，对确定的热点 SKU 批量**预热缓存**，并启用**逻辑过期**延长可服务窗口。

追问 1（工程细节）

**面试官：**“Java 里你怎么写这套互斥回源与版本化？”

**你：**

“互斥用 Redis：

```java
String lockKey = "lock:product:" + id;
boolean locked = redis.set(lockKey, nodeId, SetArgs.Builder.nx().px(2000));
if (locked) {
  try {
    Product p = db.load(id);
    // 带版本回写（Lua 保证原子判断 newVer >= oldVer）
    redis.evalsha(updateIfNewerSha, keys=[cacheKey, verKey], args=[pJson, p.version]);
  } finally {
    // 只释放自己加的锁（value 比对）
    releaseLock(lockKey, nodeId);
  }
} else {
  // 返回旧值（逻辑过期也可先回旧）
  return cachedOrFallback();
}
```

版本化我用 Lua 做：`if newVer >= currVer then set value & ver`，避免“旧值回灌”。

追问 2（失败与补偿）

**面试官：**“如果删缓存失败，或者消息消费者短暂挂了会怎样？”

**你：**

“删失败就**重试 + 告警**；消息侧我们用 **Outbox**（DB 事务里落消息）+ **至少一次** 语义，消费者**幂等**（按 key + version 去重）。如果消费者挂了，重启后会把落下的消息**补拉**补删；同时写路径保留**延时双删**，两条腿保证最终一致。”

追问 3（真实案例）

**面试官：**“讲一个你们解决‘缓存重建风暴’的真实例子。”

**你：**

“凡新的一次大促，`/product/{id}` 在 0 点同时失效，直接把 MySQL 顶高。我们加了**TTL 抖动 + 逻辑过期 + 单飞互斥**，并提前**预热热点 SKU**，开售后 QPS 峰值时 DB 仍在可控范围。

麦克尔斯那边，作品详情是**高基数但局部热**，我们在 BFF 加了 **Caffeine 100ms 本地缓存**，并把回源改为**互斥重建 + 限速**，p95 从 ~280ms 降到~90ms。”

### 三座大山：穿透 / 击穿 / 雪崩（识别与治理清单）

> **穿透/击穿/雪崩**：穿透→**空值缓存 + 校验 + 布隆**；击穿→**单飞互斥 + 逻辑过期 + 预热/两级缓存 + 限速重建**；雪崩→**TTL 抖动 + 分级限流/降级 + 渐进放量 + 多级兜底**；全程用**版本化写回**防旧值回灌，并监控 `miss burst / db fallback / hotspot TopN`。

**面试官**

“你在（深圳市凡新科技 / 麦克尔斯深圳）给商品与库存做缓存时，遇到三类经典事故：

1. **穿透**：请求大量不存在的 `productId`，缓存每次都 miss，DB 被打爆；
2. **击穿**：一个**超热点 Key** 恰好过期，瞬间几十万请求同时回源；
3. **雪崩**：大批 Key 在同一窗口过期或 Redis 集群重启，DB 被洪峰淹没。
   你分别如何**识别**与**治理**？”

**你：**

“我给自己准备了一个**小清单**，每类问题都有‘**识别信号 → 快速止血 → 长期治理**’。”

1 - 穿透（请求的 Key 根本不存在）

- **识别信号**：缓存 **Hit 率骤降**，但**每个 Key 的访问频次很低**；DB QPS 升、`NOT FOUND` 比例高。
- **快速止血**：
  - **空值缓存**：把不存在结果以**短 TTL**（30–60s）缓存，挡住重复探测；
  - **前置校验层**：对 `productId/sku` 做**格式/范围校验**，明显非法的直接拦截；
  - **限流**：对同一来源/同一前缀做速率限制。
- **长期治理**：
  - **Bloom Filter / 布隆**：把已存在的 `id` 集合装入布隆，拦截不存在的 Key；
  - **风控名单**：来源异常（爬虫/脚本）进入黑名单或更强校验。
- **真实例子**：凡新双 11 前被爬虫扫空 ID 段，Hit 率跌到 20% 左右；临时启**空值缓存 + 前置校验**立刻止血，后续上线**布隆**恢复到 80%+。

2 - 击穿（超热点 Key 过期瞬间被打穿）

- **识别信号**：Hit 率总体高，但**单 Key QPS 极高**，且在过期点出现**锯齿形** DB 峰值。
- **快速止血**：
  - **单飞互斥（mutex）**：miss 时用 `SETNX PX=2s` 抢锁，仅**一个线程**回源，其它请求要么等锁、要么直接返回旧值；
  - **逻辑过期**：缓存里存 `expireAt`，**轻度过期**时先返回**旧值**并后台刷新；
  - **强制延长 TTL**：临时把热点 Key TTL 拉长（开关可控）。
- **长期治理**：
  - **预热**：大促/上新前对热点 Key 预热；
  - **本地 + 远端两级缓存**（如 BFF Caffeine 50–200ms 超短 TTL）；
  - **重建限速**：对同一 Key 的回源加令牌桶，每秒最多 N 次重建。
- **真实例子**：麦克尔斯作品详情在发布后 1 分钟内 QPS 飙升，改为**逻辑过期 + 单飞互斥 + 两级缓存**后，DB 峰值压回正常区间，p95 从 \~280ms 降到 \~90ms。

3 - 雪崩（大面积同时失效 / 缓存整体不可用）

- **识别信号**：**大量 Key 同时过期或 Redis 重启**后，DB QPS 呈**整段抬升**；服务端 CPU 飙高，排队/超时增多。
- **快速止血**：
  - **入口分级限流 + 降级**：非关键读直接返回旧值或“受理中”，**关键读走主库**；
  - **熔断**：下游 DB 超时/错误拉高时，网关/服务端短时间**快速失败**，避免排队雪崩；
  - **随机 TTL 抖动**：紧急把 TTL 加随机（±20%）并逐步放量。
- **长期治理**：
  - **TTL 打散策略**：生成缓存时随机化 TTL；
  - **多级兜底**：热点用两级缓存；静态或半静态内容加 **CDN/边缘缓存**；
  - **冷启动保护**：Redis 集群重启后，恢复阶段**渐进式放量**（金丝雀），并对回源加**全局重建速率**上限；
  - **数据分层**：把“必需数据”（配置/白名单）做**更长 TTL** 或持久化本地快照，保证最低可用。
- **真实例子**：凡新某晚 Redis 集群滚更，错把一批 Key 设置成同一 TTL，半小时后**同刻失效**引发 DB 洪峰。复盘后统一上线**TTL 抖动**、**冷启动渐进放量**，并给热点接口加本地兜底。

追问（工程细节）

**面试官：**“如果缓存里是**旧值**但你又不想卡住用户体验，你会怎么做？”

**你：**

“走**stale-while-revalidate**：在轻度过期窗口内**先回旧值**，同时后台异步刷新。只有严重过期或加锁成功的请求才同步回源。用户体验稳定，DB 负载也平滑。”

**面试官：**“怎么避免‘旧值回灌’？”

**你：**

“**版本化写回**：缓存结构里带 `version/etag`，Lua 脚本原子地比较 `newVer >= currVer` 再覆盖；写路径也配**延时双删/CDC** 把旧值清干净。”

**面试官：**“你怎么监控这三类问题？”

**你：**

“按 **Key 前缀/接口** 维度看 `hit ratio、miss burst、db fallback qps、回源耗时、互斥锁命中率、热点 TopN、TTL 分布`。出现 miss 突增且 DB 回源同步拉高，基本就能定位是哪一类问题。”

---

## Message and Consistency

### Outbox（事务外箱）& 本地事务边界

> Outbox：单库事务落数据+事件，Publisher 异步发布；至少一次发送 + 幂等消费（`eventId`/`aggregateId+version` 去重）≈ 几乎一次；分区键保证同聚合顺序；失败退避重试，CDC/归档保性能。

**面试官**

“下单成功要**占库存**并**通知**支付/仓库/搜索。如何保证**写数据库**和**发消息**‘要么都成功，要么都不成功’，避免双写不一致？你在（深圳市凡新科技 / 麦克尔斯深圳）是怎么落地的？”

**你：**

“我们用**事务外箱**（Outbox）把‘数据变更’和‘事件生成’放到**同一个本地事务**里：

- **应用事务**里做两步：① `INSERT/UPDATE` 业务表（orders/stock…）② `INSERT outbox(event)`。只要事务提交，**两者一定同时落地**。
- 事务外的**Publisher**轮询/CDC 读取 `outbox`：`SELECT ... FOR UPDATE SKIP LOCKED LIMIT N`，发布到 Kafka/SQS/Redis Stream，成功后把这行标记 `SENT`，失败就**退避重试**并回写 `attempts/next_retry_at`。
- **消费端幂等**：事件里自带 `eventId`（或 `aggregateId+version`），消费者先查 `processed_events`（唯一键），**见过就直接 ACK**，没见过才处理并插入标记。
  总体语义是**至少一次发送 + 幂等消费 = 几乎一次（effectively-once）**。我们在凡新下单→扣减库存→异步出库通知这条链路就是这么做的；在麦克尔斯，作品发布→索引更新→通知订阅者也同理。”

**简化表结构（示意）**

```sql
CREATE TABLE outbox (
  id           CHAR(36) PRIMARY KEY,      -- eventId (UUID)
  aggregate_id CHAR(36) NOT NULL,         -- 订单/商品ID
  aggregate_type VARCHAR(32) NOT NULL,    -- ORDER / STOCK ...
  event_type   VARCHAR(64) NOT NULL,      -- OrderCreated / StockReserved...
  payload      JSON NOT NULL,
  version      INT NOT NULL,              -- 聚合版本，用于顺序/去重
  status       ENUM('NEW','SENT','FAILED') DEFAULT 'NEW',
  attempts     INT DEFAULT 0,
  available_at DATETIME NOT NULL,         -- 下次可重试时间（退避）
  created_at   DATETIME NOT NULL
);

CREATE TABLE processed_events (
  event_id CHAR(36) PRIMARY KEY,          -- 去重键
  processed_at DATETIME NOT NULL
);
```

**应用层伪代码**

```java
// Tx begin
insertOrder(...);                 // 业务写
insertOutbox(event(orderId,...)); // 同库插入消息
// Tx commit  => 原子性成立
```

**Publisher 轮询（要点）**

- 批量拉取 `status=NEW AND available_at<=now()`，用 `FOR UPDATE SKIP LOCKED` 防并发争抢；
- 发布成功 → `status=SENT`；失败 → `attempts++`，按 `min(backoff*2^attempts, cap)` 回写 `available_at`；
- **崩溃边界**：如果“已发布但未标记 SENT”重启后会再发一次，所以**消费者必须幂等**。

**顺序性与分区**

- 需要**同一订单的事件按序**：Kafka 用 `partitionKey=orderId`；SQS 用 **FIFO**，`messageGroupId=orderId`；Redis Stream 按 stream per-aggregate 或在消费者端序列化处理。
- 跨聚合全局顺序一般**不保证**，用**因果字段**（version/timestamp）校正。

**真实落地小例子**

- 凡新：`OrderCreated` 与 `StockReserveRequested` 一起落库；Publisher 发 SQS，消费者是库存服务；出现网络抖动时，**重复消息**被 `processed_events` 吸收，不再重复扣减。
- 麦克尔斯：作品发布事件走 Kafka，搜索索引消费者先做**去重**再写 ES；Publisher 用**退避+抖动**，避免抖动期把队列压爆。

**追问 1：为什么不用 DB 里直接调用消息系统事务？**

“跨资源的**分布式事务**（2PC）复杂且脆弱，Outbox 把一致性**收敛在单库事务**里，外围只需‘至少一次 + 幂等’，工程成本更低、恢复性更好。”

**追问 2：如果消息量很大，轮询会不会很慢？**

“生产里我们更倾向**CDC（如 Debezium）**或**分片轮询**：按时间或主键区间扫描；并用**归档/TTL**压缩 `SENT` 行，保持表小而快。”

### 幂等消费与去重键：表/Redis 实战与失败补偿

> **幂等消费**：`eventId`（或 `aggregateId+version`）做去重键；**同库事务**里先插 `processed_events` 再执行业务写；写法用**条件更新/UPSERT/版本检查**做到可重放；Redis `SETNX+TTL` 快速挡重复，DB 约束兜底；失败走 **DLQ+重放**，顺序用**分区键/FIFO**。

**面试官**

“支付平台和库存系统都会**重复投递**事件（网络抖动/重试）。你在（深圳市凡新科技 / 麦克尔斯深圳）如何保证**消费端幂等**？具体怎么做**去重键**、**落库顺序**、**失败补偿**？”

**你：**

“我的做法是把 ‘**去重 + 业务落库**’ 放进同一个本地事务里，保证 ‘**要么都成功，要么都回滚**’，并且提供**双层去重**（DB 强约束 + Redis 快速挡重复）。

1. **去重键**
   - 统一使用 `eventId`（UUID）或 `aggregateId + version` 作为**全局唯一键**。
   - **DB 层**建 `processed_events(event_id PK)`，天然幂等；
   - **Redis 快速去重**：`SETNX de:{eventId} 1 EX <TTL>` 抢到再处理，没抢到直接 ACK（避免同一时刻并发重复执行）。
2. **同库事务顺序**（Inbox 模式）
    ```sql
    -- 在一次事务内完成两件事：业务变更 & 去重落账
    BEGIN;
    -- ① 先插 processed_events（如果已存在直接报错/返回）
    INSERT INTO processed_events(event_id, processed_at) VALUES(:eid, NOW());
    -- ② 再做业务落库（UPDATE/INSERT ...）
    UPDATE stock SET qty = qty - :n
        WHERE sku=:sku AND qty >= :n;  -- 条件更新，重复执行也不会二次扣
    COMMIT;
    ```
    > 这样如果第二次收到同一事件，`INSERT processed_events` 会**冲突**，直接回滚，业务不会再落一次。
3. **幂等写法**
   - **条件更新**：`UPDATE ... WHERE qty>=:n`；
   - **UPSERT**：`INSERT ... ON DUPLICATE KEY UPDATE ...`（比如“首次创建订单，重复则更新状态/时间”）；
   - **版本检查**：`WHERE version = :old` 成功后 `version=version+1`，重复事件因版本不匹配**零影响**。
4. **失败补偿**
   - **指数退避 + 尝试上限**：消费失败 `attempts++`，超过阈值（如 10 次）投递到 **DLQ/停车场**；
   - **人工/自动重放**：DLQ 可按 `eventId` 批量重放；
   - **顺序性**：按 `aggregateId` 做**分区键/FIFO**，单聚合同一消费者串行处理，避免乱序导致的版本冲突。

在凡新：支付回调/库存事件会**重复 3–5 次**，我们用 `processed_events` 做硬去重，外层再加 Redis TTL 去重，**99% 的重复在入口被挡**；在麦克尔斯：作品索引更新走 Kafka，消费者先插 `processed_events`，再写 ES，重复消息直接被忽略，**不会重复建索引**。”

追问 1（Redis 被逐出或丢失怎么办）

**面试官：**“如果 Redis 因为内存淘汰把 `de:{eventId}` 清掉了，会不会又重复处理？”

**你：**

“不会，因为**DB 层还有硬约束**。Redis 只是**快速挡**；真正的幂等保证靠 `processed_events` 主键或**业务唯一约束**（例如 `user_id+coupon_id` 唯一）。即使 Redis 丢了，DB 也会拒绝重复写。”

追问 2（顺序与并发）

**面试官：**“同一 `orderId` 的事件要按顺序消费，怎么做？”

**你：**

“把 `orderId` 当作**分区键**（Kafka partition / SQS messageGroupId），确保同聚合到同一队列分区，由**单个消费者串行**处理。确需并发就用**乐观版本**：版本不匹配的更新返回 0 行，进入**重试/停靠**。”

追问 3（真实事故）

**面试官：**“讲个你们因为没做幂等导致的事故。”

**你：**

“早期库存扣减没做条件更新，重复消息会**二次扣**。修复后改成‘`processed_events` PK 去重 + `UPDATE … WHERE qty>=`’，重复消息变成幂等重放，工单直接清零。”

### 重试策略与“重试预算”：退避 + 抖动 + 限额；与幂等/熔断的协同

> **重试策略**：仅对可重试错误（5xx/超时/429）+ **幂等写**启用；**指数退避 + 抖动**，并以**重试预算**（≈≤10%）限额；与**熔断/限流/舱壁**协同——熔断开启时停止重试，仅半开探测；前端/ BFF 做**请求合并**与 `Retry-After` 遵循。

**面试官**

“在（深圳市凡新科技 / 麦克尔斯深圳）的大促或第三方网关抖动时，接口偶发超时/503/429。你怎么设计**重试策略**，既提升成功率，又不把依赖打穿？重试和**幂等**、**熔断**怎么配合？”

**你：**

“我把重试做成一个**受控系统**：**只在明确可重试的场景**触发，采用**指数退避 + 抖动**，并受‘**重试预算（Retry Budget）**’约束；同时和**幂等键**、**熔断器**联动，避免雪崩。

1. **什么时候重试 / 不重试**
   - **可重试**：`5xx`、网络异常、**超时**、`429`（配合 `Retry-After`）。
   - **不重试**：`4xx` 业务错误（如验证失败、配额不足）、幂等条件不满足。
   - **写请求**只有在**具备幂等**（`Idempotency-Key` 或“条件更新/版本检测”）时才允许重试。
2. **退避策略（带抖动）**
   - 我常用序列：`200ms → 500ms → 1.2s → 2.5s → 5s`（上限 5 次），**全抖动**或**去相关抖动**避免齐步重试。
   - **请求总截止时间（Deadline）** 优先于次数，前端/交互链路一般 **2–3s** 就该止损反馈。
3. **重试预算（核心）**
   - 定义：单位时间内，**重试数 ≤ min(10% × 成功请求数 + 常数保底, 上限)**。
   - 落地：在 BFF 或客户端维护**预算计数器**；预算用尽，后续**直接快速失败**（或返回缓存/降级），避免**重试风暴**。
   - 观测：暴露 `retry_allowed / retry_exhausted / retry_after_honored` 指标。
4. **与幂等/熔断的协同**
   - **幂等**：写链路统一携带 `Idempotency-Key`；服务端“**原子占位 + 响应快照**”确保重复请求**无副作用**。
   - **熔断**：错误/超时率高到阈值（如 50% 且 QPS≥N）→ **打开熔断**；**熔断打开期间**不再重试（或者仅做**极少探测**），等**半开**再小流量尝试。
   - **限流**：命中 `429` 时尊重 `Retry-After`，并把下一次退避与其对齐；服务端返回**明确可重试/不可重试**信号。
   - **并发舱壁（Bulkhead）**：对外呼并发设上限，防止重试把线程池塞满。
5. **凡新/麦克尔斯的落地口径**
   - 凡新：支付网关抖动时，BFF 只对携带 `Idempotency-Key` 的写请求做**最多 4 次**退避重试，并将“重试预算”设为每分钟 **≤10%**。熔断打开后，直接 **快速失败 + 降级**（受理中/稍后查询）。
   - 麦克尔斯：图片/索引服务偶发慢，客户端把超时从 3s 降到 **1.5s** 并加退避重试；服务端接入 Resilience4j 的 **Retry + TimeLimiter + CircuitBreaker** 组合，`Retry-After` 生效、预算消耗在仪表盘可见。

追问 1（工程细节：Java/Resilience4j）

**面试官：**“你会怎么在 Spring/Resilience4j 里配置？”
**你：**

```java
RetryConfig retry = RetryConfig.custom()
    .maxAttempts(5)                                // 包含首次调用
    .waitDuration(Duration.ofMillis(200))         // 基准退避
    .intervalFunction(IntervalFunction.ofExponentialBackoff(200, 2.0)) // 指数退避
    .retryOnException(ex -> isRetryable(ex))      // 只对可重试异常
    .build();
// 可替换为带随机抖动的 IntervalFunction
TimeLimiterConfig tl = TimeLimiterConfig.custom()
    .timeoutDuration(Duration.ofMillis(1500))     // 单次调用硬超时
    .build();
CircuitBreakerConfig cb = CircuitBreakerConfig.ofDefaults(); // 结合错误率阈值
```

> 额外：在拦截器中维护**重试预算**计数；若预算不足，**不进入重试装饰器**直接返回。

追问 2（请求合并与去抖）

**面试官：**“同一用户 1 秒内点了 3 次提交，怎么避免 3×N 次重试？”

**你：**

“BFF 做**去抖 + 合并**：对同一幂等键的并发请求只保留一个下行，其余‘排队等结果/直接复用响应’。这样重试也只发生在**一条请求**上。”

追问 3（真实复盘）

**面试官：**“讲个因为重试策略不当导致放大的案例。”

**你：**

“早期凡新有一段时间对 503 做**固定间隔**重试，且**无预算**，在第三方 2–3 分钟抖动时把线程池打满。复盘后改为**指数退避 + 抖动 + 10% 预算**，同时把熔断半开探测降到每秒**单次**，问题消失；成功率反而提高，因为我们不再挤占自己资源。”

### DLQ / 停车场与人工处置：可观察、可回放、可审计

> **DLQ/停车场**：重试上限或不可重试错误**停靠**，指标与告警可见；支持**筛选/批量回放**，回放走**慢车道 + 令牌桶**；消息记录 `traceId/eventId/aggregateId/error_code/attempts` 便于审计；回放幂等、设黑名单与冷却期，避免“无限回环”。

**面试官**

“真实生产里，总会有**毒性消息**（数据缺字段、顺序打乱、对方幂等键冲突）怎么也处理不了。你在（深圳市凡新科技 / 麦克尔斯深圳）如何设计 **DLQ（死信队列）/Parking Lot（停车场）**，做到**不阻塞主线**、**可定位**、**可重放**、**可审计**？”

**你：**

“我的原则是：**主线轻装前进，异常集中停靠**。落地分四件事：**准入到 DLQ 的规则**、**可观察与告警**、**可重放机制**、**审计与防二次伤害**。”

1. **什么时候进 DLQ / 停车场**
   - **达到重试上限**（比如 10 次指数退避仍失败）或命中特定错误（不可重试类，如 schema 不兼容、幂等冲突）。
   - **顺序受损**：同一 `aggregateId` 新版本已被处理、旧版本再来 → 直接停靠。
   - 我们把消息打上**失败原因 code**（`VALIDATION_ERROR / VERSION_CONFLICT / UPSTREAM_4XX`）和**最近一次异常栈摘要**，方便后续归因。
2. **可观察（第一时间定位）**
   - 指标：`dlq_size、dlq_in_rate、top_error_code、message_age_p95、redrive_success_rate`。
   - 日志 / APM：每条 DLQ 消息带 `traceId`、`eventId`、`aggregateId`，能一键跳到当时的业务日志/链路。
   - 告警：`dlq_in_rate > 基线` 或 `age_p95 > 阈值` 告警到 on-call；并附上**样本消息链接**。
3. **可重放（安全回放）**
   - **按钮式回放**：控制台支持按 `eventId` / `aggregateId` / 时间窗口**筛选 + 批量 redrive**。
   - **回放到哪**：
     - 修复了数据后，回放到**原主队列**；
     - 依赖仍不稳时，回放到**隔离的“慢车道”队列**，有更严格的速率/并发。
   - **回放幂等**：消费者已经是**幂等**，所以即便重复也不会二次扣减/二次下单。
   - **节流**：redrive 受**令牌桶**保护（例如每秒最多 50 条），避免回放本身造成**二次冲击**。
4. **审计与防二次伤害**
   - DLQ 里保留**处理历史**：`first_seen / last_attempt_at / attempts / last_error_code / operator / redrive_at`。
   - **红/黑名单**：某些 `aggregateId` 连续失败 3 次以上，进入**黑名单**，临时不再回放，等待数据修复。
   - **保留策略**：保留 7–30 天，超期自动归档到对象存储（便于审计与离线排查）。

**凡新**这边：订单出库事件偶发因**上游字段缺失**失败，我们把它们停靠到 DLQ，填补字段后**按聚合 ID 批量回放**，有速率上限，不影响主线。

**麦克尔斯**那边：作品索引更新在 ES 集群滚更时会失败，我们让 DLQ 回放走**慢车道**消费者，等 ES 恢复就自然清空；整个过程 on-call 看到 `dlq_size`、`age`、`redrive_success_rate` 一目了然。

关键表/队列与接口（示意）

```sql
-- 停车场记录（可用 DB，也可映射 DLQ 元数据到 DB 里便于查询）
CREATE TABLE dlq_events (
  event_id      CHAR(36) PRIMARY KEY,
  aggregate_id  CHAR(36),
  error_code    VARCHAR(64),
  last_error    TEXT,
  attempts      INT,
  first_seen    DATETIME,
  last_attempt  DATETIME,
  last_operator VARCHAR(64),    -- 谁点的回放/忽略
  status        ENUM('PENDING','REDRIVEN','IGNORED') DEFAULT 'PENDING'
);
```

```text
POST /ops/dlq/redrive?eventId=...|aggregateId=...|from=...&to=...
- 校验：黑名单/白名单；幂等：二次点击不重复回放
- 限速：令牌桶 N/sec
```

**队列侧实践**

- **Kafka**：DLQ 用独立主题 `topic.DLQ`，消息里保留 `headers`（`traceId、error_code、attempts`）；回放时把 `original_topic/partition/offset` 也带上便于追溯。
- **SQS**：配置 `redrive policy`（`maxReceiveCount` 达到即进 DLQ），另建一个**人工/批量 redrive**的 Lambda/Job；需要顺序时用 **FIFO + messageGroupId** 的“慢车道”回放。

追问 1（为什么要“停车场”而不是只用 DLQ？）

**你：**

“DLQ 是‘**被动**死信’，停车场是‘**主动**停靠’：当我们识别为**数据质量问题**或**顺序冲突**时，**不等重试上限**就直接停靠，避免无意义的打桩刷屏；修复后再**人工确认**回放，风险更可控。”

追问 2（如何避免“无限回放→再失败”的回环）

**你：**

“回放有**次数上限**与**冷却期**：同一 `eventId` 失败≥N 次后标记 `IGNORED`，需**人工解除**；同时在回放通道加**速率与并发上限**。另外我们会在回放前跑**干跑验证（dry-run）**，例如先校验 payload/schema 与依赖状态。”

追问 3（真实复盘）

**你：**

“凡新一次大促，支付回执 Schema 升级，部分字段名变化导致消费者报 `VALIDATION_ERROR`；我们 5 分钟内把错误集中到 DLQ、修正映射、**分批回放**，排队清空且用户侧无感知。
麦克尔斯那边，ES 扩容期间出现 `UPSTREAM_5XX`，我们把回放切到**慢车道**并降低速率，指标稳定后再全量回放。”

### 顺序性与分区键：按“聚合维度”保序，吞吐与热点的权衡

> **顺序与分区**：用 `aggregateId` 做分区键（同聚合同分区）实现**局部有序**；消费者维护 `last_version`，重复/过期丢弃，缺口停靠；热点聚合接受串行或拆流/分片；不追求全局 FIFO；跨聚合因果用 `version/causal` 描述并在源头收敛。

**面试官**

“订单→库存→出库通知这一链路里，**同一订单**的事件必须按顺序处理，但全局不需要严格顺序。你在（深圳市凡新科技 / 麦克尔斯深圳）怎么设计**分区键/队列模型**来既保证**局部有序**又能**水平扩展**？遇到**热点聚合**时怎么破？”

**你：**

“我坚持**按聚合（Aggregate）保序**，按整体吞吐做分区扩展：

- **分区键** = `aggregateId`（如 `orderId`）。Kafka 就是 `partition = hash(orderId) % N`；SQS/FIFO 就是 `messageGroupId = orderId`。这样**同一订单的事件在同一分区**，**单消费者串行**，天然有序；不同订单可并行处理、横向扩展靠**分区数 N**。
- **顺序语义不跨聚合**：我们不追求全局顺序，避免吞吐被锁死。
- **幂等 + 版本**兜底：每条事件带 `aggregateVersion`（或 `seq`）。消费者维护 `last_version(orderId)`：
  - `version <= last` → **重复/过期**，直接忽略（幂等）；
  - `version == last + 1` → 正常处理并 `last = version`；
  - `version > last + 1` → **发现缺口**（乱序/丢消息），**停靠到停车场**或**短暂缓存等待**后再处理。
    这套在凡新和麦克尔斯都实践过：Kafka/SQS 提供**分区内有序**，版本字段让我们在极端情况下**检测/修复顺序**。”

**消费端事务示意（确保“处理 + 推进版本”一致）**

```sql
BEGIN;
  -- 去重/保序：每个聚合维护一个版本表
  SELECT last_version FROM agg_progress WHERE aggregate_id=:aid FOR UPDATE;
  -- 若无记录先插入 last_version = 0
  CASE
    WHEN :version <= last_version THEN ROLLBACK; -- 重复/过期，忽略
    WHEN :version = last_version + 1 THEN
      -- 执行业务落库（幂等写法/条件更新）
      UPDATE ...;
      UPDATE agg_progress SET last_version=:version WHERE aggregate_id=:aid;
      COMMIT;
    ELSE
      -- version 跳跃：缺口，停靠到 DLQ/停车场，稍后回放
      ROLLBACK;
  END CASE;
```

**热点聚合（Hot Key）处理**

- **接受串行**：如果某个 `orderId` 本来就要求严格顺序，就接受它在单分区**串行**，吞吐上限 = 单消费者能力。
- **降热/分片聚合键**：如果热点来自**商品/库存**这类“可拆分”的聚合，就把分区键改成 `hash(skuId, bucket)`，在消费端做**按 sku 聚合后的有序合并**（只在确需极高吞吐时用，复杂度高）。
- **拆流**：把“强保序”的事件（订单状态流）与“弱保序”的事件（库存读模型刷新）分不同主题/队列，避免强约束拖累整体吞吐。
- **限速与背压**：热点聚合触发**令牌桶**，必要时降级为“受理中”。

**为什么不用“全局 FIFO/单分区”**

- 全局 FIFO 会把吞吐卡死在**单消费者**；重新分区/扩分区又会引入复杂迁移。**按聚合保序**是大多数电商链路的工程平衡点。

**真实落地**

- 在凡新，`OrderCreated/OrderPaid/OrderShipped` 以 **`orderId` 为分区键**；库存相关的 `StockReserved/Released` 也跟订单同分区，所以**订单内严格按序**。碰到活动单品成热点，我们把**读模型刷新**拆到独立主题，写链路保持串行稳定。
- 在麦克尔斯，作品发布事件用 **Kafka 分区** + `workId`，搜索索引消费者用“**版本推进**”表防乱序；偶发跳序进入**停车场**，修复后批量回放即可。

追问 1（跨聚合的“因果”如何表达）

**面试官：**“库存事件要落后于订单创建，你怎么表达这种因果？”

**你：**

“事件里带 `causal` 字段（如 `orderVersion`），消费者可在发现依赖未就绪时**短暂停靠**或**重试**；更稳妥是由**同一服务**在本地事务里产生‘订单创建’与‘库存预留请求’（Outbox），从源头保障时序。”

追问 2（扩分区与重平衡的影响）

**面试官：**“Kafka 扩分区后哈希变化，顺序还在吗？”

**你：**

“**分区内顺序仍成立**，但**键到分区的映射可能变化**，导致同一键后续去到新分区。为避免扰动，**提前估算足够的分区数**；或用**一致性哈希**/自定义分区器在扩容时减少抖动。”

追问 3（Exactly-once 的取舍）

**面试官：**“你们追求 exactly-once 吗？”

**你：**

“我们更偏向‘**几乎一次（effectively-once）**’：生产端至少一次 + **幂等消费者 + 版本推进**。Kafka 的 EOS（事务性生产/消费）对堆栈/运维要求更高，只有在**单主题内计算**且强一致计费等场景才考虑。”

### Exactly-once 的工程化取舍：追求 “effectively-once” 而非执念 EOS

> **Exactly-once 取舍**：端到端 EO 成本高；工程上以 **Outbox + 至少一次传输 + 幂等消费（去重键/条件更新/UPSERT/版本推进）+ DLQ 回放** 达到 **effectively-once**；Kafka EOS 仅限**拓扑内**且无外部副作用场景，涉及外部系统仍靠幂等落地。

**面试官**

“你在（深圳市凡新科技 / 麦克尔斯深圳）做‘下单→扣库存→出库通知→索引刷新’这条链路时，怎么保证**不多扣、不漏扣**？你会不会上**Exactly-once**？如果不用，怎么做到 **几乎一次（effectively-once）** 的工程效果？”

**你：**

“我的原则是：**跨系统端到端的 Exactly-once 很难、代价高**；我们在生产里更推崇 **‘至少一次 + 幂等 + 去重 + 版本推进 = effectively-once’**。

- **源头**（订单服务）用 **Outbox**：数据变更与事件同库同事务落地，**保证不丢**。
- **传输**（Kafka/SQS）默认**至少一次**，接受**可能重复**。
- **落地**（库存/搜索）把**幂等**做到数据模型里：
  - **去重键**：`eventId` 或 `aggregateId+version`；
  - **条件更新 / UPSERT**：`UPDATE stock SET qty=qty-:n WHERE sku=:sku AND qty>=:n`；
  - **版本推进**：维护 `last_version(aggregateId)`，只接收 `= last+1`；小于等于是重放，**零影响**；大于则**停靠/缓冲**处理顺序。
- **失败与顺序异常**进 **DLQ/停车场**，修复后**限速回放**。
  这套在凡新的订单库存、以及麦克尔斯的作品索引链路都跑得很稳：**不靠分布式 2PC**，靠**局部原子 + 全链路幂等**把效果做到位。”

**那 Kafka 的 EOS（事务性生产/消费）要不要用？**

“**只在局部计算拓扑内**（Kafka→Kafka 的流式作业、无外部 DB 副作用），并且**团队熟练/运维可控**时才考虑：比如用 idempotent producer + transactional producer/consumer 保证 **‘每条消息要么处理一次并写入目标主题，要么不处理’**。

一旦涉及 **外部系统（数据库/ES/第三方）**，仍然回到 **幂等写 + 版本推进**。EOS 会带来**事务协调开销、故障恢复复杂度**，不适合所有链路。”

**如何验证你真的达到了 effectively-once？**

“我做两个验证：

1. **对账/幂等监控**：落库端统计 `duplicate_drop_rate`、`version_gap_count`；库存余额做**定时对账**（DB 与事件总量核对）。
2. **混沌/重放演练**：把一批事件**重复投递**、**乱序投递**、甚至**模拟丢失后回放**，确认业务端只产生**一次**净效应且可收敛。”

**真实取舍例子**

“凡新：我们评估过‘订单→库存’用分布式事务，复杂度和脆弱点太多，最终选 **Outbox + 幂等消费**，重复率有但**净效果只一次**；

麦克尔斯：搜索索引刷新不要求强事务一致，就用 **at-least-once + 幂等 upsert**；需要‘感知因果’时靠 `aggregateVersion` 和‘版本推进表’保证**先后关系**。”

**小片段（消费端事务伪码）**

```sql
BEGIN;
  -- 去重 + 保序
  INSERT INTO processed_events(event_id) VALUES(:eid);  -- 若重复则失败回滚
  SELECT last_version FROM agg_progress WHERE id=:aid FOR UPDATE;
  IF :ver = last_version + 1 THEN
      -- 业务写，幂等/条件更新
      UPDATE stock SET qty = qty - :n WHERE sku=:sku AND qty >= :n;
      UPDATE agg_progress SET last_version=:ver WHERE id=:aid;
      COMMIT;
  ELSEIF :ver <= last_version THEN ROLLBACK; -- 重放，忽略
  ELSE ROLLBACK; -- 发现缺口，停靠等待
END;
```

---

## Java Concurrency

### 内存模型 & 可见性：happens-before / `volatile` 的边界

> **可见性 vs 原子性**：`volatile` 保障**可见/有序**不保障**复合原子**；计数/聚合用 `LongAdder/Atomic*` 或串行队列；**安全发布**用“不可变对象 + volatile 引用”；DCL 单例需 `volatile`；跨线程顺序靠 `start/join`、`synchronized unlock/lock`、`volatile write/read` 的 happens-before。

**面试官：**

“你们在（深圳市凡新科技 / 麦克尔斯深圳）有个**库存曝光服务**：多个线程持续累加访问量并在 1s 定时刷到 Redis。偶发地，页面 PV/库存快照会**落后**或**计数丢失**。请你解释 Java 内存模型里的 **happens-before** 关系、`volatile` 能/不能解决什么，并给出代码级修正。”

**你：**

“我先分两件事：**可见性/有序性** 和 **原子性**。

- `volatile` 只保证**写→读**的可见性和一定的**指令有序**（建立 happens-before：对同一变量 `volatile write` 先于后续线程的 `volatile read`），**不保证复合操作的原子性**。
- 对‘计数丢失’这种++操作，我会用 **原子类**（`AtomicLong`/热点高时用 `LongAdder`）或把更新放进**单线程/串行化**的队列；
- 对‘配置刷新/开关不生效’这种 **发布-订阅/开关切换**，我会用 `volatile` 或 **不可变对象 + volatile 引用**做**安全发布**。”

**反例 & 修正**

```java
// 反例：可见性不可靠 + 原子性缺失
class CounterBad {
  private long pv = 0;                 // 非 volatile，且 ++ 非原子
  void inc() { pv++; }                 // 读-改-写竞争
  long get() { return pv; }            // 可能读到旧值（CPU 缓存未刷回）
}

// 修正 1：高并发计数 —— LongAdder 更抗热点
class CounterGood {
  private final LongAdder pv = new LongAdder();
  void inc() { pv.increment(); }       // 分段计数，聚合时冲突少
  long get() { return pv.sum(); }
}

// 修正 2：配置热更新 —— 不可变对象 + volatile 引用
class Config { final int ttl; final boolean enable; Config(int t, boolean e){ttl=t;enable=e;} }
class ConfHolder {
  private volatile Config cfg = new Config(60, true); // 安全发布
  Config get(){ return cfg; }
  void reload(){ cfg = loadFromDb(); }                // 对所有线程立即可见
}
```

**happens-before 你要说得出的 4 条常用规则**

1. 程序次序规则：同一线程内的代码顺序。
2. **监视器**：对同一锁，`unlock` 先于后续线程的 `lock`（`synchronized`）。
3. **volatile**：对同一变量的 `write` 先于后续线程的 `read`。
4. **线程启动/终止**：`Thread.start()` 先于子线程内操作；子线程内操作先于 `Thread.join()` 返回。

**易错场景 & 取舍**

- **双重检查单例（DCL）**必须把实例引用设为 `volatile`，否则可能看到**半初始化对象**。
- **组合操作**（如 `check-then-act`、`put-if-absent`）用 `ConcurrentHashMap.computeIfAbsent` 或锁/CAS 保护；仅靠 `volatile` 不够。
- **计数/埋点**：热点极高时 `AtomicLong` 会退化自旋，**LongAdder**冲突更小；但**要快照**（求准确值）时用 `sum()`，它与上一刻的 `inc()` 有极小窗口差。
- **有序性**：`volatile` 可阻止相关指令重排，但**不能**代替锁来“临界区互斥”。

**项目实话实说**

“凡新那边促销统计我们一开始用 `AtomicLong`，峰值下自旋热点明显；换成 `LongAdder` 后 CPU 降了不少。Michaels 的开关配置用的是**不可变配置 + volatile 引用**，热更新后前端请求马上生效，不需要重启。”

### `synchronized` vs `ReentrantLock`：可中断 / 定时 / 公平 / 条件队列

> **锁的选择**：短小互斥→`synchronized`；需要**可中断/定时/多条件/可观测**→`ReentrantLock`。公平锁减吞吐；`unlock` 一定放 `finally`；条件队列要**循环检查**；读多写少可上 `ReentrantReadWriteLock`（只允许**降级**）；热点用**锁分段**，长等待用 **tryLock(timeout)+重试/降级**。

**面试官：**
“在（深圳市凡新科技 / 麦克尔斯深圳）的大促高峰，你们有个**库存预留**的热点段：偶发下游抖动时，线程在等待锁期间**堆积**，无法快速取消，导致**线程池被占死**。你会选择 `synchronized` 还是 `ReentrantLock`？为什么？”

**你：**

“我会把选择标准说清楚：

- **简单互斥、临界区短、无需可中断/定时/多条件队列** → `synchronized` 就够，JIT 对偏向/轻量级锁已经很优化了；
- **需要更细的控制**（例如**等待可中断**、**超时放弃**、**多条件变量**、或**可选公平性**）→ 用 `ReentrantLock`（基于 AQS）。库存预留这种‘热点且可能长等待’就该用 `ReentrantLock`，这样**等待线程可取消**，避免把线程池卡死。”

**关键差异**

- **可中断获取**：`lockInterruptibly()` 只有 `ReentrantLock` 支持 —— 等锁时能响应 `Thread.interrupt()`。
- **定时获取**：`tryLock(timeout, unit)` 允许**超时退避**，可与**重试预算**协同；`synchronized` 没法。
- **条件队列**：`newCondition()` 可建多个条件（如 `notEmpty/notFull`），`wait/notify` 只能一个条件队列且易误用。
- **公平性**：`new ReentrantLock(true)` 可公平，但吞吐下降；大多数场景用**非公平**提升吞吐。
- **可观测/灵活性**：`isLocked/hasQueuedThreads/getQueueLength` 等有助于指标化与排障。
- **语义**：两者都**可重入**；`synchronized` 通过监视器，`ReentrantLock` 通过 AQS 队列。

**等待可中断 + 超时退避（库存热点）**

```java
class StockGuard {
  private final ReentrantLock lock = new ReentrantLock(); // 非公平更高吞吐
  boolean runWithLock(Duration d, Runnable task) throws InterruptedException {
    if (lock.tryLock(d.toMillis(), TimeUnit.MILLISECONDS)) { // 定时获取
      try { task.run(); return true; }
      finally { lock.unlock(); }
    }
    return false; // 超时放弃，交给上层重试/降级
  }
}
// 调用处：失败则触发指数退避 + 幂等重试，避免长等待压死线程池
```

**多条件队列（有界缓存/异步出库）**

```java
class BoundedBuffer<T> {
  private final ReentrantLock lock = new ReentrantLock();
  private final Condition notEmpty = lock.newCondition();
  private final Condition notFull  = lock.newCondition();
  private final Deque<T> q = new ArrayDeque<>();
  private final int cap;

  void put(T x) throws InterruptedException {
    lock.lockInterruptibly();
    try {
      while (q.size() == cap) notFull.await(); // 可中断等待
      q.addLast(x);
      notEmpty.signal(); // 唤醒一个取者
    } finally { lock.unlock(); }
  }
  T take() throws InterruptedException {
    lock.lockInterruptibly();
    try {
      while (q.isEmpty()) notEmpty.await();
      T v = q.removeFirst();
      notFull.signal();
      return v;
    } finally { lock.unlock(); }
  }
}
```

**读多写少补充 - 读写锁与降级**

- `ReentrantReadWriteLock`：读多写少可提升吞吐；**可降级**（持有写锁时获取读锁再释放写锁），**不可升级**（先读后写会死锁/饥饿）。
- 极端读场景可考虑 `StampedLock`（乐观读），但 API 更复杂，注意**中断与可重入限制**。

**工程取舍与坑**

- **一定 `unlock()` 放在 `finally`**；`Condition.await()` 需在**持锁**前提下调用，醒来要**循环检查条件（防虚假唤醒）**。
- 公平锁**减少饥饿**但吞吐更低（队列严格 FIFO，缓存局部性变差）。
- **锁粗化**（把多次短锁合并）能省切换，但要警惕临界区过大造成争用；
- **锁分段/分片**（例如用 `stripe = hash(key) % N` 选择不同锁）能摊薄热点；
- **避免双重检查单例未加 `volatile`** 导致半初始化可见；
- `synchronized` 适合**非常短小**且无中断需求的路径（JIT 可内联/消除），否则用 `ReentrantLock` 获得**超时/可中断/多条件**这些工程特性。
- 指标化：导出**活跃线程数、队列长度、等待时长分位、拒绝数**；看到长等待优先**缩小临界区/分段**，其次再调线程数。

**项目口径**

“凡新那边库存预留把热点 SKU 的临界区改成 `ReentrantLock.tryLock(100ms)`，拿不到就**快速失败 + 幂等重试**，线程池再也没被‘长等待’拖死。麦克尔斯那边有个有界队列用两个 `Condition` 做 `notEmpty/notFull`，比 `wait/notify` 可读且不容易误唤醒。”

### `ThreadPoolExecutor` 七参数、队列取舍与拒绝策略

> **线程池 = 背压阀**：**有界队列 + 合理 core/max + CallerRuns/Abort 推回上游**；按**依赖分池**（Bulkhead），外呼**强制超时**，命中拒绝→**降级+退避**；拒绝把 `LinkedBlockingQueue` 用成**无界**；SynchronousQueue 需配强限流；观测 `active/queue/rejected/p95` 做动态调度。

**面试官：**

“大促瞬时高峰涌进来，你们的**下游限速**，结果你这边线程池一路涨、队列越积越多，延迟飙升甚至 OOM。你会怎么**选型与调参**，既**吸收突发**又不把下游打穿？”

**你：**

“我把线程池当成**背压阀**来设计：**有界队列 + 合理的 core/max + 拒绝策略推回上游**，再配超时/重试预算/熔断。”

1 - 七参数怎么选

`ThreadPoolExecutor(core, max, keepAlive, unit, workQueue, threadFactory, handler)`

- **corePoolSize**
  - **CPU 密集**：≈ `CPU核数`（或 `核数 * 1.0`）。
  - **IO 阻塞**：按 `核数 * (1 + 阻塞比)` 估算；例如阻塞≈3 倍计算时长，可把 `max` 提高到 `~ 核数 * 4`。
- **maximumPoolSize**：允许短时**扩张吸收突发**，但要**有限度**，避免把下游压穿。
- **keepAliveTime**：突发型业务把**非核心线程** 30–120s 回收；也可 `allowCoreThreadTimeOut(true)` 让 core 也缩。
- **workQueue（关键）**：强烈建议**有界**，容量体现你的**等待预算**。
- **threadFactory**：起**可读名**（含依赖名/用途），便于诊断；可带 MDC/traceId。
- **handler（拒绝策略）**：用来**施加背压**或**快速失败**，比“无限排队”更健康。

2 - 队列选型（取舍）

- **`ArrayBlockingQueue(cap)`（推荐）**：**有界 FIFO**，简单可控，最符合“背压”。
- **`LinkedBlockingQueue`**：默认**无界**（反模式，可能 OOM）；如用务必**指定上限**。
- **`SynchronousQueue`**：**零容量**直传；适合**低延迟 + 可弹性扩线程**，但**极易打穿下游**，除非有**强兜底**（限流/熔断）。
- **`PriorityBlockingQueue`**：有优先级但**可能饿死**低优先任务；仅在确需优先级时用。

3- 拒绝策略（语义与场景）

- **`CallerRunsPolicy`（首选）**：把任务在**调用线程**执行 → **自然限速**，把压力**传回上游**；适合 BFF/同步链路。
- **`AbortPolicy`**：抛 `RejectedExecutionException` → **快速失败**，让上层走**降级/重试**。
- **`DiscardPolicy/DiscardOldestPolicy`**：静默丢弃/丢最老任务，**不建议**用于关键业务（难排障）。

4 - 反模式 RISK 清单

- **无界队列 + 巨大 max**：看似稳，其实**无限排队 → 高尾延迟/OOM**。
- **一个池干所有事**：CPU 任务与 IO 任务**混用** → 互相拖垮（饥饿/死锁）。
- **阻塞任务塞进 `ForkJoinPool`/默认 `CompletableFuture` 池** → 线程**饥饿**。
- **任务内再 `submit()` 并同步等待**（嵌套提交）→ **线程耗尽**死锁。
- **无限等待的下游调用**（无超时）→ 线程长期占用。

> 修复：**按依赖分池（Bulkhead）**；阻塞任务用**自定义 Executor**；所有外呼**必须有超时**；避免同步等待嵌套。

5 - 一套“突发但要保护下游”的实用模板

```java
int cores = Runtime.getRuntime().availableProcessors();
int queueCap = 2000; // 等待预算：能接受在本层堆多少
ThreadPoolExecutor pool = new ThreadPoolExecutor(
    Math.max(cores, 8),          // core：基础吞吐
    Math.max(cores * 4, 32),     // max：吸收突发，但有限度
    60, TimeUnit.SECONDS,        // 非核心回收
    new ArrayBlockingQueue<>(queueCap), // 有界FIFO = 背压阀
    r -> { Thread t = new Thread(r, "outbound-stock-%d".formatted(r.hashCode())); t.setDaemon(true); return t; },
    new ThreadPoolExecutor.CallerRunsPolicy() // 调用方背压
);
pool.allowCoreThreadTimeOut(true); // 突发过后及时收缩
```

**配套策略**

- 任务里**强制超时**（`TimeLimiter`/`CompletableFuture.orTimeout`）；
- 命中拒绝策略时：返回**清晰错误**或**降级**，并按**重试预算**退避；
- 指标：`active/queue/rejected/p95`，看到**队列持续高位**优先**降入口 RPS/放大下游限额/扩容**，不是“盲目加线程”。

6 - 与限流/重试/熔断的协同

- **限流**：入口**令牌桶**控制进入线程池的速率；`429 + Retry-After` 指导上游退避。
- **重试**：只对**幂等**请求；遵守**预算**，避免把队列堆满。
- **熔断**：下游异常率/超时升高时**打开熔断**，线程池**不再接活**（或改走降级）。
- **Bulkhead（分仓壁）**：为每个关键下游**单独线程池**（或信号量池），互不牵连。

7 - 诊断与观测（上线就要有）

- 导出：`poolSize/activeCount/queueSize/completedTaskCount/rejectedCount`；
- 采样**任务耗时分位**，分依赖/分接口看；
- **线程命名**可读（带依赖名），异常栈里一眼定位；
- `rejectedCount` 异常抬头 = **背压生效**，不是“坏事”，配合**降级开关**即可。

**项目口径**

“凡新我们给每个外呼（支付、库存）各一组**有界池 + CallerRuns**，配上**重试预算**，高峰期把压力稳稳**卡在调用方**，尾延迟大幅收敛；

麦克尔斯把原来单池改成**按依赖分池**，并给 `CompletableFuture` 指定自定义 Executor，解决了**默认池被阻塞任务吃光**的问题。”

### `CompletableFuture` 任务编排：并行、超时、取消与自定义 Executor

> **CF 编排**：总是用**自定义有界线程池**；子任务 `orTimeout/completeOnTimeout + 降级`，总体 `allOf` 加 **deadline**，超时**取消 siblings**；竞速用 `anyOf`；避免在 commonPool 跑阻塞 IO；通过装饰器传递 **MDC/traceId**；别在任务里嵌套阻塞等待。

**面试官：**

“在（深圳市凡新科技 / 麦克尔斯深圳）的下单页，你需要**并行**查询：价格、库存、优惠、地址校验。要求**总体超时 1.2s**，任何子调用超时要**快速降级**，并且**不要把公共线程池卡死**。你怎么用 `CompletableFuture` 编排？”

**你：**

“我有四个原则：**自定义 Executor**、**fail-fast 超时**、**可取消**、**清晰降级**。默认 `commonPool` 是 `ForkJoinPool`，**不能塞阻塞 IO**，所以我总是传入**专用线程池**（按依赖分池）。整体用 `allOf` 聚合，子任务统一 `orTimeout` + `exceptionally` 降级；一旦总体超时或关键任务失败，**取消其余**。”

**代码骨架（并行 + 超时 + 降级 + 取消）**

```java
Executor ioPool = outboundPool("checkout-io"); // 你的自定义、有界线程池

CompletableFuture<Price> fPrice = CompletableFuture
    .supplyAsync(() -> priceApi.get(sku), ioPool)
    .orTimeout(400, TimeUnit.MILLISECONDS)                // 子调用超时
    .exceptionally(e -> Price.fallback());                // 明确降级

CompletableFuture<Stock> fStock = CompletableFuture
    .supplyAsync(() -> stockApi.get(sku), ioPool)
    .orTimeout(300, TimeUnit.MILLISECONDS)
    .exceptionally(e -> Stock.unknown());                 // 可返回未知/保守值

CompletableFuture<Coupon> fCoupon = CompletableFuture
    .supplyAsync(() -> couponApi.check(user), ioPool)
    .orTimeout(300, TimeUnit.MILLISECONDS)
    .exceptionally(e -> Coupon.empty());

CompletableFuture<AddressCheck> fAddr = CompletableFuture
    .supplyAsync(() -> addrApi.validate(addr), ioPool)
    .orTimeout(250, TimeUnit.MILLISECONDS)
    .exceptionally(e -> AddressCheck.skip());

CompletableFuture<Void> all = CompletableFuture.allOf(fPrice, fStock, fCoupon, fAddr);

// 总体超时 + 取消其余
try {
    all.orTimeout(1200, TimeUnit.MILLISECONDS).join();
} catch (CompletionException e) {
    // overall timeout/fail-fast，取消还在跑的子任务（若 API 支持中断/超时，会尽快返回）
    fPrice.cancel(true); fStock.cancel(true); fCoupon.cancel(true); fAddr.cancel(true);
}
// 聚合结果（子任务已各自降级，getNow 不会抛异常）
Result r = new Result(
    fPrice.getNow(Price.fallback()),
    fStock.getNow(Stock.unknown()),
    fCoupon.getNow(Coupon.empty()),
    fAddr.getNow(AddressCheck.skip())
);
```

**常用模式**

- **Fail-fast 聚合**：关键依赖失败就**立即返回**（而不是等其它都完成）。做法：给关键依赖单独 `orTimeout/exceptionally`，并在 `handle/whenComplete` 里触发**取消 siblings**。
- **Fastest-wins（竞速）**：`anyOf(f1, f2, ...)` 选最快结果（如多机房并发读），其余任务在 `thenAccept` 里**best effort cancel**。
- **串并混排**：`thenCompose` 串行依赖（拿到 user 再查优惠）、`thenCombine` 汇合独立分支。
- **超时占位**：`completeOnTimeout(fallback, 300, ms)` —— 超时直接返回**默认值**，不抛异常。

**异常与上下文**

- **不要 `get()`/`join()` 早取**，统一在末端聚合；
- 用 `exceptionally/handle` 明确每个子任务的**降级逻辑**；
- **MDC/traceId 传递**：`CompletableFuture` 不会自动继承 MDC，包一层 `Supplier` 复制/恢复 MDC（或用装饰器 Executor）；
- **中断语义**：取消只是一种信号，确保下游客户端**尊重超时/中断**（HTTP 客户端配置 read/connect timeout），否则线程仍被占。

**反模式（常见坑）**

- 把阻塞 IO 跑在 `ForkJoin commonPool`；
- `LinkedBlockingQueue` 无界池接 CF 任务 → **无限排队**；
- 在任务内部**同步等待另一个 CF**（嵌套 `get()`）→ **线程互等**；
- 没有 `orTimeout/completeOnTimeout`，导致**整体任务无上限**；
- 降级不明确，异常被吞掉，排障困难。

**项目口径**

“凡新那边我们把 checkout 的四个外呼**并行**起来，子任务 `250–400ms` 超时，各自有 fallback；总体 **1.2s deadline** 到就**取消其余**并回前端‘受理中’。麦克尔斯那边把默认 `commonPool` 全部替换成**分池**，p95 立即下降，且**拒绝数**成了清晰的背压信号。”

### 并发诊断与排障：死锁、线程池饱和、阻塞点定位，5 分钟 SOP

> **并发排障 SOP**：指标判型（active/queue/rejected、CPU/GC、依赖 P95）→ 采样线程栈（`Thread.print` 连打 2–3 次）定位 **I/O/锁/CPU** → 当场止血（超时/降级/背压/熔断/收紧并发）→ 根因修复（有界池+超时、分池、加锁顺序、`tryLock`、降对象膨胀、重试预算）。出现 “deadlock” 即统一**锁顺序**或使用超时锁。

**面试官：**

“促销高峰里接口 P95 飙升、线程池不出活。你如何 **5 分钟内** 判断是**CPU 打满**、**下游阻塞**、还是**锁竞争/死锁**，并给出**止血**与**根因**？”

**你：**

“我有一套 **SOP**：**看指标 → 采样线程栈 → 对症止血 → 复盘修复**。”

① 先看指标（1 分钟：判型）

- **线程池四件套**：`activeCount / poolSize / queueSize / rejectedCount`
  - `queueSize` 持续攀升 + `rejectedCount` 抬头 → **池饱和/背压生效**；
  - `active≈max` 且出活慢 → 任务**被阻塞**（I/O、锁）。
- **CPU/GC**：CPU 100% + GC 次数/停顿上升 → **计算/分配压力**或**对象膨胀**（大队列）。
- **依赖红灯**：下游 P95、超时率、熔断状态；若依赖同时抬头，优先判断**下游阻塞**。

> 结论模板：
> - CPU 高 + 线程大多 RUNNABLE → **CPU 绑定/自旋/热点**；
> - CPU 不高 + 队列涨/超时多 → **外部 I/O 或锁等待**。

② 线程栈取证（2 分钟：定位）

**命令**（连打 2–3 次，间隔 5s）：

```
jcmd <pid> Thread.print -l > tdump1.txt
jcmd <pid> Thread.print -l > tdump2.txt
```

**怎么看**

- 大量线程 `TIMED_WAITING on java.net...` / `WAITING on java.util.concurrent.CompletableFuture$Signaller` → **I/O/下游慢**或**无超时**。
- `BLOCKED (on object monitor)` / `parking to wait for <...AQS>` → **锁竞争**；若 dump 顶部出现
  `Found one Java-level deadlock` → **死锁**。
- 线程名可读（建议命名如 `outbound-stock-*`），一眼看出**哪条依赖**卡住。
- **热点类目**：
  - `AbstractQueuedSynchronizer`：`synchronized/ReentrantLock/CountDownLatch` 等等待；
  - `ForkJoinPool.commonPool-worker-*`：阻塞任务误入 **commonPool**；
  - `SynchronousQueue`：直传队列 + 下游慢 ⇒ **堆线程/打穿**。

**补刀**

```
jcmd <pid> GC.class_histogram > histo.txt   # 看大对象/队列膨胀
jfr start duration=60s filename=profile.jfr # 1 分钟 JFR 看锁/IO热点
```

③ 当场止血（1 分钟：控损）

- **强制超时/限速**：临时把外呼客户端 read/connect timeout 降到 300–800ms；尊重 `Retry-After`；将**重试预算**降为 ≤5%。
- **打开降级/熔断**：对“非关键读”直接旧值/占位；关键写**受理中**。
- **收紧并发**：把问题依赖的线程池 **max/queue** 下调，命中 `CallerRuns/Abort` 让**上游减速**（背压）；或直接**临时摘除**问题下游。
- **杀环路**：发现死锁或嵌套等待，**关闭相关入口**（灰度下线）并重启单实例释放锁。

④ 根因与修复（+ 事后规范）

**常见根因 → 修复清单**

1. **无超时/大超时** → 所有外呼**必须**配置超时（请求 deadline 统一 ≤1.5–2s）；`CompletableFuture.orTimeout / TimeLimiter` 落地。
2. **池饱和（无界队列）** → 改为**有界** `ArrayBlockingQueue`；拒绝策略用 `CallerRuns/Abort`；建立**背压指标**。
3. **阻塞任务跑在 commonPool** → 给 CF 指定**自定义 Executor**（按依赖分池）。
4. **锁竞争/死锁** →
   - 固化**加锁顺序**；
   - 长等待改 `tryLock(timeout)+重试/降级`；
   - 缩小临界区/**锁分段**；
   - 用 `ReentrantReadWriteLock`/`LongAdder` 降低写冲突。
5. **对象膨胀/GC**（大队列/大集合） → **控队列上限**、分批处理、减少中间对象。
6. **重试风暴** → **指数退避 + 抖动 + 预算**；熔断打开期间不重试，仅半开探测。

代码小工具（两段，线上非常好用）

**A. 线程池探针（日志 + 指标）**

```java
class InstrumentedExecutor extends ThreadPoolExecutor {
  InstrumentedExecutor(int core, int max, int q) {
    super(core, max, 60, TimeUnit.SECONDS, new ArrayBlockingQueue<>(q),
      r -> new Thread(r, "outbound-stock-" + r.hashCode()),
      new CallerRunsPolicy());
    allowCoreThreadTimeOut(true);
  }
  @Override protected void beforeExecute(Thread t, Runnable r) {
    // 采样记录排队时长/开始时间（可放到 MDC）
  }
  @Override protected void afterExecute(Runnable r, Throwable t) {
    // 上报耗时/异常，若 t!=null 记录
  }
}
```

**B. 锁看门狗（持锁超时报警）**

```java
class TimedLock implements AutoCloseable {
  private final ReentrantLock lock; private final long start=System.nanoTime();
  TimedLock(ReentrantLock l, long warnMs) {
    this.lock = l;
    try { if(!l.tryLock(warnMs, TimeUnit.MILLISECONDS))
            throw new RuntimeException("lock timeout"); }
    catch (InterruptedException e) { Thread.currentThread().interrupt(); }
  }
  public void close() {
    long costMs=(System.nanoTime()-start)/1_000_000;
    if (costMs > 200) log.warn("Lock held {} ms", costMs);
    lock.unlock();
  }
}
// 用法：try (var g=new TimedLock(lock,100)) { /*临界区*/ }
```

死锁“一句话排查”

- 看 `Thread.print` 顶部是否有 “**Found one Java-level deadlock**”；
- 找到两个（或多）线程互相 `BLOCKED`，标出**锁对象**与**获取顺序**；
- **修复**：统一获取顺序；或拆一把锁；或把其中一个改为 `tryLock` + 超时退避。

### 突发流量 + 下游限速，线程池怎么“吸收不作死”？

**面试官：**

“促销 5 分钟峰值打过来，你们要并发调用库存与优惠服务。现在的线程池**队列越积越多**、尾延迟飙升，偶尔还 OOM。请你说说你会怎么设计 `ThreadPoolExecutor`（参数、队列、拒绝策略），以及怎么和**超时/重试/熔断**配合，既吃下突发又不把下游打穿？最好结合你在（深圳市凡新科技 / 麦克尔斯深圳）的经历给一组**可落地**的参数。”

**你：**

“我把线程池当**背压阀**来用，不当仓库。落地分四步：

1. **按依赖分池（Bulkhead）**
   支付、库存、优惠**各一组**线程池，互不拖累；`CompletableFuture` 一律用**自定义池**，不占 `commonPool`。
2. **有界队列 + 适度弹性**
   我选 `ArrayBlockingQueue`，把队列容量当作**等待预算**而不是垃圾桶。凡新那次活动我用过一组参数：
   - `core = max(8, CPU核)`，`max ≈ 核数 * 4`（IO 阻塞型业务）；
   - `queueCap = 2000`（能接受在本层最多囤 2k 个请求，再多就宁可拒）；
   - `keepAlive = 60s`，`allowCoreThreadTimeOut(true)`（峰值后尽快回收）；
   - 队列**坚决不用无界 `LinkedBlockingQueue`**，也不会在这种场景用 `SynchronousQueue`（太容易把下游打穿）。
3. **拒绝策略 = 把压力推回去**
   用 `CallerRunsPolicy` 或 `AbortPolicy`：
   - **CallerRuns**：调用线程自己跑，**自然限速**（BFF 线程被占就降速了）；
   - **Abort**：直接抛异常，BFF 把它转成**清晰的降级/受理中**。
     命中拒绝时我会**记录指标**（`rejectedCount`）并触发**灰度降入口 RPS**。
4. **协同：超时 / 重试预算 / 熔断**
   - 外呼**硬超时**（300–800ms 一档），总体**deadline**（比如 1.2s）；
   - **只对幂等**请求重试，**指数退避 + 抖动**，**预算 ≤10%**；
   - 下游错误/超时率越线**打开熔断**，熔断期**不再重试**，仅半开探测；
   - 命中 `429` 尊重 `Retry-After`，把下一次退避对齐。

**实战口径**

- 在**凡新**，库存外呼池我们配了：`core=12, max=48, queue=2000, CallerRuns`，外呼 HTTP 客户端 `read/connect=500ms`，命中拒绝就**快速失败 + 幂等重试**（预算 10%）。峰值时**rejected**会上来，但尾延迟收敛，下游也没有被打穿。
- 在**麦克尔斯**，图片索引原本用无界队列，直接把 GC 顶爆。改成**有界 + Abort** 后，前端拿到“受理中”占位；等消费链路恢复我们**异步补齐**，整体体验反而更稳。”

**一段可复用代码（骨架）**

```java
int cores = Runtime.getRuntime().availableProcessors();
ThreadPoolExecutor pool = new ThreadPoolExecutor(
    Math.max(cores, 8),
    Math.max(cores * 4, 32),
    60, TimeUnit.SECONDS,
    new ArrayBlockingQueue<>(2000),
    r -> { Thread t = new Thread(r, "outbound-stock-" + r.hashCode()); t.setDaemon(true); return t; },
    new ThreadPoolExecutor.CallerRunsPolicy() // 或 AbortPolicy
);
pool.allowCoreThreadTimeOut(true);
```

**面试官可能追问 & 我会补充**

- **为什么不用无界队列？** → 它把延迟藏起来，最后以 OOM/长尾爆雷。
- **为什么不用 `SynchronousQueue`？** → 直传 + 下游慢会疯狂拉起线程或大量阻塞，风险更大。
- **如何判定“队列该多大”？** → 结合**SLA × 目标吞吐**估算最大在途数；超出就拒绝/降级，而不是堆。
- **监控看什么？** → `active/queue/rejected/p95`；看到队列高位稳定 5 分钟，就**降入口/扩下游**而不是加线程。

### `CompletableFuture` 并行编排要做到：fail-fast + 可取消 + 明确降级

**面试官：**

“下单页要并行查：价格、库存、优惠、地址校验。要求**1.2s 总超时**；任何**关键依赖失败要立刻返回（fail-fast）**；其余子任务要**可取消**；每个子任务都要有**明确降级**。你会怎么写？（结合你在凡新/麦克尔斯的做法说说）”

**你：**

“我的套路是：**自定义有界线程池** + **每个子任务 orTimeout + 降级**，再加一段‘**首个失败即取消其他**’的钩子，整体再套一层**deadline**。”

```java
// 1) 专用有界线程池（别用 commonPool）
Executor ioPool = outboundPool("checkout-io"); // 有界 + CallerRuns/Abort

// 2) 子任务：各自 timeout + 明确降级（fallback）
var fPrice = CompletableFuture.supplyAsync(() -> priceApi.get(sku), ioPool)
    .orTimeout(400, MILLISECONDS)
    .exceptionally(e -> Price.fallback());

var fStock = CompletableFuture.supplyAsync(() -> stockApi.get(sku), ioPool)
    .orTimeout(300, MILLISECONDS)
    .exceptionally(e -> Stock.unknown());

var fCoupon = CompletableFuture.supplyAsync(() -> couponApi.check(user), ioPool)
    .orTimeout(300, MILLISECONDS)
    .exceptionally(e -> Coupon.empty());

var fAddr = CompletableFuture.supplyAsync(() -> addrApi.validate(addr), ioPool)
    .orTimeout(250, MILLISECONDS)
    .exceptionally(e -> AddressCheck.skip());

// 3) fail-fast：任一“关键依赖”异常 → 取消其余（best-effort）
List<CompletableFuture<?>> all = List.of(fPrice, fStock, fCoupon, fAddr);
all.forEach(f -> f.whenComplete((r, ex) -> {
    if (ex != null) all.forEach(o -> { if (o != f) o.cancel(true); });
}));
// 关键依赖你可以只挂在价格/库存上；非关键（比如优惠、地址）失败不触发取消

// 4) 总体 deadline（1.2s）
try {
    CompletableFuture.allOf(fPrice, fStock, fCoupon, fAddr)
        .orTimeout(1200, MILLISECONDS).join();
} catch (CompletionException e) {
    // 总体超时或关键失败：日志打点 + 统一降级响应（受理中/稍后刷新）
    fPrice.cancel(true); fStock.cancel(true); fCoupon.cancel(true); fAddr.cancel(true);
}

// 5) 聚合结果（getNow 避免再抛异常）
var result = new CheckoutView(
    fPrice.getNow(Price.fallback()),
    fStock.getNow(Stock.unknown()),
    fCoupon.getNow(Coupon.empty()),
    fAddr.getNow(AddressCheck.skip())
);
```

**工程要点**

- 在线上，**HTTP 客户端也要配置 connect/read 超时**，不然 CF 取消不了真正的阻塞。
- **关键依赖失败即刻取消**能把平均资源占用降下来，凡新促销时我们就是这么做的；麦克尔斯把图片/地址校验标成**非关键**，失败只降级，不触发 fail-fast。
- 观测：按接口维度暴露 `timeout_count / cancelled_count / fallback_ratio / pool_rejected`，一眼看出是**下游慢**还是**线程池饱和**。

**常见坑 & 一句话纠偏**

- 把阻塞 IO 跑在 `commonPool` → **改自定义有界池**；
- 没 `orTimeout/completeOnTimeout` → **加子任务超时**，避免整体被拖死；
- 在子任务里 `get()` 等另外一个 CF → **用组合算子 (`thenCombine/anyOf`)**，别嵌套阻塞；
- 只取消 CF 不设置客户端超时 → **取消信号传不到下游**；
- 降级不明确 → **每个分支都 `.exceptionally(e -> fallback)`**。

### 锁竞争 / 死锁如何 5 分钟内定位并修复？

**面试官：**

“高峰期你们的下单接口 P95 飙升，线程池 `active≈max` 但出活很慢。线程栈里很多 `BLOCKED`/`parking to wait for <...AQS>`。请你说说**如何快速确认是不是锁竞争/死锁**，以及**当场止血 + 代码层修复**？”

**你：**

“我会按 **SOP** 来：**看指标→打线程栈→当场止血→代码修复**。

**① 判型（1 分钟）**

- 线程池：`active≈max`、`queue↑`、`rejected↑`；CPU 不高（<60%），说明多半是**等待**而不是算力。
- 依赖侧 P95 正常 → 更像**锁竞争**；若依赖也慢，则优先处理下游。

**② 线程栈取证（2 分钟）**

```
jcmd <pid> Thread.print -l > tdump1.txt
jcmd <pid> Thread.print -l > tdump2.txt
```

- 看到大量 `BLOCKED (on object monitor)`（`synchronized`）或 `parking to wait for <...AbstractQueuedSynchronizer>`（`ReentrantLock`），并且 dump 顶部若出现
  `Found one Java-level deadlock` → **确定死锁**。
- 记录**锁对象地址**与**调用栈**，确认是否存在**交叉获取顺序**。

**③ 当场止血（1 分钟）**

- 暂时把热点路径换成 **`tryLock(timeout)` + 快速降级/重试**，避免长时间占用线程；
- **缩小临界区**（把远程调用/IO 移出锁内）；
- 对热点 key 做**分段锁**（`stripe = hash(key)%N`），立刻摊薄争用；
- 如果确认死锁且可灰度，**摘流/重启**单实例解除僵持，同时拉低入口 RPS。

**④ 代码修复（复盘落地）**

- **统一加锁顺序**（最根本）：所有线程都按同一顺序获取 `lockA → lockB`；
- **避免嵌套等待**：临界区只做数据结构操作；
- **可中断/超时**：用 `ReentrantLock.lockInterruptibly()` / `tryLock(t,unit)`；
- 读多写少改 **`ReentrantReadWriteLock`**；计数热点用 **`LongAdder`** 替代全局锁。

**反例（易死锁写法）**

```java
// T1 顺序：A -> B，T2 顺序：B -> A  => 经典死锁
synchronized (lockA) {
  // ...
  synchronized (lockB) { /* ... */ }
}
synchronized (lockB) {
  // ...
  synchronized (lockA) { /* ... */ }
}
```

**修正（统一顺序 / 超时退避）**

```java
// 统一获取顺序：按 hash 比较决定 A/B 的先后
Object first = System.identityHashCode(a) < System.identityHashCode(b) ? a : b;
Object second = first == a ? b : a;
synchronized (first) {
  synchronized (second) { /* 临界区仅做内存操作 */ }
}

// 或改 ReentrantLock + 超时，避免长等待卡死线程池
boolean ok = lock.tryLock(80, TimeUnit.MILLISECONDS);
if (!ok) return fallback(); // 快速降级/重试（具幂等）
try { /* 临界区 */ }
finally { lock.unlock(); }
```

**真实口径（结合经历）**

- **凡新**：活动页库存热 key 竞争，早期把**HTTP 调用放在锁内**导致队列暴涨；修复为**缩小临界区 + tryLock(100ms)**，拿不到就降级成“受理中”，线程池不再被拖死。
- **麦克尔斯**：图片处理链路出现互相持有两把锁的交叉顺序，`Thread.print` 直接报 deadlock；统一锁顺序后问题根除，并加了**锁看门狗**（持锁>200ms报警）。

**一句话总结**

> 判断：`BLOCKED/AQS` + CPU 不高 → 锁问题。止血：`tryLock(timeout)`/缩小临界区/分段锁/降级。修复：**统一锁顺序**、可中断/超时获取、读写分离与热点规避。”

### 线程池饱和 + 重试风暴，如何协同治理？

**面试官：**

“高峰期库存接口开始超时，你们的调用方按老规矩**固定间隔重试 3 次**，结果线程池**队列暴涨**、**拒绝数飙升**，还把下游**彻底打穿**。你在（深圳市凡新科技 / 麦克尔斯深圳）会怎么**同时**处理线程池饱和和重试风暴？”

**你：**

“我把它当一个**闭环控制问题**：线程池充当**背压阀**，重试遵守**预算**，熔断在异常时**切断正反馈**。落地四件事：**限流前置 → 有界池背压 → 重试预算 + 退避抖动 → 熔断/降级**。”

1 - 前置限流（入口滴灌）

- BFF/网关用**令牌桶**控进入下游线程池的速率；命中 `429` 明确返回 `Retry-After`。
- 预算估算：在途上限 ≈ `active + queueCap`，例如池 `max=48`、队列 `2000` → **最多 2048** 在途，网关把**每秒入池速率**限制在 `2048 / SLA秒` 左右。

2 - 线程池 = 背压阀（而不是仓库）

- **有界队列**（`ArrayBlockingQueue`），`CallerRuns` 或 `Abort`。
- 一旦 `queueSize` 高位稳定 ≥ 60s，就**主动降入口 RPS**或**扩容**，而不是“继续加线程”。
- 观测四指标：`active / queue / rejected / p95`。

3 - 重试要有“预算”（≤10%）+ 指数退避 + 抖动

- **只对幂等**请求重试；**固定间隔** → 全改 **指数退避 + 抖动**。
- **重试预算**：单位时间内 `retry_count ≤ 0.1 * success_count + 50`（保底 50）。用**原子计数**实现；**预算耗尽就快速失败**。
- 退避序列：`200ms, 500ms, 1.2s, 2.5s, 5s`，并加 full jitter。

**Java 伪码（带预算 + 抖动 + Retry-After）**

```java
boolean allowed = retryBudget.tryConsume(); // 原子扣减，返回是否还有预算
if (!allowed) throw new RetryBudgetExhausted();

Duration wait = nextBackoffWithJitter(attempt); // 指数退避+抖动
if (resp.is429() && resp.retryAfter() != null) {
  wait = max(wait, resp.retryAfter());          // 与 Retry-After 对齐
}
sleep(wait);
```

4 - 熔断 + 半开探测（切断风暴）

- 错误率/超时率越线（例如**错误率 >50% & 请求数>每秒 N**）→ **打开熔断**；
- 熔断打开期间**停止重试**；只在**半开**状态用少量探针请求；失败再回开路。

5 - 实战口径（我会这样说）

- **凡新**：我们把库存外呼池（`core=12,max=48,queue=2000,CallerRuns`）配了**重试预算 10%**；抖动后的退避 + 熔断后**不重试**，峰值时 `rejected` 上来但尾延迟稳定，下游没有被打穿。
- **麦克尔斯**：图片索引链路把“固定 3 次重试”改成**预算 + 抖动 + Retry-After**，并把失败任务挪到**慢车道队列**；风暴当场止住，回收后再**限速回放**。

6 - 一键止血（值班 SOP）

- 立刻把客户端**read/connect timeout**降至 `300–800ms`；
- 将线程池 `max` 和 `queueCap` **下调**，命中 `CallerRuns/Abort` 把压力推回；
- 开启**降级**（返回旧值/受理中）；
- 打开/收紧**熔断阈值**；
- 查询并临时禁止**固定间隔重试**的调用方（脚本批量下发配置）。

7 - 反模式（必须一口气指出）

- **无界队列 + 固定间隔重试** → 高尾延迟 + OOM + 打穿下游；
- **熔断打开仍在重试** → 负反馈；
- **忽略 `Retry-After`** → 和对方节拍打架；
- **写请求无幂等键还重试** → 双扣/乱序副作用。

**一句话总结**

> **有界池 + 背压（CallerRuns/Abort）** 吸收突发，**重试=预算化+退避抖动**，**熔断**在异常期切断正反馈，入口**令牌桶**稳节奏——四管齐下才能把风暴压成可控波动。

### 线程池监控与告警：看哪些指标？阈值怎么定？

**面试官：**

“你在（深圳市凡新科技 / 麦克尔斯深圳）如何给 `ThreadPoolExecutor` 做**可观测**？具体**哪些指标**、**告警阈值**、**看板布局**，以及**值班 SOP**怎么写？”

**你：**

“我用‘**SLO → 指标 → 告警 → 看板 → Runbook**’五件套。”

1 - 指标（Pool 维度 + 任务维度）

**Pool 维度（核心）**

- `pool_active`（当前活跃线程数）
- `pool_max` / `pool_core`
- `queue_size` / `queue_capacity`（求 `queue_fill_ratio = size/cap`）
- `rejected_total`（按原因分：`abort`/`callerRuns`/`discard`…）
- `completed_total`、`task_submitted_total`

**任务维度（直观反映体验）**

- `task_exec_seconds` 直方图（P50/P95/P99）
- `task_wait_seconds`（入队→开始执行的排队时间）
- `timeout_total`、`cancelled_total`（CF/HTTP 客户端统计）
- 依赖侧：外呼 **成功率**、**超时率**、**429/5xx** 比例（对齐重试/熔断）

> 线程命名要可读（如 `outbound-stock-*`），日志和 dump 一眼定位。

2 - 告警（Prometheus/语义）

**饱和预警（持续 2–5m）**

- `active/max > 0.8` **并且** `queue_fill_ratio > 0.7` 且上升
- 或 `rate(rejected_total[1m]) > 0` 连续 3 分钟
- 或 `task_exec_seconds{quantile="0.95"} > SLA×1.5` 持续 5 分钟

**风暴/退化**

- `rate(rejected_total[1m]) > 50`（硬拒绝暴涨）
- `rate(timeout_total[1m]) > X` 且 熔断**关闭** → 提醒**开启熔断**
- `rate(callerRuns_total[1m])` 持续上升 → **上游被背压**（通知降入口）

**示例规则（伪 PromQL）**

```yaml
- alert: PoolSaturation
  expr: (executor_active / executor_max > 0.8)
        and (executor_queue_size / executor_queue_cap > 0.7)
        and on(pool) (deriv(executor_queue_size[2m]) > 0)
  for: 3m
  labels: {severity: warning}
  annotations: {runbook: "rb-threadpool-saturation"}

- alert: PoolRejectSpike
  expr: rate(executor_rejected_total[1m]) > 20
  for: 2m
  labels: {severity: critical}
  annotations: {runbook: "rb-threadpool-reject"}
```

3 - 看板布局（1 屏能判型）

- **Top 行（总体）**：P95 延迟、成功率、重试率、熔断状态。
- **中间（Pool）**：`active/max`、`queue_size/cap`、`rejected`、`callerRuns` 叠图 + 斜率；
- **底部（任务）**：`task_wait_seconds` 与 `task_exec_seconds` 分布；依赖侧 5xx/429/超时趋势。
- 边上放**实例 TopN**（按 `queue_size` 和 `rejected` 排），便于点名。

4 - 采集与埋点（Micrometer 示例）

```java
MeterRegistry r = ...;
ThreadPoolExecutor p = /* 你的有界池 */;
Gauge.builder("executor_active", p, ThreadPoolExecutor::getActiveCount).register(r);
Gauge.builder("executor_queue_size", p, e -> e.getQueue().size()).register(r);
Gauge.builder("executor_queue_cap", p, e -> ((ArrayBlockingQueue<?>)e.getQueue()).remainingCapacity()
                                  + e.getQueue().size()).register(r);
Counter rejected = Counter.builder("executor_rejected_total")
    .tag("pool","outbound-stock").register(r);
// 在自定义 RejectedExecutionHandler 里 rejected.increment();
Timer execTimer = Timer.builder("task_exec_seconds").publishPercentiles(0.5,0.95,0.99).register(r);
// 在 InstrumentedExecutor.afterExecute 里记录耗时；waitTime 可在 beforeExecute 采样
```

5 - 值班 Runbook（5 条当场动作）

1. **判断类型**：看 `active/max`、`queue_fill_ratio`、`rejected`、依赖侧 P95。
2. **当场止血**：
   - 降**客户端 read/connect 超时**到 300–800ms，开启/收紧**熔断**；
   - 降入口 RPS 或开**灰度降级**；
   - 临时**下调 pool.max/queueCap**，让 `CallerRuns/Abort` 生效（把压力推回）。
3. **重试收敛**：把**固定间隔重试**切换为**预算 ≤10% + 指数退避**。
4. **排查根因**：`jcmd Thread.print -l` ×3，确认 I/O/锁/CPU；必要时 JFR 60s。
5. **复盘修复**：无界队列→有界；阻塞任务→自定义池；热点锁→`tryLock(timeout)`/分段；默认 CF 池→分池。

**项目口径（口语）**
“凡新我们把‘队列斜率>0’作为**早预警**，常能在拒绝暴涨前降入口；麦克尔斯把 `callerRuns_total` 暴露出来，一看到上升就知道**上游已被背压**，能协同前端做降级。”

**一句话总结**

> **看“饱和 + 斜率 + 拒绝”三件事**：`active/max`、`queue_fill_ratio`、`rejected`。告警触发时按 Runbook：**限时/熔断/降入口/收紧池/退避重试**；看板一屏判型、指标能回放，问题就很难失控。

### 把并发策略整合落地：一段可直接用于面试的完整口语回答

**面试官：**

“黑五/双 11 高峰时，你如何**端到端**保证下单链路既扛得住突发、又不把下游打穿？请结合你在（深圳市凡新科技 / 麦克尔斯深圳）的实战，说清楚：线程池/队列/拒绝策略、超时与取消、重试与熔断、锁/热点治理、监控与止血动作。最好给出可以落地的参数与结果。”

**你：**

“我把它当一条‘**受控的水路**’来设计：入口限流 → 线程池背压 → 可取消的并行调用 → 预算化重试 + 熔断 → 热点锁治理 → 可观测与值班 SOP。

1. **入口节拍**
   - 网关上**令牌桶**先限速，避免把洪水一次性压到应用。命中 `429` 带 `Retry-After`。
2. **线程池 = 背压阀，不是仓库**
   - 按依赖**分池**（支付/库存/优惠各一组），避免相互拖垮；
   - 用**有界队列** `ArrayBlockingQueue`，队列容量就是**等待预算**；
   - 拒绝策略用 **CallerRuns**（让上游自然减速）或 **Abort**（快速降级）。
   - 在凡新库存外呼上，我们落了：`core=12, max=48, queue=2000, keepAlive=60s, allowCoreTimeout=true, CallerRuns`。
   ```java
   ThreadPoolExecutor pool = new ThreadPoolExecutor(
       12, 48, 60, TimeUnit.SECONDS,
       new ArrayBlockingQueue<>(2000),
       r -> new Thread(r, "outbound-stock-" + r.hashCode()),
       new ThreadPoolExecutor.CallerRunsPolicy()
   );
   pool.allowCoreThreadTimeOut(true);
   ```
3. **并行外呼：fail-fast + 可取消 + 明确降级**
   - `CompletableFuture` 一律用**自定义有界池**，子调用 `orTimeout`，总体加 **deadline**；关键依赖失败**立刻取消 siblings**：
   - 价格/库存 250–400ms 超时；总 **1.2s** deadline；每个分支 `exceptionally` 落到明确的 fallback（例如“受理中”/“未知库存”）。
4. **重试=预算化 + 退避抖动；熔断切回路**
   - 只对**幂等**请求重试；**指数退避 + full jitter**，**预算 ≤10%**（`retry_count ≤ 0.1 * success + 50`）；
   - 熔断打开期间**不重试**，仅半开做少量探测；命中 `429` 对齐 `Retry-After`。
5. **热点与锁**
   - 把**远程调用移出临界区**，热点路径改 `tryLock(80–100ms)+降级`，必要时做**锁分段**（`stripe = hash(key)%64`）；
   - 计数/埋点用 **LongAdder** 替代全局锁；读多写少的结构换 **读写锁**（只允许降级）。
6. **可观测 & 值班 SOP**
   - 指标：`active/max、queue_size/cap、rejected、task_wait/exec p95、timeout、cancelled、依赖侧 5xx/429`；
   - 告警阈值：`active/max>0.8 && queue_fill>0.7 && 斜率>0` 连续 3 分钟；`rejected` 突增；`p95>SLA×1.5`。
   - 止血流程（5 分钟）：
     1. 降 HTTP **read/connect** 超时到 300–800ms；
     2. 开/收紧**熔断**与**降级**；
     3. **下调 pool.max/queueCap** 让背压生效；
     4. 降入口 RPS；
     5. `jcmd Thread.print -l`×3 判断是 I/O 还是锁，必要时 **tryLock+降级** 或热重启单实例解除死锁。

**结果（实话实说）**

- 在**凡新**，活动高峰把库存外呼从“无界队列+固定重试”改为上述方案后，**尾延迟 P95 从 ~1.8s 降到 ~350ms**，即使 `rejected` 有抬头，下游 QPS 仍稳定、不被打穿；
- 在**麦克尔斯**，图片索引链路原先 OOM，是无界排队 + 固定重试导致。我们切到**有界 + Abort + 预算化重试 + 慢车道回放**，**GC 恢复稳定**，用户侧得到“受理中/稍后刷新”的一致体验。”

**一句话总结**

> 限流稳节拍，**有界池+CallerRuns/Abort** 做背压，`CF` **超时+取消+降级**，重试**预算化**并与**熔断**协同，热点锁用 **tryLock/分段/缩小临界区**，配上指标与 SOP——这套在我们两家公司的高峰都扛过实战。”

---

## Observability and Release

### 日志 / 指标 / 追踪（OTel / Prom / Grafana）

场景 A - Trace 贯穿与日志关联

**面试官：** 你如何保证一次请求的 TraceID 能跨多个微服务，并且在日志里也能串起来？

**我：** 我用 OTel 的 W3C `traceparent`/`tracestate` 头做上下文传播，Java 侧用 OTel Java agent 或 spring-boot autoconfigure 自动注入。HTTP 客户端/服务端会把上下文放进线程上下文。日志侧用 SLF4J MDC，logback pattern 带上 `%X{trace_id} %X{span_id}`。这样 Grafana 里看到异常点，点回 trace，再用 traceId grep 日志就能一跳到位。

**面试官：** 给个日志模式的例子？

**我：** logback pattern 类似：`%d{ISO8601} %-5level [%thread] %logger{36} traceId=%X{trace_id} spanId=%X{span_id} - %msg%n`。生产环境建议结构化 JSON，字段名固定，避免模糊解析。

**面试官：** 高并发下如何避免漏链路或乱串？

**我：** 三点：1 - 所有出口都用支持上下文的 HTTP/gRPC 客户端；2 - 禁掉线程池里“丢上下文”的自定义包装，或用 `ContextPropagators`/`TaskDecorator` 传递上下文；3 - 异步回调也要从 `Context.current()` 取 span。

场景 B - 指标基线与 RED/USE

**面试官：** 你在 Grafana 看一个服务，最先看哪些指标？

**我：** 先 RED：Rate（QPS/吞吐）、Errors（4xx/5xx、失败率）、Duration（p50/p90/p95）。系统面看 USE：Utilization（CPU/内存/线程/连接池占用）、Saturation（排队长度、GC、磁盘队列）、Errors（系统级错误）。这两套能快速定位是应用逻辑问题还是资源瓶颈。

**面试官：** 直方图怎么设置更有用？

**我：** 用 Micrometer/OTel 的 histogram，把关键接口打直方图，并且合理分桶，比如 API 延迟 10ms–5s 对数分桶，避免过细导致时序规模爆炸；为高价值接口开启 exemplars，这样能从 p95 点直接跳到样本 trace。

**面试官：** 标签会不会把 Prometheus 撑爆？

**我：** 控卡三条：1 - 禁止高基数标签（如 userId、请求体 hash）；2 - 控制 path 归一（模板化 `/orders/{id}`）；3 - 聚合维度有限白名单，非必要不打点。必要时在 collector 侧做 relabel/drop。

场景 C - 故障排查路径（p95 飙高）

**面试官：** 晚高峰 p95 从 120ms 涨到 900ms，你怎么排？

**我：** 先看 RED，确认是全局还是某两个接口；再看下游依赖面板（DB/Redis/外部 API）的 duration 和错误率；看 Saturation：线程池队列、连接池等待、GC、CPU 抢占。如果某接口异常，点 exemplars 拉一条慢 trace，看 span 哪一段膨胀，是 SQL 慢、重试风暴还是外部超时。最后回日志，用 traceId 定位代码上下文。

**面试官：** 如果是数据库导致的呢？

**我：** 短期：提高连接池上限+慢查询阈值观测、加缓存/只读副本、扩大超时并降低重试次数。长期：索引/SQL 计划优化、写读分离、热点拆表。并把回滚/降级开关写进 playbook。

场景 D - 采样与成本

**面试官：** Trace 量很大你怎么控成本？

**我：** 常规用 head sampling，比如 5–10%，并为错误/高延迟的请求动态提高采样；关键交易链路可按路由白名单全采。高流量场景用 tail sampling 在 collector 聚合后决定保留“异常/代表性”的 trace。日志用事件级采样（如相同异常 1 分钟仅采集 N 条）。

**面试官：** 什么时候必须全量？

**我：** 金融结算、风控审计或发布窗口短时全量，保证可追溯；窗口外恢复采样，避免存储爆炸。

场景 E - 日志质量与脱敏

**面试官：** 如何保证日志既可排障又不泄露隐私？

**我：** 结构化日志 + 字段级脱敏：手机号/邮箱打掩码或哈希；严禁落原始密钥/令牌；异常栈限制深度；在网关或 SDK 统一过滤敏感 query/body。落地上通过集中规则（logback turbo/filter 或 sidecar filter）统一治理。

**面试官：** 线上临时调试怎么做而不打扰业务？

**我：** 用日志级别动态开关（按 traceId/用户/路由范围），短时间提升到 DEBUG 并设置采样上限；或者用“诊断标记”头，仅对带标记的请求增加附加日志和 span 事件。

场景 F - 看板到行动（Runbook）

**面试官：** 看到错误率升高到 3%，你会怎么做？

**我：** 先确认是否真错误（4xx 分离），若 5xx 超出阈值，执行 Runbook：1 - 启动限流/熔断，止血下游；2 - 切回上一稳定版本或关闭灰度批次；3 - 打开降级开关；4 - 在 10 分钟观察窗内复核 RED 和关键下游指标；5 - 复盘记录 root cause、修复项和阈值是否需要重估。

### SLO / 告警阈值 / 误差预算

口袋清单（带走就能用）

- 先定**用户视角**的 SLI，再定 SLO，再考虑是否对外成 SLA。
- 事件型 SLI 最易落地：成功率 & p95/p99；入口统一计数、重试去重。
- 告警用“症状页警 + 原因工单”，多窗口燃尽率（1h 快、6h 慢）。
- 错误预算是**发布闸门**与**稳定性投资**的货币。
- 4xx 单列看板；确属策略问题的 4xx 需要进可用性讨论。

高分表达

- “我们用**多窗口燃尽率**做页警，症状导向叫醒人、原因导向建工单，既及时又不扰民。”
- “错误预算不是 KPI，而是**发布节奏与降级开关**的决策依据。”

场景 A - 把概念讲清楚

**面试官：** SLI、SLO、SLA 有什么区别？

**我：** SLI 是可量化的“怎么衡量好坏”（如成功率/延迟）；SLO 是我们对 SLI 设定的目标（例如 99.9% 月可用性）；SLA 是对外承诺/赔偿条款。我会先定 SLI，再给出合理的 SLO，最后再谈是否作为 SLA 对外承诺。

场景 B - 怎么选 SLI 才不踩坑

**面试官：** 给一个 API 服务，你会选什么 SLI？

**我：** 两类：

1. 可用性（事件型）：成功请求数 / 有效请求总数（2xx+部分 3xx），4xx 单独统计不计入后端错误；
2. 延迟（事件型）：p95/p99 小于阈值的占比。
   计数口径在**入口层**统一，避免各服务各算；对重试要去重或按“用户视角”计次，避免“重试风暴把失败率算双份”。

场景 C - SLO 怎么定

**面试官：** 99.9% 与 99.99% 怎么取舍？

**我：** 看业务损失与成本曲线。99.99% 月度只允许 ~4.3 分钟的错误预算，排障/发布窗口都被压缩；若团队值守和自动化不足，99.99% 反而导致频繁告警与回滚。我一般：核心交易 99.95% 或 99.99%，一般读 API 99.9%，并在高峰期/大促临时提升采样与观察窗口。

场景 D - 错误预算怎么“用”

**面试官：** 说说错误预算与发布节奏的关系。

**我：** 错误预算 = 1 − SLO。比如 99.9% 月度预算约 43.2 分钟。如果近期 burn rate 高/预算见底，我会：1 - 冻结非紧急发布；2 - 开启降级策略（缓存/静态页/关闭次要特性）；3 - 聚焦稳定性工作（回放测试、限流、熔断、超时统一）。预算“花得其所”，而不是“花光就挨打”。

场景 E - 告警分级与多窗口燃尽率

**面试官：** 你如何设告警阈值，既能及时又不扰民？

**我：** 用“多窗口 + 燃尽率（burn rate）”组合：

- **快速页警**：1 小时窗口，burn rate≈14（意味着很快把预算烧掉）→ 立刻叫醒人；
- **缓慢工单**：6 小时窗口，burn rate≈6 → 创建工单/白天处理。
  这类“症状导向”的告警（失败率/高延迟）用于**叫醒人**；“原因导向”（CPU、队列、连接池）降一级，只**发工单**。

场景 F - 4xx 飙升算不算“可用性”失败

**面试官：** 如果 4xx 飙升到 10%，算不算 SLO 违约？

**我：** 后端可用性 SLI 通常不把**用户侧错误**（4xx）算入失败，但需要**单独面板**盯住（比如 401/429）。场景允许的话，可把“确实由我们策略导致的 429”记为部分失败，避免通过“把错误算给用户”来掩盖问题。

场景 G - 延迟类 SLO 与指标选择

**面试官：** 只看平均延迟可以吗？

**我：** 不行。要看 p95/p99，并关注**分位数占比**（如“p95<300ms 的请求 >= 99%”）或**尾延迟预算**。发布/灰度期间建议用 exemplars 把 p95 面板直接跳转到对应 trace，把问题定位到具体依赖或 SQL。

场景 H - 预算见底时要做什么

**面试官：** 错误预算剩余 <25% 了，你接下来怎么做？

**我：** 执行预案：1 - 冻结发布；2 - 针对 Top N 故障模式出修复计划与回归用例；3 - 分析告警噪声，优化阈值或聚合；4 - 复盘把“根因→改进→验证”闭环，并视情况调整 SLO（不盲目拔高）。

### 发布与回滚（灰度 / 蓝绿 / 金丝雀）

口袋清单

- 部署与发布解耦：**先部署、后放量、可回滚、开关止血**。
- 金丝雀看“**症状指标**”（错误率/延迟/SLO），双窗口判定，快速窗做页警。
- DB 变更按 **expand→migrate→contract**，回滚只回应用不回破坏性 schema。
- 健康 = readiness（接流量）≠ liveness（存活）；加 **preStop** 保证优雅下线。
- 依赖超时/重试/幂等一致，防“重试放大器”。

高分表达

- “我们用**分阶段放量 + 症状阈值闸门**，触发即关特性或回滚，爆炸半径始终可控。”
- “**数据库遵循 EMC 三段式**，任何时候都能安全退回上一稳定版本。”

场景 A - 选择发布策略

**面试官：** 滚动、蓝绿、金丝雀怎么选？

**我：** 先看“风险 × 影响面”。小改动、无状态服务→滚动；需要极速回滚/零停机→蓝绿（流量一键切回旧环境）；高风险或对尾延迟敏感→金丝雀，按 1%→5%→25%→100%节拍放量，用指标做闸门。受限资源时，滚动最省，蓝绿最贵，金丝雀介于两者之间但需要更强的度量与自动化。

场景 B - 健康检查与优雅下线

**面试官：** 你如何避免刚上线就被流量打爆或“假健康”？

**我：** 三件事：

1. `readinessProbe` 作为“可接流量”的唯一入口，启动时加 `startupProbe` 防止冷启动被判死。
2. **优雅下线**：`preStop` + `terminationGracePeriodSeconds`，让连接池/队列清空。
3. **预热**：新 Pod 先自检、拉缓存、建连接，再放入负载均衡。必要时配短暂**黏性会话**避免状态打散。

场景 C - 自动回滚的阈值与观察窗

**面试官：** 金丝雀回滚用什么阈值更靠谱？

**我：** 我用“**多指标 + 双窗口**”：

- 快窗 5 分钟：失败率 > 2×基线且绝对值 > 1%；p95 延迟 ↑40%；饱和度（队列/连接池）> 80%。
- 慢窗 30 分钟：burn rate 触发（错误预算燃尽率超阈）→回滚或冻结放量。
  只盯症状类指标（错误率、延迟、SLO 违约），资源类（CPU、内存）只做佐证，避免“假阳性”。

场景 D - 数据库变更与可回滚

**面试官：** 上线涉及数据库 schema 怎么做才能可回滚？

**我：** **expand → migrate → contract** 三段式：

1. **expand**：向后兼容的新增（加列/加索引/加可空列），老版本也能用；
2. **migrate**：应用双写/回填/灰度读，验证新路径；
3. **contract**：确认稳定后再删旧列/旧逻辑。
   回滚只回**应用版本**，不回“破坏性 schema”；若必须改非兼容项，先做影子表/影子写，确认正确再切换。

场景 E - 特性开关与“部署≠发布”

**面试官：** 如何降低一次发布的爆炸半径？

**我：** **先部署，后发布**。用**特性开关**控制新功能暴露面：按人群、按比例、按租户逐步打开；遇到问题先“关开关”而不是“滚版本”。关键链路配**熔断/降级开关**，比如关闭推荐、降级为静态页，确保核心交易可用。

场景 F - 兼容性与依赖治理

**面试官：** 多服务协同升级怎么避免“鸡生蛋”问题？

**我：** **向后兼容的接口版本化**（v1/v2 并存），先上游，后下游；或引入**适配层**在网关做协议转换。依赖超时、重试、幂等都要统一：外呼接口先降重试，避免“重试风暴放大故障”。

场景 G - 突发事故：内存泄漏

**面试官：** 新版本导致内存飙升，你怎么处置？

**我：** 立即**停止放量**，把金丝雀权重降到 0，观察 5 分钟；若旧版本一切正常，**回滚**并触发 Runbook：1 - 收集堆快照/指标与异常样本 trace；2 - 评估是否需要限流/降级保护下游；3 - 事后复盘：泄漏点、回归用例、阈值是否需要调整。

场景 H - Kubernetes 细节落地

**面试官：** K8s 下你会启用哪些发布保护？

**我：**

- **RollingUpdate**：`maxUnavailable=0`、`maxSurge=1` 保证容量；关键服务加 **PodDisruptionBudget**。
- **分批金丝雀**：Argo Rollouts / Flagger 配 1%→5%→25%→50%→100% 阶梯与指标闸门。
- **回滚**：`kubectl rollout undo` 或在 CD 系统一键回滚到上个 Stable。
- **配置幂等**：镜像不可变 tag、ConfigMap/Secret 版本化，避免“回滚代码却加载新配置”。

---

## Kubernetes / Cloud-Native

### Pod & Container 基础：Pod 为什么是最小调度单位；资源请求/限制；重启策略；日志规范

- **Pod 是“进程组”**：同 IP/localhost、共享 Volume、同调度命运；适合主容器 + sidecar/Init 协作。
- **requests 决定放得下，limits 决定用得了**：延迟敏感服务常用“**内存有限、CPU 不限**”避免 throttle；关注 QoS 级别。
- **OOM 与 CrashLoop**：内存超限 → OOMKill；启动期/探针过严 → CrashLoopBackOff；排查用 `describe`、`logs -p`、事件时间线。
- **Init/Sidecar 生命周期**：Init 只做前置短任务；Sidecar 要配 `preStop` 与合理终止宽限，避免截断请求。
- **日志首选 stdout/stderr + JSON**：集中采集、字段固定（建议含 `trace_id`/`span_id`）；避免无界本地文件。
- **临时存储有额度**：为 `emptyDir`/日志等设 **ephemeral-storage** request/limit，防驱逐；监控磁盘压力量表。
- **镜像不可变**：用 **Digest 或不可变 tag**，避免“回滚代码却拉到新镜像”。

> “把 Pod 当作一个**共享网络与卷的 OS 进程组**；**调度看 requests，约束看 limits**。对延迟敏感应用，我们**内存设限、CPU 不限**，日志走 stdout 的结构化 JSON，再用 sidecar/daemon 采集。”

场景 A - 为什么调度单位是 Pod 而不是容器

**面试官：** K8s 为什么以 Pod 为最小调度单位？

**我：** 因为同一 Pod 里的容器**共享一组资源与命名空间**：一个 IP/端口空间、同一 localhost、共享 Volume、同一调度命运。这让**主容器 + 辅助容器**（如 sidecar 代理、日志收集、初始化任务）能在**进程间通信成本极低**的前提下协作。调度放到“Pod 级别”，能把这些强耦合进程一次性编排、扩缩和回滚。

场景 B - 资源请求/限制与 QoS

**面试官：** `requests` 和 `limits` 分别起什么作用？如何避免性能抖动？

**我：** `requests` 影响**调度**与预留；`limits` 由 cgroup **强制约束**。

QoS 取决于两者配置：Guaranteed（req=lim 且全量设置）> Burstable > BestEffort。

性能要稳：

- 对**延迟敏感**服务，常用“**设内存 limit，CPU 不设 limit**（仅设 request）”避免 CPU throttle；
- 内存超限会被 **OOMKill**，要留安全裕度，并观测 RSS 与堆外内存；
- 结合 HPA 扩容，用**平均利用率**而不是瞬时峰值，配稳定窗口防抖。

场景 C - 重启策略与常见陷阱

**面试官：** `restartPolicy` 有哪些，和控制器的关系是？

**我：** Pod 级有 `Always`/`OnFailure`/`Never`。Deployment/ReplicaSet 强制 `Always`（服务型进程），Job 通常 `OnFailure`。

常见陷阱是**把启动失败**误当业务错误：探针/环境变量缺失导致 **CrashLoopBackOff**。

排查顺序：`kubectl describe` 看事件 → `kubectl logs -p` 看上次崩溃日志 → 校验探针与资源是否过严。

场景 D - Init/Sidecar 与生命周期

**面试官：** Init 与 Sidecar 你怎么用？

**我：** **InitContainer** 做**一次性前置**（拉配置、等待依赖、做数据迁移的“哨兵检查”）；全部成功后主容器才启动。**Sidecar**长期伴随主容器（如 envoy、日志/指标收集）。

注意：

- Init 不能做长时任务，否则**阻塞扩容**；
- Sidecar 需要**优雅下线**（`preStop` + 合理 `terminationGracePeriodSeconds`），否则请求可能在退出时被截断。

场景 E - 日志与临时存储

**面试官：** 线上日志怎么落？为什么很多团队不建议写本地文件？

**我：** **首选 stdout/stderr + 结构化 JSON**，由节点或 sidecar 采集器（如 Fluent Bit）拉走。

写本地文件易踩：**轮转不可控、占用临时存储、迁移丢失**、侧车再采集一跳延迟。

若必须本地，挂 `emptyDir` 并**限制临时存储 request/limit**，防止因磁盘压力被驱逐。

### Service / Ingress / Gateway：流量路径；超时/重试/黏性会话；常见 502/504 排查口径

- **路径心智图**：Client → DNS → 外部 LB → **Ingress/Gateway(L7)** → **Service(L4)** → Pod。
- **Service 取舍**：ClusterIP 内网；NodePort 兜底；LoadBalancer 云上对外。公网常用 **LB + Ingress**。
- **502 vs 504**：502 看**上游断连/协议/TLS/探针**；504 看**超时不匹配**（Ingress 超时 < 应用时长）。
- **超时/重试对齐**：**外松内紧**，幂等请求才有限重试；加**抖动回退**，避免放大故障。
- **会话保持**：优先**无状态**；粘性仅作短期权衡，注意**滚动升级 + TTL**。
- **真实 IP 与限流**：信任链路明确；`X-Forwarded-For`/Proxy Protocol 配套；限流优先按**令牌/租户**。

> “我把流量治理分层：**L7 的 Ingress/Gateway 负责路由与策略，L4 的 Service 负责发现与均衡**；**超时与重试外松内紧**，502 查协议/探针，504 查超时链路，尽量无状态避免粘性副作用。”

场景 A - 从 Client 到 Pod 的流量路径

**面试官：** 说一下从用户到后端 Pod 的典型路径，以及各层的职责？

**我：** 常见是 **Client → DNS → 外部 LB → Ingress/Gateway → Service → Pod**。

- **Ingress** 是 L7 路由与 TLS 终止（基于域名/路径），**Gateway API** 是 Ingress 的升级版，能力更细（超时、重试、流量分配更原生）；
- **Service** 做 L4 发现与负载（ClusterIP/NodePort/LoadBalancer），屏蔽 Pod 漂移；
- **Pod** 由 kube-proxy/iptables/ipvs 命中，落到后端容器。

场景 B - ClusterIP / NodePort / LoadBalancer 什么时候用

**面试官：** 三种 Service 类型你怎么取舍？

**我：**

- **ClusterIP**：集群内访问（默认）；
- **NodePort**：简单对外、无云厂商 LB 时兜底；
- **LoadBalancer**：云上最常用，对外暴露一个公网/私网 VIP。

通常**公网进来用 LB + Ingress**；集群内服务间通信用 ClusterIP。

场景 C  - 502 vs 504 如何快速定位

**面试官：** 线上有时 502，有时 504，你怎么分辨与排查？

**我：**

- **502** 多为**上游断连/握手失败**（后端没起好、协议不匹配、TLS 终止错位、readiness 失败）；
- **504** 是**超时**（Ingress/Gateway 超时 < 应用处理时长）。

排查顺序：看 Ingress/Gateway 日志与**后端探针**；核对**超时链路**（Client / Ingress / Service / 应用）是否一致；若是 gRPC，要检查 **:authority / HTTP2** 与 **grpc-timeout** 是否正确。

场景 D - 超时与重试的对齐

**面试官：** 超时与重试在 Ingress、应用、客户端各如何配置更稳妥？

**我：** 原则是**从内到外逐级放宽**、且**只对幂等请求有限重试**：

- **客户端**设置**总超时**；
- **Ingress/Gateway** 设置**请求超时 < 客户端**，并限制**最大重试次数 + 抖动回退**；
- **应用**内部更短的**下游超时**，避免被外层重试放大。

同时把**429/503**这类**可重试**与**不可重试**错误区分，避免“重试风暴”。

场景 E - 会话保持与无状态改造

**面试官：** 要不要开 sticky session？

**我：** 能**无状态**就无状态，状态放**外部存储（Redis/DB）**。

确需粘性：

- 短期解决“冷缓存命中率低/长连接握手成本高”；
- 注意**滚动升级**时粘性会把旧 Pod“粘死”，要配较短 cookie TTL 或在灰度阶段只对小流量粘性。
- 长期建议**一致性哈希或外部会话**来替代。

场景 F - 真实客户端 IP 与限流

**面试官：** 应用层如何拿到用户真实 IP 做审计/限流？

**我：** 在 Ingress/Gateway **保留并信任** `X-Forwarded-For` / `X-Real-IP`，应用只读**受信代理**插入的头。若外部 LB 用 **Proxy Protocol**，Ingress 要同步开启。限流优先按**用户令牌/租户**，IP 只作兜底（NAT 下 IP 不稳定）。

### Probes 与优雅下线：startup/readiness/liveness 的边界；`preStop` + `terminationGrace`；预热与缓存

- **三分工**：startup 保护冷启动；**readiness 是唯一接流量闸门**；liveness 只为**自愈重启**。
- **轻量 readiness**：只测“可安全接流量”的内部条件；外部依赖用**熔断/降级**而非探针硬连。
- **优雅下线五步**：`SIGTERM → readiness false → preStop 排空 → 宽限期 → SIGKILL`；HTTP/2/gRPC 要做 **drain**。
- **发布保护**：`startupProbe` 足够宽松、**预热**后才 ready；`maxUnavailable=0`、`maxSurge=1`、关键服务加 **PDB**。
- **时序一致性**：Ingress/Gateway/应用的**超时与 Keep-Alive** 与 `terminationGrace` 对齐，避免 502/504。
- **观测字段**：在日志/指标中记录 **probe 状态变化、终止信号、排空耗时、未完成请求数**，便于复盘。

> “我们把 **readiness 当唯一闸门**、**liveness 只做自愈**，发布时先 `SIGTERM`→`readiness=false`→`preStop` 排空并对 gRPC 做 **drain**，让**时序与超时**严格对齐，就不会在滚动升级里炸出 502/504。”

场景 A - 三类 Probe 到底怎么分工

**面试官：** startup、readiness、liveness 的边界？

**我：**

- **startupProbe**：**冷启动期的保护罩**。在它“放行”前，liveness/readiness 都**不生效**，避免慢启动被误杀。
- **readinessProbe**：**唯一接流量的闸门**。失败 = 从 Service 端点摘除，不再分到新请求。
- **livenessProbe**：进程**自愈**开关。持续失败 = kubelet **重启容器**。

调参：先给 startup 足够的 `initialDelaySeconds`/`failureThreshold`，readiness 再严格一些，liveness 最保守，避免“自杀循环”。

场景 B - “假健康报错”与依赖检查

**面试官：** 常见“健康但报错”的原因？

**我：**

两类坑：

1. **探针写太重**：readiness 每次都跑依赖探活（连 DB/外部 API），一旦下游抖动就把自己摘流，形成雪崩；
2. **指标口径错**：liveness 监控业务错误码，导致小抖动触发重启。

实践：**readiness 仅检查“是否能安全接流量”**（线程/队列/连接池可用、关键本地依赖就绪），**下游依赖用熔断/降级**兜住，不要让探针放大故障。

场景 C - 优雅下线的完整序列

**面试官：** 描述一次优雅下线的时序。

**我：**

1. **接收 SIGTERM**（或节点驱逐/滚动升级触发）；
2. 立刻让 **readiness 返回失败** → 从 Service 端点摘除，**不再接新流量**；
3. 执行 **preStop**：停止接受新连接，开启**连接/队列排空**，等待飞行中的请求完成；
4. 在 `terminationGracePeriodSeconds` **宽限期**内完成收尾；
5. 若仍未完成，最后 **SIGKILL** 强制终止。

细节：HTTP/2/gRPC 要开启**连接耗尽**（drain），设置**服务器/反代的 Keep-Alive 超时**覆盖宽限期，避免升级时 502/504。

场景 D - 发布时 502/504 的探针与时序问题

**面试官：** 滚动升级偶发 502/504，你会怎么看？

**我：**

多半是**时序没对齐**：新 Pod **未预热**就被判 ready（缓存未建、JIT/连接未起），或旧 Pod **还在处理**时已被摘除/被 SIGKILL。

动作：

- 给 **startupProbe** 足够时间；
- **预热**：启动后先拉缓存/建连接再放行 readiness；
- **preStop + 合理的 terminationGrace** 保证排空；
- Ingress/网关配置**上游连接耗尽**与**超时一致性**；
- Deployment 设 `maxUnavailable=0, maxSurge=1`，关键服务配 **PDB**，杜绝容量“塌陷”。

场景 E - gRPC / 长连接的健康与排空

**面试官：** gRPC 如何做健康检查与排空？

**我：**

健康用 **gRPC health check**（`grpc.health.v1.Health/Check`），readiness 只要返回 SERVING；下线时先**停止新流量**，再对现有 HTTP/2 流开启 **drain**（发送 GOAWAY/限制新流），等待活跃 RPC 完成，宽限期内强制截止未完成的长尾。

### HPA 与自动扩缩容：CPU/内存 vs 自定义指标；`stabilizationWindow`、`scaleDownPolicy` 抖动治理；冷启动

- **按负载选指标**：CPU/内存适合计算型；I/O/外依赖型优先**队列深度/并发/QPS**等**自定义指标**。
- **容量估算**：`副本 ≈ (峰值 QPS / 单 Pod QPS) × 1.2~1.5`；把这个目标转成**平均每 Pod 的目标值**给 HPA。
- **快涨慢降**：v2 behavior：`scaleUp 0s 窗口 + (Pods/Percent)`；`scaleDown 稳定窗口 ≥300s`，每分钟最多 -20%。
- **冷启动治理**：`startupProbe` + **预热后才 readiness**；`minReplicas` ≥ 稳态需求；必要时**过量预置**少量空载副本。
- **联动策略**：重试有**预算 + 抖动**；必要时引入 **KEDA/Adapter** 用**队列/并发**扩容。
- **与集群层协同**：确保 **requests** 合理，配 **Cluster Autoscaler** 与 **PDB**，避免“扩容无位可排”。
- **限流优先**：扩容跟不上先**限流/降级**，再扩容，防止尾延迟雪崩。

> “我们把 HPA 做成**快涨慢降**，指标按**负载类型**选择（CPU vs 队列/并发），**预热后才接流量**，并用 **Cluster Autoscaler + PDB** 保证扩容能落地；扩不及时时先**限流**守住尾延迟。”

场景 A - 该选什么指标来扩缩容

**面试官：** 你用什么作为 HPA 指标？CPU 就够了吗？

**我：**

看负载类型。

- **CPU/内存**适合 **CPU 绑定**或**内存增长性**的服务；
- **自定义指标**适合业务负载，比如**每 Pod 并发数/QPS、队列长度、pending jobs**；
- I/O 或外部依赖主导的服务，用 CPU 扩容常常**动作滞后**或**放大抖动**。这类我更偏向**队列深度/吞吐**或**请求在途数**做目标。

场景 B - 如何设定目标值与估算容量

**面试官:** 有一个 API，每个 Pod 稳态能抗 `~200` QPS，峰值要 3k QPS，你怎么估？

**我：**

先给**容量心智算式**：`需要副本 ≈ 峰值 QPS / 单 Pod 可承载 QPS × 安全系数(1.2~1.5)`。

所以 `3000/200=15`，×1.3 安全系数 ≈ **20 副本**。HPA 的**自定义平均目标**就设为**每 Pod 200 QPS 左右**（或并发 200），`minReplicas` 至少**略大于稳态需求**，防止频繁冷启动。

场景 C - 控制抖动与“锯齿”

**面试官：** 怎么避免 HPA 来回抖？

**我：**

用 **v2 behavior** 做“**快涨慢降**”：

- **scaleUp**：窗口 0s，允许**每分钟 +100% 或 +10 个 Pod**，取较大者；
- **scaleDown**：**稳定窗口 300s**，每分钟最多 `-20%`；
- 另外配合 **readiness 预热**与**队列排空**，保证新 Pod 真能接流量，旧 Pod 退场不丢请求。

> 示例片段（可参考）：

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata: { name: api-hpa }
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: api
  minReplicas: 4
  maxReplicas: 50
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 60
  behavior:
    scaleUp:
      stabilizationWindowSeconds: 0
      selectPolicy: Max
      policies:
      - type: Percent
        value: 100
        periodSeconds: 60
      - type: Pods
        value: 10
        periodSeconds: 60
    scaleDown:
      stabilizationWindowSeconds: 300
      selectPolicy: Max
      policies:
      - type: Percent
        value: 20
        periodSeconds: 60
```

场景 D - 冷启动与“扩容不见效”

**面试官：** 扩容了，但延迟仍然高，是不是 HPA 没用？

**我：**

常见是**冷启动/连接预热**没做：新 Pod 标记 ready 太早，缓存/连接/JIT 还没好；或者节点资源不足，需要 **Cluster Autoscaler** 先拉新节点。

处理：

- **startupProbe** + **预热**后再通过 **readiness**；
- `minReplicas` 给到**稳态需要量**，避免每次从 1 拉满；
- 结合 **集群自动扩容**与 **PDB**，保证新副本有位可调度。

场景 E - 与重试/超时/队列的联动

**面试官：** 高峰下，扩容跟不上，重试会把问题放大吗？

**我：** 会。

对**幂等请求**保留**有限重试 + 抖动回退**，同时以**队列长度/在途数**作为**外部指标**驱动扩容（KEDA 或 Prometheus Adapter）。把**重试预算 ≤10%** 写进策略，避免“重试风暴”。

场景 F - HPA 与 VPA/requests 的关系

**面试官：** 用 HPA 的同时，requests/limits 和 VPA 怎么配？

**我：** **CPU HPA 依赖 requests 计算利用率**，所以必须合理设置 `requests`；**VPA 与 HPA 不要同时调同一资源**（常见是 VPA 只建议或只动内存，HPA 只看 CPU/自定义指标）。还要避免 **CPU limit 过低**导致节流，引起**假高利用率**误扩容。

### 配置与发布安全：ConfigMap/Secret 版本化与回滚；镜像不可变标签；RollingUpdate 参数；PDB 与 `drain`

- **配置即代码**：只走 Git；变更 = 新对象 + **`checksum/config` 注解触发滚动**；保留 `revisionHistoryLimit` 可回滚。
- **Secret 安全链**：etcd **KMS 加密** → **不落盘** → **最小权限** → **外部密管（External Secrets/ASM）** 同步。
- **镜像不可变**：禁用 `:latest`，用 **digest 或唯一 tag**；保证回滚是字节级一致。
- **Rolling 护栏**：`maxUnavailable=0`、`maxSurge=1`、**PDB**；与 startup/readiness、preStop 配套。
- **drain 规范**：评估 emptyDir；按 PDB 节奏驱逐；必要时先扩后 drain，观察排空指标。
- **ConfigMap/Secret `immutable: true`**：线上默认打开；更新走“新名 + 滚动”，热更仅在明确支持热加载的组件上用。
- **审计**：记录镜像 digest、配置版本、rollout 修订号到变更单，问题可追溯。

> “我们把配置当代码，用 `checksum/config` 驱动**可审计、可回滚**的发布；镜像以 **digest** 固定，Rolling 配 **0/1**（`maxUnavailable/Surge`）和 **PDB**，节点维护走 drain，Secret 交给云密管同步，整条链路既安全又可控。”

场景 A - 配置改了，怎么“像代码一样可回滚”

**面试官：** 线上改配置最怕不可追溯，你怎么管？

**我：**

规则是“**配置即代码**”：

1. ConfigMap/Secret **只经由 Git** 合并到主干；
2. **不直接热改**，而是**生成新对象 + 触发滚动**；
3. 在 Deployment 的 **podTemplate 加 `checksum/config` 注解**，值为配置内容 hash，变更即滚动；
4. 开启 `revisionHistoryLimit`，用 `kubectl rollout undo` 一键回滚到上个稳定版本。

场景 B - Secret 安全与来源

**面试官：** Secret 放 K8s 里就安全吗？

**我：** 只 base64 **不等于加密**。

要点：

- 集群层开 **etcd 加密（KMS 包封）**；
- 应用层**不落盘**（优先 env 或 tmpfs volume），**最小权限**读取；
- 建议用 **External Secrets/ASM** 同步（结合 IRSA），密钥托管在云密管里，K8s 只拿到用量副本；
- 对 Secret 也可用 `immutable: true`，误改直接创建新名。

场景 C - 镜像标签为何必须“不可变”

**面试官：** 我们一直用 `:latest`，有什么问题？

**我：**

回滚会“**名同实异**”。必须**不可变镜像**：

- 用 **digest**：`image: repo/app@sha256:...`；
- 或 CI 产出 **唯一 tag（含 commit SHA）** 并冻结。

回滚到版本 X 才是**字节级一致**，诊断与审计才可信。

场景 D - RollingUpdate 的护栏

**面试官：** 如何减少滚动升级的抖动与风险？

**我：**

- `strategy.rollingUpdate`: **`maxUnavailable=0`，`maxSurge=1`** 保容量；
- 关键服务配 **PDB**（如 `minAvailable: 80%`），防止维护/升级把副本一次性赶没；
- 结合 **startup/readiness + preStop**（上一步讲过）确保**预热后才接流量、下线先排空**；
- `revisionHistoryLimit` ≥ 3，出现异常 `kubectl rollout undo` 秒退。

场景 E - 节点维护与 drain 的正确姿势

**面试官：** 节点维护常见翻车点？

**我：**

忽视 **PDB** 与 **临时存储**：

- 维护前 `kubectl drain <node> --ignore-daemonsets --delete-emptydir-data`，但提前评估是否有**关键信息在 emptyDir**；
- 没有 PDB 的服务可能被一次性驱逐，**容量塌陷**；
- 观测 **排空时长/未完成请求数**，必要时临时**加副本**再 drain。

场景 F - ConfigMap/Secret 的“热更与不可变”

**面试官：** 要不要开 `immutable: true`？

**我：**

推荐**线上默认 immutable**，避免被临时改坏；要更新就**新建对象名**（或新 key），更新 `checksum/config` 触发滚动。

确需热更：挂 **volume** 模式（非 env），应用自己**监听文件变动**并热加载，同时仍需保留 Git 源与审计。

极简参考片段：

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: api-config-v3
immutable: true
data:
  APP_MODE: "prod"
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api
spec:
  revisionHistoryLimit: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 0
      maxSurge: 1
  template:
    metadata:
      annotations:
        checksum/config: "sha256:abcd1234..." # 来自 ConfigMap 内容的 hash
    spec:
      containers:
      - name: app
        image: repo/api@sha256:... # 不可变镜像
        envFrom:
        - configMapRef:
            name: api-config-v3
```

### 最小权限与身份（RBAC / OIDC / IRSA）：ServiceAccount 绑定最小权限；云资源精细授权；密钥不落盘

- **RBAC 粒度**：Role/RoleBinding 优先；仅在跨命名空间时用 ClusterRole/Binding。
- **默认拒绝**：不给默认 SA 权限；每个工作负载**专用 SA**。
- **IRSA 三件套**：OIDC Provider → 信任策略限定 `sub` → SA 注解 `role-arn`，全程**无静态密钥**。
- **策略三减**：主体到 SA、资源到 ARN 前缀、动作到必要动词；拆分只读/只写角色。
- **审计**：打开 CloudTrail；记录 **AWS 请求 ID + 角色名** 到应用日志，配合 traceId 串起链路。
- **验证**：上线前跑策略模拟；灰度期监控 `AccessDenied` 与异常比率。
- **轮换**：密钥走**云密管/External Secrets**，能不用静态密钥就不用。

> “我们把权限当成**爆炸半径控制器**：RBAC 到 SA，IRSA 把角色限定到 `sub`，策略再按资源前缀和动词最小化，既无静态密钥，又能在 CloudTrail 里把每一次访问和 trace 串成闭环。”

场景 A - 为什么“最小权限”与可观测同等重要

**面试官：** 你为什么总强调最小权限？

**我：**

它直接决定**爆炸半径**。权限越细，误配/入侵带来的影响越小；而且有了**可观测**（审计日志、traceId、请求 ID），我们能把“谁用什么权限做了什么”串起来，既能**快速止血**也能**可追溯**。

场景 B - RBAC 四件套与常见误用

**面试官：** 说说 RBAC 的基本对象与误用？

**我：**

**Role / ClusterRole / RoleBinding / ServiceAccount**。

常见误用：

- 直接把 **ClusterRole cluster-admin** 绑给默认 SA；
- 用 **ClusterRoleBinding** 给 **命名空间内** 的场景（过大）；
- 忘了**按资源+动词**最小化（如只给 `get,list,watch`），导致“读写通杀”。

场景 C - OIDC 与 IRSA 的工作流（EKS 示例）

**面试官：** IRSA 是怎么把 AWS 权限给到 Pod 的？

**我：**

三步：

1. 集群注册 **OIDC Provider**；
2. 建一个 **IAM Role**，信任策略允许 `sts:AssumeRoleWithWebIdentity`，并按 **ServiceAccount 的 sub** 限定；
3. 在 **ServiceAccount** 上加 **`eks.amazonaws.com/role-arn`** 注解即可。运行时 kubelet 注入 Web Identity Token，Pod 以此换取临时凭证，全程**无静态 AccessKey**。

场景 D - 如何“只给需要的那一点点”

**面试官：** 具体怎么做到“只给需要的”？

**我：** 三层缩小：

- **主体范围**：信任策略把 `sub` 精确到 `system:serviceaccount:<ns>:<sa>`；
- **资源范围**：策略里按**资源 ARN/前缀**限到最小（如 `s3://bucket/prefix/*`）；
- **动作范围**：只给必要动词（如 `s3:PutObject` 而非 `s3:*`）。

再配**只读/只写**两套角色，按工作负载绑定。

场景 E - 为什么不把密钥塞进 Pod

**面试官：** 直接把 AccessKey 当 Secret 注入不更简单吗？

**我：**

风险巨大：**泄漏面广**、**轮换困难**、**审计不准**。IRSA 的 **STS 临时凭证**会自动轮换，落库的是**最小授权的角色**，CloudTrail 里能看到**操作主体=role**，可追溯。

场景 F - 验证与审计

**面试官：** 上线前如何验证权限“刚刚好”？

**我：**

预先用 **IAM Policy Simulator** 或最小化策略生成工具；灰度期间把**拒绝事件**（`AccessDenied`）与 **CloudTrail** 接到告警；应用侧日志带上**调用目标与请求 ID**，遇到拒绝能一跳定位。

极简参考片段（EKS · IRSA）

**IAM 信任策略（摘）**

```json
{
  "Version": "2012-10-17",
  "Statement": [{
    "Effect": "Allow",
    "Principal": { "Federated": "arn:aws:iam::<ACCOUNT_ID>:oidc-provider/<OIDC_ISSUER>" },
    "Action": "sts:AssumeRoleWithWebIdentity",
    "Condition": {
      "StringEquals": {
        "<OIDC_ISSUER>:aud": "sts.amazonaws.com",
        "<OIDC_ISSUER>:sub": "system:serviceaccount:observability:adot-collector"
      }
    }
  }]
}
```

**K8s ServiceAccount（摘）**

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: adot-collector
  namespace: observability
  annotations:
    eks.amazonaws.com/role-arn: arn:aws:iam::<ACCOUNT_ID>:role/adot-collector
```

**最小化 S3 写入策略（示例）**

```json
{
  "Version": "2012-10-17",
  "Statement": [{
    "Effect": "Allow",
    "Action": ["s3:PutObject"],
    "Resource": ["arn:aws:s3:::my-bucket/observability/*"]
  }]
}
```

---

## Full-Stack

### React/TypeScript 基础最小面（函数组件/Hook、受控 vs 非受控、常用 TS 工具类型、错误边界）

- Hooks 遵循**顺序模型**：只在顶层、只在函数组件/自定义 Hook 调用。
- **State 触发渲染，Ref 不触发**；昂贵派生用 `useMemo`，函数引用稳定用 `useCallback`。
- `useEffect` 防三坑：**漏依赖 / 误用副作用 / 请求竞态**（用 AbortController 清理）。
- 表单：**小表单受控**，**大表单非受控 + 表单库 + schema 校验**；事件类型用 `React.ChangeEvent<>` 等。
- Props 建模：优先 `ComponentProps`、`Pick/Partial/Record`，**显式 children**。
- 错误边界：仍用**类组件** + Sentry 上报，链上 **release/environment/traceId**。
- 大列表：**稳定 key + 虚拟化 + React.memo**，必要时路由/组件级拆分与选择性水合。

> “我把 React 当**纯函数 + 状态槽位**来写：**State 负责可见变化、Ref 负责持久引用、Effect 只做副作用**；类型上用 `ComponentProps` 组合与显式 `children`，错误边界交给类组件并配合 Sentry 串起 trace。”

场景 A - Hooks 心智与规则

**面试官：** 用一句话讲讲 Hooks 的工作方式和两条硬规则？

**我：** Hooks 基于**渲染顺序**存取状态，每次渲染都会拿到独立的快照；两条硬规则是**只在最顶层调用**、**只在 React 函数组件或自定义 Hook 中调用**。违反顺序（如条件里调用）会把状态槽位对乱，导致“幽灵状态”。

场景 B - State vs Ref vs Memo

**面试官：** 什么时候用 `useState`，什么时候用 `useRef` 或 `useMemo`？

**我：** 需要**触发重新渲染**用 `useState`；只是**跨渲染持久存值**且**不触发渲染**用 `useRef`（比如保存上一次的值/计时器句柄）；**昂贵计算缓存**或**派生数据**用 `useMemo`，**函数稳定引用**用 `useCallback`，否则子组件容易反复重渲。

场景 C - `useEffect` 依赖与常见坑

**面试官：** `useEffect` 最容易踩的坑？

**我：** 三个：

1. **漏依赖**：依赖数组少了变量导致读到旧值；
2. **不必要的副作用**：可以放到渲染阶段的纯计算不该进 effect；
3. **竞态**：发请求没做取消/标记，晚返回覆盖早返回。实践里我会**把数据获取放到事件/路由层**，并在 effect 中清理 AbortController。

场景 D - 受控 vs 非受控表单

**面试官：** 复杂表单你怎么选？

**我：** 简单场景用**受控组件**（值走 state）；大表单或高频输入用**非受控 + 表单库**（如 `react-hook-form`）减少重渲，**校验放 schema**（Zod/Yup），提交时统一做**客户端 + 服务器**双校验。TS 上事件类型常用：`React.ChangeEvent<HTMLInputElement>`、`React.FormEvent<HTMLFormElement>`。

场景 E - Props 的类型建模

**面试官：** 复用已有组件的 Props，你怎么在 TS 里写？

**我：** 用工具类型组合，比如：

```ts
type ButtonProps = React.ComponentProps<'button'> & { loading?: boolean };
type CardTitleProps = Pick<CardProps, 'title' | 'subtitle'>;
type ApiResult<T> = { data: T; error?: string };
```

避免 `React.FC` 的隐式 `children` 争议，**显式声明 `children?: React.ReactNode`** 更清晰。默认值用函数参数默认值，不再用 `defaultProps`。

场景 F - 错误边界与异常收敛

**面试官：** 函数组件如何做错误边界？

**我：** 错误边界目前仍是**类组件**（`componentDidCatch`），我会写一个通用 `ErrorBoundary` 包裹路由级或关键区域；函数组件内部用 `try/catch` 只能抓**事件处理**，抓不到渲染期错误。上报走 **Sentry**，带上 **release/environment/traceId**，方便和后端串联。

场景 G - 列表渲染与性能

**面试官：** 大列表卡顿怎么治？

**我：** 三点：

1. **稳定的 key**（业务 id），避免索引当 key；
2. **虚拟列表**（如 react-window）减少真实节点数；
3. 把**纯展示子组件 `React.memo`**，并用 `useCallback/useMemo` 稳定 props 引用，配合选择性水合/分片渲染优化首屏。

### 路由与表单（React Router v6、嵌套路由/懒加载、表单校验与数据流）

- **布局壳 + `<Outlet/>`**：壳承载导航/鉴权/面包屑，子页只换内容。
- **路由层鉴权**：刷新/直链可拦截；角色/租户细分放在壳或守卫组件。
- **懒加载 + 骨架屏 + 预取**：体感更丝滑，避免白屏。
- **表单选型**：小表单受控；大表单**非受控 + 表单库 + schema 校验**；提交端再做一次服务器校验。
- **服务器状态交给 Query**：提交后 `invalidateQueries`；筛选/分页用 **search params**。
- **乐观更新 + 快照回滚**：不卡 UI，失败可还原；危险操作要二次确认。
- **类型与可及性**：字符串→数字/日期的安全转换；`aria-*` 与错误聚焦别缺。

> “我把**路由当布局与守卫层**、把**表单当数据契约**：壳管鉴权与骨架，页面只管内容；校验走 schema，服务器状态交给 Query，提交用**乐观更新 + 快照回滚**提升体验。”

场景 A - 路由最小心智

**面试官：** React Router v6 里，嵌套路由与布局你怎么解释？

**我：** 把“父路由”当**布局壳**，子路由在 `<Outlet/>` 里渲染。好处是：壳只渲一次，内部页面切换更快；权限/导航/面包屑都挂在壳上，避免重复实现。

场景 B - 受保护路由与重定向

**面试官：** 鉴权怎么做最稳妥？

**我：** 把**鉴权逻辑放在路由层**：若未登录就 `navigate('/login', { replace:true })`；若有角色/租户限制，**在布局壳判断**并给出 403 页。这样比页面里再判断更不易遗漏，刷新/直链都能拦住。

场景 C - 懒加载与骨架屏

**面试官：** 路由懒加载会带白屏吗？

**我：** 用 `lazy()` + `<Suspense fallback={<Skeleton/>}>`，**父壳不重渲**，只替换子路由；列表页配**骨架屏 + 首屏最小数据**，同时在 `onMouseEnter` 的导航上**预取下一页模块**，减少感知延迟。

场景 D - 表单：受控 vs 非受控

**面试官：** 大表单怎么选型？

**我：** 小表单**受控**最直观；大表单 **非受控 + 表单库（react-hook-form）** 更省渲染。校验放 **schema（Zod/Yup）**，表单库只负责收集与错误展示；**提交再做一次服务器校验**，口径一致。

场景 E - 数据流与缓存

**面试官：** 表单提交成功后，列表如何同步刷新？

**我：** 用**请求层缓存**（TanStack Query）管理服务器状态：提交成功后 `invalidateQueries(['items'])`。路由层用**搜索参数**表达筛选（`useSearchParams`），避免把服务器状态塞进全局 store。

场景 F - 乐观更新与回滚

**面试官：** 创建/删除要不卡 UI？

**我：** 做**乐观更新**：先改本地缓存显示成功，再发请求；失败时**回滚**到之前的快照，并弹出错误。对“不可逆/高风险”动作仍需**二次确认**。

场景 G - 常见踩坑

**面试官：** 表单里最常见的三个坑？

**我：**

1. **数字/日期类型**：HTML 输入拿到的是字符串，提交前要做类型安全转换；
2. **受控组件性能**：大表单每击键 re-render，需**节流/去抖**或改走非受控；
3. **可访问性**：`label htmlFor`、`aria-invalid`、错误聚焦到第一个有问题的控件。

迷你代码片段（TypeScript，精简可复用）

**受保护路由（布局壳守卫）**

```tsx
function ProtectedLayout() {
  const user = useUser(); // null 或 {role: 'admin'}
  const navigate = useNavigate();
  useEffect(() => { if (!user) navigate('/login', { replace: true }); }, [user]);
  if (!user) return null; // or spinner
  return <Outlet/>;
}
```

**表单（react-hook-form + Zod）**

```tsx
const schema = z.object({
  title: z.string().min(1),
  price: z.coerce.number().min(0) // 字符串安全转数字
});
type FormData = z.infer<typeof schema>;

const { register, handleSubmit, formState: { errors } } = useForm<FormData>({
  resolver: zodResolver(schema)
});
const onSubmit = (data: FormData) => mutate(data); // TanStack mutation

<form onSubmit={handleSubmit(onSubmit)}>
  <input {...register('title')} aria-invalid={!!errors.title}/>
  <input {...register('price')} />
  <button type="submit">Save</button>
</form>
```

**乐观更新 + 回滚（TanStack Query）**

```ts
const qc = useQueryClient();
const mutation = useMutation({
  mutationFn: createItem,
  onMutate: async (newItem) => {
    await qc.cancelQueries({ queryKey: ['items'] });
    const snapshot = qc.getQueryData<Item[]>(['items']);
    qc.setQueryData<Item[]>(['items'], (old = []) => [{ ...newItem, id: 'temp' }, ...old]);
    return { snapshot };
  },
  onError: (_err, _vars, ctx) => { qc.setQueryData(['items'], ctx?.snapshot); },
  onSettled: () => qc.invalidateQueries({ queryKey: ['items'] })
});
```

### SSR / CSR / 选择性水合（取舍与指标：TTFB/TTI/CLS；岛屿架构要点）

- **选择用指标说话**：内容/SEO 看 **TTFB/LCP** 选 SSR；交互复杂看 **TTI/INP** 选 岛屿/选择性水合。
- **Streaming SSR + Suspense**：先流壳与就绪块，慢数据用骨架后续补齐。
- **只水合“需要交互”的岛**：其余静态直出，非首屏组件用 **可见/空闲**时水合。
- **去重数据请求**：SSR 数据**注水到客户端缓存**，避免二次请求。
- **防水合不一致**：统一时区/随机数/格式；首屏避免仅客户端可见的副作用。
- **缓存分层**：公共页 Edge 缓存 + SWR；登录态/个性化**不共享缓存**。
- **安全搭配**：SSR/岛屿搭配 **CSP nonce/hash**，敏感变量不进前端包。

> “**首屏靠服务器，交互靠岛屿，JS 只为交互而来**；用 Streaming SSR 撑 TTFB/LCP、用选择性水合把 TTI 拉早，缓存与安全头在边缘分层协同。”

场景 A - 30 秒讲清三者取舍

**面试官：** 用半分钟讲讲 SSR、CSR、选择性水合，分别适合什么场景？

**我（你）：** SSR 把**首屏 HTML** 在服务器生成，**TTFB/SEO 友好**；CSR 由浏览器拉包后再渲染，**交互灵活、边缘更轻**；选择性水合把页面拆成“**岛屿**”，只有需要交互的组件才下载并水合，**首屏小、JS 少**。落地上：**内容/营销页→SSR**，**后台仪表盘→CSR**，**混合复杂页→SSR + 岛屿/选择性水合**。

场景 B - 指标导向的选择

**面试官：** 你如何用指标而不是偏好做选择？

**我：**

看三类指标：

- **首屏可见**：TTFB、LCP —— SSR/Streaming SSR 更占优；
- **可交互**：TTI、INP —— 岛屿/部分水合把 JS 最小化，TTI 更早；
- **稳定性**：CLS —— SSR 时确保骨架尺寸稳定；CSR/岛屿则靠占位与避免布局抖动。

场景 C - Streaming SSR 与 Suspense 边界

**面试官：** SSR 首屏快，但数据慢怎么办？

**我：** 用 **Streaming SSR + Suspense 边界**：先把**壳与已就绪分块**流给浏览器，慢数据在边界内**占位骨架**，就绪后再流增量 HTML；客户端仅对交互岛屿水合，避免整页阻塞。

场景 D - 选择性水合的策略

**面试官：** 哪些组件该“变成岛屿”？

**我：**

把**纯展示**留在服务器渲染，只有**需要交互/动画/可视化**的变成岛屿（水合）；优先策略：

1. **按路由拆**：详情页/列表页的交互区是岛；
2. **按可见性/时机**：非首屏组件用 `client:idle`/`client:visible`（Astro）或懒加载；
3. **按重要度**：表单、图表、编辑器先水合，其它延后。

场景 E - 常见故障与防线

**面试官：** SSR/水合最常见的坑？

**我：**

- **水合不一致**：服务端与客户端渲染结果不同，导致警告或脱水失败 —— 保证**同一数据与随机数/日期格式**一致，禁止**仅在客户端存在的副作用**影响首屏 HTML；
- **双重请求**：SSR 拉了数据，客户端又在 `useEffect` 重拉 —— 用**同构数据层/缓存注水**（将 SSR 数据注入窗口变量或 Query 初始缓存）消除重复；
- **大 JSON 注水**：把 100KB+ 的对象塞进首屏 HTML —— 改为**边请求边流**或**分块注水**；
- **安全**：SSR 中**绝不把敏感变量注入前端**，用 CSP/nonce 约束内联脚本。

场景 F - 边缘/缓存与渲染模式协同

**面试官：** CDN/边缘如何配合渲染模式？

**我：**

- **可缓存 SSR**：对**公共、变更不频繁**页面（文档、营销）做 **Edge Cache + 短期 TTL + Stale-While-Revalidate**；
- **个性化/登录态**：走 **CSR 或 SSR + 私有缓存**（Vary: Cookie/Authorization），避免把用户数据缓存成公共副本；
- **API**：与页面缓存分层（CDN → 边缘 KV/缓存 → 源站），减少首屏依赖链路。

迷你片段

**Astro 岛屿（选择性水合）**

```astro
---
// 纯展示部分直接 SSR
import Hero from '../components/Hero.astro'
import Chart from '../components/Chart.jsx' // 交互组件
---
<Hero />
<!-- 首屏不阻塞：可见时才水合 -->
<Chart client:visible />
<!-- 次要交互：空闲时水合 -->
<SomeWidget client:idle />
```

**SSR 数据注水到客户端（伪代码）**

```html
<!-- 服务端模板 -->
<script>
  window.__BOOTSTRAP__ = JSON.parse("...escaped-JSON...");
</script>
```

```ts
// 客户端数据层
const bootstrap = (window as any).__BOOTSTRAP__;
queryClient.setQueryData(['page', id], bootstrap.pageData);
```

### CSP / 缓存策略（CSP `nonce/hash`、Cache-Control/ETag、CDN/Edge/浏览器多级缓存）

- **CSP 最小模板**：`default-src 'self'`; `script-src 'self' 'nonce-<nonce>' 'strict-dynamic'`; `object-src 'none'`; `base-uri 'self'`; `frame-ancestors 'none'`; 按需补 `img-src`/`font-src`/`connect-src` 白名单。
- **先 Report-Only** 再收紧：用 `Content-Security-Policy-Report-Only` 收集违例。
- **指纹化文件名 + 超长缓存**：`app.[hash].js` 配 `max-age=31536000, immutable`。
- **HTML 用 SWR**：`s-maxage + stale-while-revalidate` 给 CDN；浏览器侧 `no-store` 或 `max-age=0`。
- **API 分公有/私有**：公有数据 `ETag/Last-Modified + s-maxage`；私有数据 `Vary: Authorization, Cookie` 并短 TTL/不缓存。
- **前端监控放行**：`connect-src` 白 Sentry/上报域；上报携带 `release/env/trace_id`。
- **不要把机密注入前端**：机密留在后端/边缘密管；前端只收短期令牌。

> “**快靠缓存，稳靠指纹，安全靠 CSP**：指纹化静态资源配一年 immutable，HTML 用 SWR 短缓存可回源，CSP 用 `nonce + strict-dynamic` 收紧脚本面，再把上报域放进 `connect-src`，既跑得快也守得住。”

场景 A - 30 秒讲清“安全头 + 缓存”的组合拳

**面试官：** 为啥前端要同时谈 CSP 和缓存？

**我：** 因为“**快**”和“**安全**”是一对门神：**缓存**让静态资源飞起来，**CSP** 兜住 XSS 与第三方脚本风险；两者要配合——比如**带哈希指纹的不可变资源**才能安全地设超长缓存；而页面 HTML 因为含动态内容和 nonce，要**短缓存/可回源**。

场景 B - CSP 最小可用策略

**面试官：** 你的 CSP 最小落地长啥样？

**我：** 以“**默认严、按需白**”为原则：`default-src 'self'`，脚本用 `'nonce-<nonce>' + strict-dynamic`，禁用 `object-src`，限制 `frame-ancestors`。图片/字体/CDN 按域名白。错误先用 `Content-Security-Policy-Report-Only` 观测，再转正。

场景 C - nonce vs hash

**面试官：** 用 nonce 还是 hash？

**我：** **SSR 页面**推荐 **nonce**（每次响应注入、和 CSP 头一致）；**稳定内联片段**可用 **hash**。若使用构建产物（React/Astro），我们尽量**不写内联脚本**，而是外链脚本 + nonce，仅留极小的启动脚本。

场景 D - 缓存分层与策略

**面试官：** 静态资源、HTML、API 各怎么配缓存？

**我：**

- **静态资源（带指纹）**：`Cache-Control: public, max-age=31536000, immutable`（CDN/浏览器都长缓存）。
- **HTML**：`s-maxage=60, stale-while-revalidate=30`（CDN 可短缓存并回源刷新），浏览器端 `max-age=0` 或 `no-store`。
- **API**：公开数据可 `s-maxage` + ETag；**用户态**加 `Vary: Authorization, Cookie`，多用短 TTL 或 `no-store`，避免缓存越权。

场景 E - ETag / SWR / 版本灰度

**面试官：** 如何在“新版本上线”时既快又稳？

**我：** **文件名指纹**保证老用户缓存不冲突；HTML 走 **SWR**（CDN 先回旧副本，再异步回源），回源命中 **ETag/Last-Modified** 省流量。灰度时以**路由级别**逐步上新模板，失败即刻回退，而静态资源保持不可变。

场景 F - 前端监控与 CSP 的联动

**面试官：** CSP 开了以后，Sentry 之类的上报受影响吗？

**我：** 需要在 `connect-src` 放行 Sentry 上报域名；Source Map 下载域也要白。推荐**前后端统一 TraceID**，前端错误上报携带 `release/environment/trace_id`，后端能一跳串联。

场景 G - 常见坑

**面试官：** 说三个你见过的坑？

**我：**

1. **把 nonce 只加在 `<script>` 标签**，却忘了 CSP 头里的 `script-src` 没带同一 nonce；
2. **静态资源没做指纹**却设了长缓存，导致热修无法生效；
3. **把敏感变量烘焙进 HTML/JS**，CSP 再严也拦不住泄漏，因此**机密只在服务端或边缘机密管控**，前端只接收**临时令牌**。

迷你配置片段（可直接参考改造）

**Nginx（静态资源与 HTML）**

```nginx
# 静态资源（带指纹）
location ~* \.(js|css|png|jpg|svg|woff2)$ {
  add_header Cache-Control "public, max-age=31536000, immutable";
  try_files $uri =404;
}

# HTML：CDN 前可短缓存，浏览器不缓存
location = /index.html {
  add_header Cache-Control "no-store";
  try_files $uri /index.html;
}
```

**CSP 头（后端或边缘注入，带 nonce）**

```http
Content-Security-Policy:
  default-src 'self';
  script-src 'self' 'nonce-{{nonce}}' 'strict-dynamic';
  style-src 'self' 'unsafe-inline';
  img-src 'self' data: blob:;
  font-src 'self';
  connect-src 'self' https://o123456.ingest.sentry.io https://api.example.com;
  frame-ancestors 'none';
  base-uri 'self';
  object-src 'none';
  report-to csp-endpoint; report-uri https://report.example.com/csp;
```

**服务器模板中注入 nonce（伪代码）**

```html
<!-- 服务端生成 nonce，并同值写入 CSP 头与标签 -->
<script nonce="{{nonce}}">
  window.__BOOT__ = {...}; // 仅小型启动脚本
</script>
<script src="/static/app.3a9f1c.js" nonce="{{nonce}}"></script>
```

**CloudFront / CDN（HTML SWR 示例）**

- Behavior for `/*.html`: `Cache-Control: s-maxage=60, stale-while-revalidate=30`
- Behavior for `/static/*`: `Cache-Control: public, max-age=31536000, immutable`
- Add `Accept-Encoding`, `Authorization`, `Cookie` to **Vary/Cache key** 视业务而定（私人页不要共享缓存）。

### Sentry 埋点与错误上报（前后端统一 TraceID、Source Map、错误分级与去噪）

- **三类信号**：错误 + 性能事务 + breadcrumbs；统一携带 `release / environment / traceId`。
- **自动链路**：BrowserTracing 为匹配域注入 `sentry-trace`/`baggage`，后端 SDK 接续；日志里加 **traceId 到 MDC**。
- **控噪三件套**：`sampleRate / tracesSampler` 分层采样；`ignoreErrors / denyUrls` 去噪；`fingerprint` 合并。
- **PII 保护**：`beforeSend/beforeBreadcrumb` 脱敏；只记录 **URL 模板 + 状态码/时长**，不带请求体。
- **Source Map & Release**：以 **commitSHA** 做 release；构建时上传 Source Map；环境分离告警。
- **分级告警**：新问题/回归/频率三触发；On-call 看 **Issues/Performance/Releases** 联动。
- **回放可选**：必要时开启 `Replays`（低采样），仅在问题定位阶段临时提升。

> “我们让 **release / environment / traceId** 成为三件套：前端用 BrowserTracing 自动透传到后端，日志写入 MDC；上报前统一脱敏，采样与去噪分层做，既能一跳复现，也不被噪音淹没。

场景 A - 我们到底想从前端采集哪些“信号”？

**面试官：** Sentry 在前端主要采集什么？

**我：** 三类：

1. **错误事件**（未捕获异常、Promise 拒绝、资源加载失败）；
2. **性能事务**（页面加载、路由切换、接口调用的 span，定位尾延迟）；
3. **用户线索**（breadcrumbs：点击/路由/控制台日志），用于复现路径。都带上 **release / environment / traceId**，方便和后端一跳串联。

场景 B - 如何把前后端 Trace 串起来？

**面试官：** Sentry 怎么实现“前端点到后端 span”？

**我：** 开启 **BrowserTracing** 后，前端对匹配域名自动注入 `sentry-trace` 与 `baggage` 头；后端 SDK 会据此继续当前 trace。若你用 OTel，也能与 `traceparent` 并存，通过**统一的请求 ID / traceId** 写入日志（MDC），做到 **Grafana → Trace → 日志** 全链打通。

场景 C - 如何控噪？（采样与忽略）

**面试官：** Sentry 事件很多会噪音，怎么控？

**我：** 分层：

- **错误采样**：`sampleRate` 控整体上报；对**高价值路由**在 `beforeSend` 提升（或在 `tracesSampler` 里提升事务采样）；
- **忽略名单**：`ignoreErrors`（如 *ResizeObserver loop limit*）、`denyUrls`（如浏览器插件脚本）；
- **归并 & 指纹**：对“同栈同路由”合并；设置 `fingerprint` 统一某类业务错误。

场景 D - PII 与脱敏

**面试官：** 隐私怎么保证？

**我：** 默认关闭 PII，上报前在 `beforeSend / beforeBreadcrumb` **删或掩码** email/手机号/Token；Network 面包屑只保留 **URL 模板**与**状态码/时长**，**不带请求体**。必要时收敛域名到白名单。

场景 E - Source Map 与 Release

**面试官：** 线上压缩代码如何还原栈？

**我：** 打包生成 **Source Map** 并随 **release 版本**上传到 Sentry（以 commit SHA 作为 release），然后在客户端 `Sentry.init` 中设置同名 `release`。上线后按 **environment**（prod/staging）拆分告警与面板。

场景 F - 告警与落地闭环

**面试官：** 告警怎么配？

**我：** **错误级别分级**（Fatal/High/Medium），按 **新问题/回归/频率** 触发；把告警推到 **On-call 通道**；看板用 **Issues + Performance + Releases** 三视图，值班按“**症状→定位→复现→修复→验证**”走闭环。

最小可用片段

**前端（React/TS）Sentry 初始化（含脱敏 + 性能 + 采样 + 追踪）**

```ts
import * as Sentry from '@sentry/react';
import { BrowserTracing } from '@sentry/browser';

Sentry.init({
  dsn: import.meta.env.VITE_SENTRY_DSN,
  release: import.meta.env.VITE_COMMIT_SHA,     // e.g. "web@a1b2c3d"
  environment: import.meta.env.MODE,            // "prod"/"staging"
  integrations: [
    new BrowserTracing({
      tracePropagationTargets: [/^https:\/\/api\.example\.com/, /^\//], // 只对这些域注入追踪头
      routingInstrumentation: Sentry.reactRouterV6Instrumentation(history),
    }),
  ],
  tracesSampleRate: 0.1,                        // 性能事务采样（基础）
  replaysSessionSampleRate: 0.0,                // 需要时再临时打开
  replaysOnErrorSampleRate: 0.1,
  beforeSend(event) {
    // 脱敏示例：掩码 email/phone
    const redact = (s?: string) => s?.replace(/[A-Z0-9._%+-]+@[A-Z0-9.-]+\.[A-Z]{2,}/ig, '[email]')
                                     .replace(/\b1[3-9]\d{9}\b/g, '[phone]');
    if (event.user?.email) event.user.email = '[email]';
    event.request && (event.request.url = event.request.url?.replace(/\?.*$/, '')); // 去掉 query
    if (event.message) event.message = redact(event.message);
    return event;
  },
  ignoreErrors: [/ResizeObserver loop limit exceeded/i],
  denyUrls: [/extensions\//i, /chrome-extension:\/\//i],
});
```

**后端（Spring Boot）串联 Trace → 日志（MDC）→ Sentry**

```java
// 依赖：io.sentry:sentry-spring-boot-starter
@Component
public class TraceMdcFilter implements Filter {
  @Override public void doFilter(ServletRequest req, ServletResponse res, FilterChain chain)
      throws IOException, ServletException {
    try {
      String traceId = io.sentry.Sentry.getSpan() != null
          ? io.sentry.Sentry.getSpan().getTraceId().toString()
          : java.util.UUID.randomUUID().toString();
      org.slf4j.MDC.put("trace_id", traceId);
      chain.doFilter(req, res);
    } finally {
      org.slf4j.MDC.clear();
    }
  }
}
// logback pattern: traceId=%X{trace_id}
```

**构建时上传 Source Map（示意）**

```bash
export SENTRY_AUTH_TOKEN=xxxx
export SENTRY_ORG=my-org
export SENTRY_PROJECT=web
export SENTRY_RELEASE="web@$(git rev-parse --short HEAD)"
npm run build
npx sentry-cli releases new "$SENTRY_RELEASE"
npx sentry-cli releases files "$SENTRY_RELEASE" upload-sourcemaps dist --url-prefix "~/static" --rewrite
npx sentry-cli releases finalize "$SENTRY_RELEASE"
```

### 环境变量管理（构建期 vs 运行期、公开变量白名单、CI/CD 注入、敏感信息不入包）

- **构建期 vs 运行期**：后端读运行期 env；前端 SSR/边缘**运行期注入**公开变量，避免重建。
- **白名单前缀**：`VITE_* / NEXT_PUBLIC_* / REACT_APP_* / PUBLIC_*`；**机密不打包**。
- **注水/端点**：`/env.json` 或 `window.__ENV__` 只含公开信息（域名、版本、上报 DSN 等）。
- **.env 分层**：`.env` → `.env.<env>` → `.env.local`（不入库）→ CI/CD 注入为最终来源。
- **密钥治理**：机密通过**密管→K8s Secret→运行期 env**，前端只拿临时令牌/公开变量。
- **缓存配合**：HTML 短缓存 + SWR；静态指纹资源长缓存。
- **启动校验**：用 schema 校验环境变量（缺失/格式错立刻 fail fast）。

> “**构建期决定可见，运行期决定行为**：机密只在服务器/边缘，以 env 注入；前端只读**公开前缀**，HTML 用 **SWR**，配置改了**无需重建**。”

场景 A - 构建期 vs 运行期

**面试官：** 前端/后端的环境变量，最大的分界线是什么？

**我：** **构建期决定“打进包里”的常量，运行期决定“服务的行为”**。后端天然读**运行期**环境（`process.env`/`System.getenv`）；纯前端 SPA 若把变量烘焙进 bundle，就**必须重建才能改**。所以 SSR/边缘网关推荐**运行期注入**（如 `/env.json` 或 HTML 注水），前端仅读取**公开白名单**。

场景 B - 前端“公开变量”的白名单机制

**面试官：** 怎么防止把机密打进前端包？

**我：**

用**白名单前缀**：

- **Vite**：`VITE_*`，通过 `import.meta.env.VITE_XXX` 访问；
- **Next.js**：`NEXT_PUBLIC_*`；**CRA**：`REACT_APP_*`；**Astro**：`PUBLIC_*`。

**不带前缀**的一律仅在构建/服务器可见，**机密绝不进入前端 bundle**。

场景 C - SSR/边缘的“运行期注入”

**面试官：** 既要不打包机密，又要前端拿到 API Base 等地址怎么办？

**我：** 让服务器在**运行期**提供一个只含**公开信息**的端点（例如 `/env.json`）或在 HTML 注水一个 `window.__ENV__`。这样**灰度/多环境切换**不用重建包，边缘/CDN 也能按路由或租户回不同配置。

场景 D - .env 分层与 CI/CD

**面试官：** `.env` 怎么管理不会乱？

**我：**

**12-Factor** 思路：

`.env`（默认）→ `.env.development` / `.env.production` → `.env.local`（**不入库**）→ **CI/CD 注入为最终来源**。机密走**密钥管控**（Vault/ASM），通过管道注入到 **K8s Secret**，后端再以环境变量读取；前端只拿公开变量。

场景 E - 常见坑与规避

**面试官：** 说三个你见过的坑？

**我：**

1. **把私钥/令牌打进前端包**（爬虫轻松拿走）→ **白名单前缀 + 审计构建产物**；
2. **静态 HTML 被 CDN 长缓存**，环境切了但页面里旧的注水还在 → HTML 走**短缓存/SWR**；
3. **变量未校验**导致线上才报错 → 启动时用 **schema 校验**（Zod/Yup）强约束。

场景 F - K8s/容器里的映射关系

**面试官：** 你如何把“配置即代码”落到容器？

**我：** **ConfigMap（非机密）/ Secret（机密） → env/envFrom/volume 挂载**。镜像保持**不可变**，环境差异通过 **Deployment 的 env** 注入；回滚只需 `rollout undo`，不用重打镜像。

迷你落地片段

**前端（Vite/TS）—— 公开变量的类型校验**

```ts
// env.ts
import { z } from 'zod';

const Env = z.object({
  VITE_API_BASE: z.string().url(),
  VITE_SENTRY_DSN: z.string().optional(),
  VITE_RELEASE: z.string().min(7)
});
export const env = Env.parse(import.meta.env);
```

**SSR 运行期注入（Node/Express 版 /env.json）**

```ts
// server.ts
import express from 'express';
const app = express();
app.get('/env.json', (_req, res) => {
  res.set('Cache-Control', 'no-store');
  res.json({
    API_BASE: process.env.API_BASE,      // 仅公开信息
    SENTRY_DSN: process.env.SENTRY_DSN_PUBLIC,
    RELEASE: process.env.RELEASE
  });
});
```

前端启动时拉 `/env.json`，再初始化请求层/监控，不需要重建包即可切换环境。

**K8s：ConfigMap/Secret 注入运行期 env**

```yaml
apiVersion: v1
kind: ConfigMap
metadata: { name: web-config }
data:
  API_BASE: "https://api.example.com"
  RELEASE: "web@a1b2c3d"

---
apiVersion: v1
kind: Secret
metadata: { name: web-secret }
type: Opaque
stringData:
  SENTRY_DSN_PUBLIC: "https://***" # 公开可用的上报 DSN（不含管理密钥）

---
apiVersion: apps/v1
kind: Deployment
metadata: { name: web }
spec:
  template:
    spec:
      containers:
      - name: web
        image: repo/web@sha256:....
        envFrom:
        - configMapRef: { name: web-config }
        - secretRef:    { name: web-secret }
```

**Spring Boot 读取 env（后端仅运行期）**

```java
@Value("${API_BASE}") private String apiBase;
@Value("${SENTRY_DSN_PUBLIC:}") private String sentryDsn;
```
