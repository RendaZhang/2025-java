<!-- START doctoc generated TOC please keep comment here to allow auto update -->
<!-- DON'T EDIT THIS SECTION, INSTEAD RE-RUN doctoc TO UPDATE -->
**Table of Contents**  *generated with [DocToc](https://github.com/thlorenz/doctoc)*

- [高频问题库](#%E9%AB%98%E9%A2%91%E9%97%AE%E9%A2%98%E5%BA%93)
  - [API Design & Reliability](#api-design--reliability)
    - [契约清晰：资源建模 & 语义化接口（Contract Clarity）](#%E5%A5%91%E7%BA%A6%E6%B8%85%E6%99%B0%E8%B5%84%E6%BA%90%E5%BB%BA%E6%A8%A1--%E8%AF%AD%E4%B9%89%E5%8C%96%E6%8E%A5%E5%8F%A3contract-clarity)
    - [版本化策略：URI vs Header；向后兼容与下线流程](#%E7%89%88%E6%9C%AC%E5%8C%96%E7%AD%96%E7%95%A5uri-vs-header%E5%90%91%E5%90%8E%E5%85%BC%E5%AE%B9%E4%B8%8E%E4%B8%8B%E7%BA%BF%E6%B5%81%E7%A8%8B)
    - [鉴权与授权：JWT/OIDC、最小权限、Token 续期与旋转](#%E9%89%B4%E6%9D%83%E4%B8%8E%E6%8E%88%E6%9D%83jwtoidc%E6%9C%80%E5%B0%8F%E6%9D%83%E9%99%90token-%E7%BB%AD%E6%9C%9F%E4%B8%8E%E6%97%8B%E8%BD%AC)
    - [幂等性：幂等键、PUT vs POST、重试安全](#%E5%B9%82%E7%AD%89%E6%80%A7%E5%B9%82%E7%AD%89%E9%94%AEput-vs-post%E9%87%8D%E8%AF%95%E5%AE%89%E5%85%A8)
    - [限流-重试-熔断：客户端与服务端协同；退避策略；降级与快速失败](#%E9%99%90%E6%B5%81-%E9%87%8D%E8%AF%95-%E7%86%94%E6%96%AD%E5%AE%A2%E6%88%B7%E7%AB%AF%E4%B8%8E%E6%9C%8D%E5%8A%A1%E7%AB%AF%E5%8D%8F%E5%90%8C%E9%80%80%E9%81%BF%E7%AD%96%E7%95%A5%E9%99%8D%E7%BA%A7%E4%B8%8E%E5%BF%AB%E9%80%9F%E5%A4%B1%E8%B4%A5)
    - [错误码与可观察性：统一错误模型 / Trace-ID / 指标-日志-链路关联](#%E9%94%99%E8%AF%AF%E7%A0%81%E4%B8%8E%E5%8F%AF%E8%A7%82%E5%AF%9F%E6%80%A7%E7%BB%9F%E4%B8%80%E9%94%99%E8%AF%AF%E6%A8%A1%E5%9E%8B--trace-id--%E6%8C%87%E6%A0%87-%E6%97%A5%E5%BF%97-%E9%93%BE%E8%B7%AF%E5%85%B3%E8%81%94)
    - [灰度发布与回滚：逐步放量 / 健康检查 / 一键回滚](#%E7%81%B0%E5%BA%A6%E5%8F%91%E5%B8%83%E4%B8%8E%E5%9B%9E%E6%BB%9A%E9%80%90%E6%AD%A5%E6%94%BE%E9%87%8F--%E5%81%A5%E5%BA%B7%E6%A3%80%E6%9F%A5--%E4%B8%80%E9%94%AE%E5%9B%9E%E6%BB%9A)
  - [DB & Cache](#db--cache)
    - [索引选型与失效（B+Tree、组合索引、覆盖索引、左前缀）](#%E7%B4%A2%E5%BC%95%E9%80%89%E5%9E%8B%E4%B8%8E%E5%A4%B1%E6%95%88btree%E7%BB%84%E5%90%88%E7%B4%A2%E5%BC%95%E8%A6%86%E7%9B%96%E7%B4%A2%E5%BC%95%E5%B7%A6%E5%89%8D%E7%BC%80)
    - [事务与隔离级别：RC / RR；MVCC 的一致性读 vs 当前读；如何避免幻读与超卖](#%E4%BA%8B%E5%8A%A1%E4%B8%8E%E9%9A%94%E7%A6%BB%E7%BA%A7%E5%88%ABrc--rrmvcc-%E7%9A%84%E4%B8%80%E8%87%B4%E6%80%A7%E8%AF%BB-vs-%E5%BD%93%E5%89%8D%E8%AF%BB%E5%A6%82%E4%BD%95%E9%81%BF%E5%85%8D%E5%B9%BB%E8%AF%BB%E4%B8%8E%E8%B6%85%E5%8D%96)
    - [慢查询定位：执行计划、扫描行数、回表/下推、坏味道清单](#%E6%85%A2%E6%9F%A5%E8%AF%A2%E5%AE%9A%E4%BD%8D%E6%89%A7%E8%A1%8C%E8%AE%A1%E5%88%92%E6%89%AB%E6%8F%8F%E8%A1%8C%E6%95%B0%E5%9B%9E%E8%A1%A8%E4%B8%8B%E6%8E%A8%E5%9D%8F%E5%91%B3%E9%81%93%E6%B8%85%E5%8D%95)
    - [读写分离的坑：主从延迟、读旧值、强一致读 / 亲和策略](#%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E7%9A%84%E5%9D%91%E4%B8%BB%E4%BB%8E%E5%BB%B6%E8%BF%9F%E8%AF%BB%E6%97%A7%E5%80%BC%E5%BC%BA%E4%B8%80%E8%87%B4%E8%AF%BB--%E4%BA%B2%E5%92%8C%E7%AD%96%E7%95%A5)
    - [缓存一致性：Cache-Aside 双删顺序、消息通知/回源、热键与热点保护](#%E7%BC%93%E5%AD%98%E4%B8%80%E8%87%B4%E6%80%A7cache-aside-%E5%8F%8C%E5%88%A0%E9%A1%BA%E5%BA%8F%E6%B6%88%E6%81%AF%E9%80%9A%E7%9F%A5%E5%9B%9E%E6%BA%90%E7%83%AD%E9%94%AE%E4%B8%8E%E7%83%AD%E7%82%B9%E4%BF%9D%E6%8A%A4)
    - [三座大山：穿透 / 击穿 / 雪崩（识别与治理清单）](#%E4%B8%89%E5%BA%A7%E5%A4%A7%E5%B1%B1%E7%A9%BF%E9%80%8F--%E5%87%BB%E7%A9%BF--%E9%9B%AA%E5%B4%A9%E8%AF%86%E5%88%AB%E4%B8%8E%E6%B2%BB%E7%90%86%E6%B8%85%E5%8D%95)
  - [Message and Consistency](#message-and-consistency)
    - [Outbox（事务外箱）& 本地事务边界](#outbox%E4%BA%8B%E5%8A%A1%E5%A4%96%E7%AE%B1-%E6%9C%AC%E5%9C%B0%E4%BA%8B%E5%8A%A1%E8%BE%B9%E7%95%8C)
    - [幂等消费与去重键：表/Redis 实战与失败补偿](#%E5%B9%82%E7%AD%89%E6%B6%88%E8%B4%B9%E4%B8%8E%E5%8E%BB%E9%87%8D%E9%94%AE%E8%A1%A8redis-%E5%AE%9E%E6%88%98%E4%B8%8E%E5%A4%B1%E8%B4%A5%E8%A1%A5%E5%81%BF)
    - [重试策略与“重试预算”：退避 + 抖动 + 限额；与幂等/熔断的协同](#%E9%87%8D%E8%AF%95%E7%AD%96%E7%95%A5%E4%B8%8E%E9%87%8D%E8%AF%95%E9%A2%84%E7%AE%97%E9%80%80%E9%81%BF--%E6%8A%96%E5%8A%A8--%E9%99%90%E9%A2%9D%E4%B8%8E%E5%B9%82%E7%AD%89%E7%86%94%E6%96%AD%E7%9A%84%E5%8D%8F%E5%90%8C)
    - [DLQ / 停车场与人工处置：可观察、可回放、可审计](#dlq--%E5%81%9C%E8%BD%A6%E5%9C%BA%E4%B8%8E%E4%BA%BA%E5%B7%A5%E5%A4%84%E7%BD%AE%E5%8F%AF%E8%A7%82%E5%AF%9F%E5%8F%AF%E5%9B%9E%E6%94%BE%E5%8F%AF%E5%AE%A1%E8%AE%A1)
    - [顺序性与分区键：按“聚合维度”保序，吞吐与热点的权衡](#%E9%A1%BA%E5%BA%8F%E6%80%A7%E4%B8%8E%E5%88%86%E5%8C%BA%E9%94%AE%E6%8C%89%E8%81%9A%E5%90%88%E7%BB%B4%E5%BA%A6%E4%BF%9D%E5%BA%8F%E5%90%9E%E5%90%90%E4%B8%8E%E7%83%AD%E7%82%B9%E7%9A%84%E6%9D%83%E8%A1%A1)
    - [Exactly-once 的工程化取舍：追求 “effectively-once” 而非执念 EOS](#exactly-once-%E7%9A%84%E5%B7%A5%E7%A8%8B%E5%8C%96%E5%8F%96%E8%88%8D%E8%BF%BD%E6%B1%82-effectively-once-%E8%80%8C%E9%9D%9E%E6%89%A7%E5%BF%B5-eos)
  - [Java Concurrency](#java-concurrency)
    - [内存模型 & 可见性：happens-before / `volatile` 的边界](#%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B--%E5%8F%AF%E8%A7%81%E6%80%A7happens-before--volatile-%E7%9A%84%E8%BE%B9%E7%95%8C)
    - [`synchronized` vs `ReentrantLock`：可中断 / 定时 / 公平 / 条件队列](#synchronized-vs-reentrantlock%E5%8F%AF%E4%B8%AD%E6%96%AD--%E5%AE%9A%E6%97%B6--%E5%85%AC%E5%B9%B3--%E6%9D%A1%E4%BB%B6%E9%98%9F%E5%88%97)
    - [`ThreadPoolExecutor` 七参数、队列取舍与拒绝策略](#threadpoolexecutor-%E4%B8%83%E5%8F%82%E6%95%B0%E9%98%9F%E5%88%97%E5%8F%96%E8%88%8D%E4%B8%8E%E6%8B%92%E7%BB%9D%E7%AD%96%E7%95%A5)
    - [`CompletableFuture` 任务编排：并行、超时、取消与自定义 Executor](#completablefuture-%E4%BB%BB%E5%8A%A1%E7%BC%96%E6%8E%92%E5%B9%B6%E8%A1%8C%E8%B6%85%E6%97%B6%E5%8F%96%E6%B6%88%E4%B8%8E%E8%87%AA%E5%AE%9A%E4%B9%89-executor)
    - [并发诊断与排障：死锁、线程池饱和、阻塞点定位，5 分钟 SOP](#%E5%B9%B6%E5%8F%91%E8%AF%8A%E6%96%AD%E4%B8%8E%E6%8E%92%E9%9A%9C%E6%AD%BB%E9%94%81%E7%BA%BF%E7%A8%8B%E6%B1%A0%E9%A5%B1%E5%92%8C%E9%98%BB%E5%A1%9E%E7%82%B9%E5%AE%9A%E4%BD%8D5-%E5%88%86%E9%92%9F-sop)
    - [突发流量 + 下游限速，线程池怎么“吸收不作死”？](#%E7%AA%81%E5%8F%91%E6%B5%81%E9%87%8F--%E4%B8%8B%E6%B8%B8%E9%99%90%E9%80%9F%E7%BA%BF%E7%A8%8B%E6%B1%A0%E6%80%8E%E4%B9%88%E5%90%B8%E6%94%B6%E4%B8%8D%E4%BD%9C%E6%AD%BB)
    - [`CompletableFuture` 并行编排要做到：fail-fast + 可取消 + 明确降级](#completablefuture-%E5%B9%B6%E8%A1%8C%E7%BC%96%E6%8E%92%E8%A6%81%E5%81%9A%E5%88%B0fail-fast--%E5%8F%AF%E5%8F%96%E6%B6%88--%E6%98%8E%E7%A1%AE%E9%99%8D%E7%BA%A7)
    - [锁竞争 / 死锁如何 5 分钟内定位并修复？](#%E9%94%81%E7%AB%9E%E4%BA%89--%E6%AD%BB%E9%94%81%E5%A6%82%E4%BD%95-5-%E5%88%86%E9%92%9F%E5%86%85%E5%AE%9A%E4%BD%8D%E5%B9%B6%E4%BF%AE%E5%A4%8D)
    - [线程池饱和 + 重试风暴，如何协同治理？](#%E7%BA%BF%E7%A8%8B%E6%B1%A0%E9%A5%B1%E5%92%8C--%E9%87%8D%E8%AF%95%E9%A3%8E%E6%9A%B4%E5%A6%82%E4%BD%95%E5%8D%8F%E5%90%8C%E6%B2%BB%E7%90%86)

<!-- END doctoc generated TOC please keep comment here to allow auto update -->

# 高频问题库

- **Java**：集合/并发（synchronized、Lock、CAS、线程池）、JVM（内存结构、GC）、异常与最佳实践
- **Spring**：IOC/AOP、RestController、Actuator、配置管理、事务/连接池
- **微服务 & K8s**：Deployment/Service/Ingress/HPA/探针、无状态、滚动发布与回滚、ConfigMap/Secret
- **AWS & 云原生**：EKS NodeGroup vs Fargate、ALB、IRSA/OIDC、ECR、S3、CloudWatch、AMP、Grafana
- **DevOps**：CI/CD（GitHub Actions OIDC）、Trivy、回滚策略、IaC（Terraform 后端与锁）、最小权限
- **SRE**：SLI/SLO/错误预算、MTTR、Chaos、容量与成本权衡
- **行为/英文**：冲突处理、推动落地、失败复盘、跨团队协作、影响力（每题准备一条 STAR）

---

## API Design & Reliability

### 契约清晰：资源建模 & 语义化接口（Contract Clarity）

> **契约清晰（资源建模/语义化 URL/统一字段语义/错误模型/可观测 Trace-ID）**：对外坚持 Canonical Model + OpenAPI/JSON Schema 校验，分页/排序/时间/金额/ID 统一约定，返回体可观测可排障；渠道差异放在内部映射层，外部只做向后兼容的增量字段。

**面试官：**
“你在跨境电商/库存中台里，如何把**商品/库存**这类核心对象建模，并通过**清晰稳定的 API 契约**让前端、第三方渠道（Shopify/WooCommerce 等）都能稳用？如果上游平台字段各不相同、且业务量在活动期飙升，你会怎么设计接口与返回结构？”（你可以结合你在深圳市凡新科技 & Michaels 的经历来回答）

**你：**

“我会先做一个**Canonical Model（规范化域模型）**，然后把各平台的字段映射进来，API 对外只暴露**我们的一致语义**。例如把 `Product`、`Variant`、`StockItem` 拆清，`/products/{id}`、`/variants/{id}`、`/stocks?variantId=...&channel=...` 用**语义化 URL**和**查询参数**表达资源与过滤。之所以坚持对外契约稳定，是因为我们的服务在活动期会到 **80k–150k req/day，峰值 \~1.2k QPS**，而且要保持 P95 < 140ms，所以**任何破坏性变更都会造成放大效应**。这在我现在的工作环境里是常态（AWS 上 6 个 Spring Boot 微服务 + 自动扩容），因此我会把契约做成**可文档化、可校验**的，比如 OpenAPI + JSON Schema，前后端都能对齐检查。”

“以**库存**为例，我会规定：

- **ID 与类型**：所有 ID 统一用字符串（避免某些平台 `variant_id` 的长整型在 JS 客户端精度丢失）；金额统一**分为最小货币单位**（如分）、**货币代码**单独字段；时间全用 **RFC3339 UTC**。
- **分页与排序**：`page/size/sort` 统一格式；对大列表返回 `nextCursor` 以便前端/任务稳定翻页。
- **并发读写**：读 API 返回 `ETag/Last-Modified`，写 API 支持 `If-Match` 做并发控制；结合缓存（我们线上用 **Redis Cluster + Aurora 只读副本** 做读写分离），读路径可控、延迟稳定。”

“在 **Michaels** 做电商与 MakerPlace 的 API 时，我们也坚持把**登录与用户域**契约化，比如 **JWT 轮换 + OAuth2** 统一安全语义，接口文档清晰，移动端/前端对接成本低；同时在性能上通过**索引与响应结构优化**把接口延迟打下来，证明**契约清晰**有助于定位与优化。 ”

“错误返回我会统一一个**错误模型**：

```json
{ "code": "STOCK_NOT_FOUND", "message": "Stock item not found", "requestId": "trace-id", "details": { "variantId": "v123", "channel": "shopify" } }
```

配合**Trace-ID** 贯通到 CloudWatch / X-Ray / Grafana，这对我们线上**快速定位**很关键（我们有完善的可观测和零停机发布流程）。”

追问 1（场景深挖）

**面试官：**“上游新增了一个渠道特有字段，比如 `shopify_location_id`，但你不想污染对外契约，怎么处理？”

**你：**

“我不会把渠道细节渗透进公开模型，而是：

1. **内部映射层**吸收它（Connector DTO）；
2. 对外契约只在**业务确实需要**且跨渠道有共同语义时才**增量添加**字段（只做向后兼容的**可选字段**）；
3. 对必须透传的极少数字段，用 `extensions.*` 命名空间承载，并在 OpenAPI 标注**非核心**。这样不破坏现有调用方，也避免**破坏性变更**在高峰期放大。”（与我们在活动期高 QPS 的稳定性目标一致。）

追问 2（项目落地）

**面试官：**“你在凡新或 Michaels 有没有因为契约不清导致事故？后来怎么改的？”

**你：**

“有一次库存批量同步的响应里，**金额字段单位**没写清，导致一个下游任务把分当元，差点误触发大额补货。后来我们把**金额强制最小单位 + 货币代码**写进 Schema，并在 CI 里做**契约校验**与**示例响应校验**；同时在库存批同步流程里也加了**幂等键与步骤化编排**（我们用 **Lambda + SQS + Step Functions** 重构这条链路，整体耗时也从 ~25min 降到 ~7min）。”

### 版本化策略：URI vs Header；向后兼容与下线流程

> **版本化（URI 大版本 + Header 可选；向后兼容优先）**：非破坏性演进留在同大版本，破坏性才切 v2；提供兼容层与双写验证；Deprecation/Sunset 通知 + 分版本监控 + 强制下线日程，做到“可见、可控、可回滚”。

**面试官：**
“你在（深圳市凡新科技/麦克尔斯深圳）做商品与库存接口时，有一次业务要从‘单仓数量’升级到‘多仓分布库存’。这对返回结构是**破坏性变更**：原来 `stockQuantity` 是一个整数，现在要返回 `warehouses[]` 明细。你会怎么做**版本化**？是放在 URL 里 `/v2/stocks`，还是用 `Accept: application/vnd.xxx+json;v=2` 的 Header？旧客户端还在跑，你怎么做到**平滑迁移**和**按期下线**？”

**你：**

“对**外部/多团队依赖**的 API，我优先选 **URI 大版本**（`/api/v1/...` → `/api/v2/...`），因为它**可见性强**、文档和路由隔离清晰，前端/第三方也最好理解。对于**内部 BFF 或同域微服务**之间，我会保留 **Header 版本**（`Accept: ...;v=2`），用 Spring 的内容协商把同一条路径映射到不同的 `produces`。
落地上我会遵循这几条：

1. **非破坏性演进**（新增可选字段、增加新接口）只在**同大版本**里做，比如在 v1 返回里增加 `warehouses`（可选），同时保留 `stockQuantity`；
2. **破坏性变更**（字段语义变化、枚举收缩等）才启 **v2** 路径；
3. **双写/影子读** 验证：服务内部先把多仓逻辑双写到新表/新索引，线上对 v2 做**小流量灰度**，对比指标与告警；
4. **治理与下线**：对 v1 返回 **Deprecation/Sunset** 头（例如 `Sunset: <日期>`），在 API 网关或 Ingress/Grafana 里**按版本打点**，当 v1 调用量 < X% 持续 Y 天，就发最后通知并**切 410**；
5. 期间提供一个**兼容层**：v2 服务对老客户端仍可回填 `stockQuantity = sum(warehouses[].qty)`，让迁移有缓冲期。
   在凡新那边做促销高峰时，这种‘大版本在 URL，小迭代在 Schema’的策略更稳；在麦克尔斯那边，移动端同学更喜欢**明确的路径版本**，他们升级 App 时能直观看到 v2。整体目标是让‘**破坏性只发生在大版本切换**’，其它都是**增量可兼容**。”

追问 1（深挖迁移计划）

**面试官：**“如果大量老客户端一时半会升级不了，导致你迟迟不能下线 v1 怎么办？”

**你：**

“我们会把**兼容层**做成**可配置的**：

- 先在 v2 内部保留一层**适配器**把 `warehouses` 聚合成 `stockQuantity` 返回给 v1 客户；
- 在 API 网关对 v1/v2 的**QPS、错误率、延迟**做**分版本监控**，并在每次版本公告后给出**采纳率**；
- 设一个明确的**日程线**：例如 90 天后进入‘降级窗口’，老版本只做**安全修复**不加新特性；180 天后**强制下线**（返回 410 + 链接到迁移文档）。
  这样我们既不拖累新版本的演进，也给合作方足够时间。”

追问 2（工程实现）

**面试官：**“Spring Boot 里你怎么同时支持 URI 版本和 Header 版本？”

**你：**

“实际做法是**对外统一用 URI 大版本**，对内需要时再开 Header 协商：

- 控制器层：`/api/v1/...` 与 `/api/v2/...` 各有路由；
- 若同一路径用 Header：在 `@RequestMapping` 的 `produces` 里声明 `application/vnd.renda.stock+json;v=1/2`，并配置 `ContentNegotiationStrategy`；
- OpenAPI 文档分**两个 group**（v1/v2）生成 swagger，CI 里对两套 **JSON Schema** 做**契约校验**与**向后兼容检查**（新增字段只能是可选、禁止删除/改义）。
  配合灰度和回滚开关，风险可控。”

### 鉴权与授权：JWT/OIDC、最小权限、Token 续期与旋转

> **鉴权与授权（JWT/OIDC，最小权限，续期与旋转）**：统一 OIDC，短寿命 Access + 旋转 Refresh + 撤销表；按 `scope/aud` 做最小权限；服务间用客户端凭证/临时凭证；区分 401/403 并可观测（traceId/指标/告警）。

**面试官：**
“你在（深圳市凡新科技 / 麦克尔斯深圳）做订单与库存 API 时，前端（Web/小程序/APP）和三方渠道都要访问。你怎么做**统一登录与鉴权**？具体到 **JWT/OIDC** 的落地细节、**最小权限**的授权设计、以及**Access/Refresh** 的**续期与旋转**，你怎么权衡安全与可用性？”

**你：**

“我们把**身份认证**统一到 OIDC（例如 IDP：Cognito/Keycloak/公司自建），客户端用 **Auth Code + PKCE** 获取 **短时 Access Token（JWT）** 和 **较长 Refresh Token**。服务端（Spring Boot）作为 **OAuth2 Resource Server** 校验 JWT 的签名与过期，走 **JWKs** 自动拉取公钥并做**缓存**。
**授权**层面，我坚持**最小权限**：

- 面向外部调用，用 **scope** 粒度（`product:read`、`stock:write`），避免一刀切的 `admin`；
- 面向内部微服务，采用 **audience（aud）** 与 **资源级/操作级**组合（比如只能改“库存”但不能改“价格”），把权限做成**可配置策略**（如基于角色/属性的 ABAC）。
  **续期**我用‘短 Access + 可旋转 Refresh’：Access 约 5–15 分钟，Refresh 7–30 天，**刷新时旋转**（旧 Refresh 立即失效），并把 **jti（令牌唯一 ID）** 写进**黑名单/撤销表**（Redis/DB），防止被盗用。
  我们线上有**并发与多设备**，所以刷新接口设计成**幂等**，只承认‘最新签发’的 Refresh；如果同一 Refresh 被重复使用，我会触发**全账户 Refresh 封禁**并发告警。
  对**服务间调用**，我们禁用‘人为生成的长寿命 Token’，而走**客户端凭证流**或云原生临时凭证（比如 IRSA 访问云资源），降低泄漏风险。
  最后，把**401/403** 语义分清（未认证 vs 已认证但无权限），错误体里带 **traceId**，利于排障。”

追问 1（深挖安全细节）

**面试官：**“如果 JWT 泄露，或者我们要**强制登出**某个用户，怎么让**本来‘自包含’不可撤销**的 JWT 立即失效？”

**你：**

“我们有两层方案：

1. **短 TTL 的 Access** + **Gateway 层的撤销列表**：把需要立刻失效的 `jti` 放到 Redis，API Gateway 或全局过滤器先查撤销表，命中就拒绝；
2. **旋转 Refresh** + **一次性使用**：刷新时颁发新的 Refresh，并把旧的标记为‘已消费’，如果旧的再次出现就判定可疑并封禁。
   这两层能把‘自包含 Token 不可撤销’的问题控制在可接受窗口内（几分钟级）。此外我们开启 **kid（key id）轮换**，密钥换代时能平滑过渡。”

追问 2（可观测与故障演练）

**面试官：**“如何观察和演练这个体系？”

**你：**

“指标我们会分三类：

- **认证失败率**（签名/过期/撤销命中）、**刷新成功率**、**刷新重放**告警；
- **授权拒绝率**（403）按 `scope`/`aud` 分维度；
- **JWKs 拉取与缓存命中率**、**IDP 延迟**。
  我们有**失效演练**（把某用户/某应用加入撤销表；把某把密钥下线），确认 401 生效、刷新被拒并且告警到位。”

追问 3（项目落地）

**面试官：**“能结合你在凡新/麦克尔斯的经验说个具体例子吗？”

**你：**

“促销高峰时，**库存写入**要打得很紧，我们把 `stock:write` 单独成 scope，并给三方渠道一个**只读**的 `stock:read`。有次某脚本用错了客户端凭据，触发了**403**，我们通过错误体里的 `traceId` 很快定位到是**权限维度**不对，不是程序 Bug。
另外一次移动端升级后，出现**Refresh 重放**，我们通过**旋转 + jti 撤销**挡住了重放，并把这一模式加入**风控告警**。这些属于真实环境里‘安全与可用’的平衡：尽量短的 Access + 自动刷新，搭配清晰的权限边界。”

### 幂等性：幂等键、PUT vs POST、重试安全

> **幂等性（POST 幂等键、PUT/DELETE 自幂等、回调按事件 ID 去重）**：服务端原子占位（Redis SETNX 或 DB 唯一键）+ 响应快照复用；设置幂等窗口 TTL；与**指数退避**配合避免重试风暴；异步链路用 **Outbox + 消费端幂等** 保证最终一致。

**面试官：**
“促销高峰里，用户可能**连点两次下单**，第三方支付/库存回调也可能**重复推送**。你在（深圳市凡新科技 / 麦克尔斯深圳）如何保证**不会重复创建**订单/扣减库存？你具体怎么设计**幂等键**、**返回语义**，以及**与重试策略的配合**？”

**你：**

“我把问题分两层：**写接口幂等** + **事件/回调幂等**。

- **写接口（客户端→我们）**：POST 创建类接口要求客户端带 `Idempotency-Key`（或我们在 BFF 生成），幂等键 = `method + path + canonical(body) + user/tenant` 的哈希。服务端先做 **原子占位**（Redis `SETNX`/DB 唯一键），抢到占位才执行业务；执行完把**响应快照**缓存起来（含状态码、关键字段）。后续同键请求直接返回**同一份响应**（201 或 200），而不是再执行业务逻辑。
- **事件/回调（他们→我们 / 我们→下游）**：以**事件 ID**当幂等键，消费者端先查 `processed_events`（DB/Redis）是否见过，没见过才处理并**原子写入**‘已处理’标记；见过就直接 ACK。

  语义上我会遵守：

1. **PUT/DELETE** 本身具幂等；
2. **POST** 通过 `Idempotency-Key` 做到‘**功能幂等** + **响应幂等**’；
3. 返回如果命中幂等缓存，带一个 `Idempotent-Replay: true` 的响应头，方便排障。
   这套在凡新那边的订单与库存写路径都落了地；在麦克尔斯那边，支付回调我们就是用**回调事件 ID**做幂等键的。”

追问 1（工程细节）

**面试官：**“你怎么避免并发条件下的**双写**？Redis 会不会不可靠？”

**你：**

“占位一定要**原子**且**可恢复**：

- Redis 用 `SET key value NX EX=ttl`，抢到才继续；执行完成把**响应摘要**写回同 key，值里存状态与必要字段。
- 如果担心 Redis 丢数据或需要强一致，我会在 DB 里建一张 `idempotency` 表（`idempotency_key` 唯一索引），业务在**同一事务里**插入占位记录并处理。并发下只有一个事务能成功，其它事务收到**唯一键冲突**后转为读已存在的响应摘要。
- **TTL（幂等窗口）** 按业务风险定，比如创建订单 24h，库存写入 1–3h。窗口内重复请求都命中缓存；窗口外按新请求处理。”

**（伪代码，面试口述用）**

```java
// before controller
String key = hash(method, path, canonical(body), userId);
if (redis.setNx(key, "PROCESSING", ttl)) {
    Result r = handleBusiness();  // do create order / stock deduct in tx
    redis.set(key, serialize(r), ttl);   // cache response snapshot
    return r;                            // 201 Created
} else {
    return deserialize(redis.get(key));  // replay same response (201/200)
}
```

追问 2（重试与错误语义）

**面试官：**“如果下游超时了你会怎么重试？怎么避免**重试风暴**？”

**你：**

“我把**幂等**和**重试**绑在一起设计：

- 客户端/任务统一用**指数退避 + 抖动**（如 200ms、500ms、1.2s…，上限 5–7 次）；
- 后端在返回体里给出**可重试与否**：`429/503` 搭配 `Retry-After`，**可重试**；`4xx` 里非瞬时错误**不可重试**；
- 幂等键保证重试**不会产生副作用**；
- 对**写链路的异步下发**（比如出库通知）用**事务外箱（Transactional Outbox）+ 队列**，消费端也按事件 ID 幂等；如用队列的 FIFO + 去重（SQS FIFO/内容去重）进一步降重。”

追问 3（项目落地）

**面试官：**“给我一个你真实遇到的例子。”

**你：**

“凡新那边在大促高峰，有用户在慢网环境**连点两次下单**，以前会出现两张‘相同订单’，后来我们把 BFF 统一生成 `Idempotency-Key`，落到后端做**占位 + 响应快照**，第二次直接重放响应，问题就没了。
在麦克尔斯那边，**支付平台回调**会在网络抖动时**重复推送** 3–5 次，我们用回调的 `eventId` 做幂等键，消费者先查‘已处理表’，见过就**幂等 ACK**，**不会重复扣款/更新**。这两处上线后，重复写导致的工单几乎归零，告警也更干净。”

### 限流-重试-熔断：客户端与服务端协同；退避策略；降级与快速失败

> **限流-重试-熔断**：入口令牌桶 + 服务内并发舱壁；只对幂等请求做**指数退避+抖动**（10% 重试预算）；**P95×1.5** 量级超时；错误/超时率阈值触发熔断与半开探测；读链路缓存降级、写链路排队受理；全链路观测 `429/5xx/熔断/重试`。

**面试官：**
“在大促或库存同步高峰时，你们的下游（支付、三方渠道、库存引擎）会抖动。你在（深圳市凡新科技 / 麦克尔斯深圳）如何**限流**、**重试**、**熔断**，既保护下游又保证整体体验？说说**策略与参数**，以及在 Spring Boot/K8s 里怎么落地？”

**你：**

“我把它当成一套‘**压力控制闭环**’：**入口限流 → 调用重试/超时 → 熔断与降级 → 指标与自愈**。

- **限流（服务端）**：网关/Ingress 层做**令牌桶**（每租户/每 API），比如：`RPS=200，突发=400`；服务内部再做**并发量阈值**（`maxConcurrent=200`），超出立刻**快速失败**返回 `429`，带 `Retry-After`。促销期我们会给**关键写接口**更严格的限额，读接口放宽。
- **限流（客户端/BFF）**：BFF 自身也做**本地并发限制**，避免把抖动放大成**重试风暴**。对同一个用户或同一个商品操作，我们会合并/串行化。
- **重试**：只对**幂等**的请求做，且**超时/503/429**才重试；使用**指数退避 + 抖动**（如 200ms、500ms、1.2s、2.5s、…，最多 5 次），并设**重试预算**（例如每分钟请求的 ≤10% 可用于重试），防止二次放大。
- **超时**：外呼都设置**硬超时**（P95×1.5 左右起步），避免线程长时间占用；不同下游不同超时，禁止“一个超时管天下”。
- **熔断**：错误率或超时率超过阈值（如 50% 且请求数≥N），**打开熔断**一段时间（比如 30–60s），期间直接失败；然后**半开**探测少量请求，恢复后再关闭。
- **降级**：读链路返回**缓存/近似值**（例如价格/库存读走 Redis 缓存 + 过期容忍），写链路则**排队/延后**（Outbox + 队列）并返回**受理中**；对非关键接口直接**功能降级**（如去掉次要字段/统计）。
- **观测**：分**版本/租户/接口**统计 `RPS、并发、429/5xx、超时率、重试次数、熔断状态`，并把**限流命中率**、**重试预算**暴露到仪表盘；做到‘哪里在保护、保护了多少’一目了然。

在 EKS + Spring Boot 的落地：网关（NGINX Ingress/ALB/API Gateway）做**硬限流**；服务内用 **Resilience4j**（或等价组件）做**TimeLimiter/Retry/RateLimiter/CircuitBreaker/Bulkhead**；部署层面给关键服务加**HPA + PDB + 合理的资源限制**，让自动扩容和限流协同而不是对冲。”

追问 1（具体数值与权衡）

**面试官：**“能给我一组你实际会用的参数吗？怎么权衡‘保护下游’和‘用户体验’？”

**你：**

“拿**库存读**举例：

- 网关限流：每租户 `RPS 200，突发 400`；服务内并发 `maxConcurrent 200`；
- 超时：P95 大约 80ms，那我会先设 150–200ms 的客户端超时；
- 重试：指数退避 + 抖动，最多 4–5 次；预算 10%；
- 熔断：统计窗口 10s，错误/超时率 >50% 且调用数≥50，打开 30s；半开 10 个请求探测；
- 降级：命中熔断即读缓存（可过期 30–60s），返回‘库存大于 0/未知’这类**弱一致**信息，并提示页面做**轻提示**。
  促销时我们宁可**严格保护写链路**（下单/扣减），把失败暴露得更明显一些，也不要把下游打挂导致**全局雪崩**。读链路能容忍短期不准，写链路尽量排队或受理中。”

追问 2（如何避免重试风暴）

**面试官：**“如果某个依赖刚好抖了 2–3 分钟，所有客户端一起重试会不会压塌你们？”

**你：**

“我们做了三件事：

1. **重试预算**：全局限制重试占比 ≤10%，一旦达到预算，就**不再重试**直接快速失败；
2. **抖动**：退避时间随机化，避免同一时刻同步重试；
3. **排队/背压**：服务内并发满载时直接拒绝（429），并把 `Retry-After` 写清楚；异步链路用队列限速消费。
   此外，**观测层**有‘**异常重试速率**’的告警，看见就先把重试预算降下来，必要时临时调低入口 RPS。”

追问 3（项目落地&复盘）

**面试官：**“有没有真实的事故和你们的改进？”

**你：**

“凡新那边有次第三方库存端点在半夜抖了 5 分钟，早期我们没做重试预算，导致重试量把线程池打满。复盘后我们引入**预算 + 并发舱壁**（Bulkhead），并把**缓存降级**做成开关，一键打开后 P95 直接回落。
在麦克尔斯那边，MakerPlace 图片处理链路偶发慢，我们把**超时**从 3s 切到 1.5s 并加熔断，前端降级为**低清图占位**；随后把慢服务拆到**独立队列**限速消费，线上体验稳定了。”

### 错误码与可观察性：统一错误模型 / Trace-ID / 指标-日志-链路关联

> **错误码与可观察性**：统一错误模型（`code/message/traceId/details`）+ 明确 4xx/5xx 映射；W3C trace 贯穿响应体/日志/指标；结构化日志携带 MDC（traceId 等）；RED 指标联动 APM；采样对 5xx/高延迟强制保留；日志脱敏与告警基线到位。

**面试官：**
“促销高峰里，你们的下单接口间歇性报错。客服只拿到一条报障：‘结算失败，请稍后重试’。你怎么通过**统一错误码**和**可观测性**（日志、指标、分布式追踪）**迅速定位**是用户错误还是服务端问题？结合你在（深圳市凡新科技 / 麦克尔斯深圳）的实践讲讲。”

**你：**

“我会把‘错误→定位’做成**一跳直达**：

1. **统一错误模型**让前端/客服拿到**可报障信息**：

```json
{ "code":"PAYMENT_TIMEOUT", "message":"Payment timeout, please retry",
  "traceId":"8a3c60f7…", "hint":"retry after 3s",
  "details":{ "orderId":"o123", "gateway":"stripe" } }
```

2. **Trace-ID** 贯穿**响应体 + 日志 + 指标 + 链路追踪**：客服把 `traceId` 给我们，我们在日志平台/可观测平台（EKS 上的 OTel/ADOT→Prometheus/Grafana/CloudWatch/X-Ray）就能**直跳到那条请求**。
3. 指标采用 **RED 法**（Rate/Errors/Duration）：先看该接口 `5xx/4xx` 比例和 P95 是否异常，再通过 `traceId` 打开**分布式链路**定位慢点或失败点（比如外呼支付网关超时 vs 我们的校验 400）。
   在凡新的库存/订单链路，我们这样能把‘用户填错地址’（**4xx**）和‘支付网关波动’（**5xx/超时**）几分钟内区分清楚；在麦克尔斯的 MakerPlace，我们把 `traceId` 展示在移动端错误页里，客服直接抄给我们。”

追问 1（设计与规范）

**面试官：**“错误码你怎么分层？HTTP 状态码和业务码怎么配？”

**你：**

“**HTTP 只表达通用语义**，**业务码表达具体原因**：

- 4xx：用户侧/可预期错误，比如 `VALIDATION_FAILED`、`AUTH_REQUIRED`、`RATE_LIMITED`；
- 5xx：服务侧/依赖侧，比如 `UPSTREAM_TIMEOUT`、`DB_DEADLOCK`、`STOCK_INCONSISTENT`。
  **映射规则**固定：`400/401/403/404/409/422/429` 用于常见场景；`500/502/503/504` 对应服务端/依赖错误。错误体统一四段：`code/message/traceId/details`；**message 面向人类**（可本地化），`code` 面向程序，`details` 放**可排障字段**（不含敏感信息）。
  另外我们会规定：**相同幂等键的重放**返回同一业务码，并在响应头加 `Idempotent-Replay:true`，排障更清楚。”

追问 2（工程落地）

**面试官：**“在 Spring Boot / K8s 上，你怎么把 Trace-ID 贯穿并串起‘指标-日志-链路’三件事？”

**你：**

“落地分三步：

1. **链路追踪**：W3C `traceparent` 头透传（网关→BFF→微服务→下游）；未携带则在网关生成。后端用 **OpenTelemetry SDK** 自动注入 span。
2. **结构化日志**：日志全用 JSON，`traceId/spanId/tenant/userId` 写进 **MDC**，日志 Appender 自动带上：

```java
// in a filter
String traceId = currentTraceIdOrGenerate();
MDC.put("traceId", traceId);
// controller logs will carry it; also return in body/header
```

3. **指标关联**：在计时器/计数器（Micrometer）上打 `api=placeOrder, outcome=success|error, http_status=...` 等标签；Grafana 面板支持按 `traceId` 链接到 APM，形成**点开指标→跳到具体 trace→再看相关日志**的三连。
   采样方面：默认 **概率采样**（如 5–10%），对 `5xx`/高延迟请求**强制采样**，确保关键问题有完整 trace。”

追问 3（质量与风控）

**面试官：**“怎么避免把隐私或安全数据打到日志？告警怎么设？”

**你：**

“我们做了两件事：

- **日志脱敏/拦截**：统一的 `LogSanitizer` 过滤 `password/token/card/email` 等字段；**从不**把 Authorization、JWT、银行卡号写日志；错误体的 `details` 也做白名单。
- **告警基线**：
  - SLO：如下单成功率 99.5%，**误差预算**消耗≥5% 触发告警；
  - `5xx` 比例、`429` 命中率、P95 超过阈值持续 5 分钟；
  - **错误码分布**偏移（例如 `PAYMENT_TIMEOUT` 激增）触发调度，第一时间看依赖健康与熔断状态。
    这样既安全又可定位。”

追问 4（真实案例）

**面试官：**“讲个你遇到的真实定位案例。”

**你：**

“凡新一次活动夜里，`/checkout` 的 `5xx` 抬头。我们从 Grafana 上看到 `UPSTREAM_TIMEOUT` 占比升高，随手点进一个 trace，发现**支付网关调用 3s 超时**。同时日志里同一个 `traceId` 显示我们内部处理只有 40ms——很快就定位到是**外部依赖抖动**，切开关走**降级队列**并调低重试预算，几分钟内恢复。
麦克尔斯那边有次是**4xx**暴涨，错误码 `VALIDATION_FAILED`，trace 里显示是 `postalCode` 校验新规则上线，回滚后即恢复。统一错误码 + trace 贯穿真的省了太多时间。”

### 灰度发布与回滚：逐步放量 / 健康检查 / 一键回滚

> **灰度与回滚**：滚动/金丝雀/蓝绿按风险选型；金丝雀 1%→5%→25%→50%→100%，以 `5xx/4xx/P95/资源/错误码分布` 设守卫，失败自动回滚；探针 + preStop 优雅下线；DB 走 **expand→migrate→contract**，特性开关解耦交付与发布；发布即实验、指标闭环。

**面试官：**
“你在（深圳市凡新科技 / 麦克尔斯深圳）做库存与下单链路时，如何把**高风险改动**安全上线？比如新版本涉及**缓存策略调整 + 一个字段语义变化**，你怎么做**灰度策略**、**健康检查**、以及**快速回滚**？”

**你：**

“我的思路是‘**发布即实验**’：把每次上线当成带监控的实验，**小流量试水 → 指标达标再放量 → 不达标立即回滚**。

- **发布策略选择**
  - **滚动更新（K8s 默认）**：常规小改动；`maxUnavailable=0、maxSurge=25%`，确保无损切换。
  - **金丝雀（Canary）**：有**性能/缓存语义**风险时，用 1%→5%→25%→50%→100% 分阶段放量；按 **4xx/5xx、P95、错误预算燃尽率** 设**自动阻断/回滚**阈值。
  - **蓝绿（Blue/Green）**：涉及**大版本或依赖升级**时，用全量双环境；流量开关一键切回旧环境。
  - **特性开关（Feature Flag）**：把业务开关与**交付开关解耦**，先灰度代码，再灰度开关。
- **健康检查与就绪**
  - **Startup/Readiness/Liveness** 三探针分工明确：Startup 给冷启动、Readiness 挡流量、Liveness 防僵死；
  - **preStop 钩子 + 优雅下线**（如 `sleep 5`），配合 Ingress/ALB 目标组的健康阈值，避免**半关闭阶段**丢请求；
  - **只读路径**先验证（GET/查询），**写路径**在金丝雀阶段**缩小配额**并捆绑幂等等保护。
- **回滚**
  - **一键回滚**：Helm `rollback` / Argo Rollouts `abort` / 网关流量权重回拨；
  - **数据向后兼容**：DB 走 **expand→migrate→contract** 两阶段，确保旧版本还能工作；
  - **事后验证**：回滚后继续观察 10–30 分钟，确认指标归位。”

追问 1（数据库与契约同步）

**面试官：**“如果涉及数据库表结构变更，你怎么保证灰度与回滚不被数据库‘卡住’？”

**你：**

“我严格走**向后兼容的双阶段**：

1. **Expand**：先上线 v1.5，加**新列/新表**（可空）与**写入双写**，读路径仍用旧列；
2. **Migrate**：异步回填/对账，监控读取命中率与差异；
3. **Switch**：新版本开始读新列，但**旧列仍保持实时同步**；
4. **Contract**：确认稳定（通常一到两周）才删除旧列。
   这样**任何阶段都可回滚**到旧版本；契约层面只做**向后兼容**的新增字段，不做破坏性变更。”

追问 2（阈值设定与自动回滚）

**面试官：**“你会设置哪些‘自动回滚’阈值？”

**你：**

“按**金丝雀批次**设硬阈值，例如每 5 分钟窗口：

- **错误率**：`5xx > 1%` 或 `4xx（非校验）> 3%` 立即阻断；
- **延迟**：`P95 较基线上浮 > 30%` 阻断；
- **资源**：`CPU>80% & GC 暴涨` 或 `容器重启` 超阈值阻断；
- **错误码分布**：`PAYMENT_TIMEOUT`、`STOCK_INCONSISTENT` 异常抬头即阻断。
  阻断后：**自动回滚 + 触发告警 + 开启降级**（读走缓存、写排队）。这些阈值在凡新大促和麦克尔斯的 MakerPlace 我都踩过坑，后来固定为发布模板的一部分。”

追问 3（工程落地：K8s / Spring Boot）

**面试官：**“能给我一套落地做法吗？”

**你：**

“K8s 侧：

- Deployment：`rollingUpdate.maxSurge=25%、maxUnavailable=0`；配 PDB 保证有**最小可用副本**；
- 探针：`startupProbe` > `readinessProbe` > `livenessProbe`，超时阈值分开设；
- HPA：根据 `CPU/内存/自定义 QPS` 指标自动扩，但**不代替**限流；
- 渐进式交付：**Argo Rollouts/Flagger** 配 `setWeight` 与 `metric` 守卫，失败自动 `abort`；
- 网关：ALB/NGINX 做**权重路由**；金丝雀阶段把**写操作**路由比例更低。

应用侧：

- Spring Boot 加**启动预热**（连接池/缓存）与**优雅停机**；
- 加**版本与commit id**到 `/actuator/info`；
- 监控面板预置‘上线看板’：金丝雀 vs 基线对比（RPS/5xx/4xx/P95/CPU/重启数）。这样**发布即实验**就能闭环。”

追问 4（真实落地）

**面试官：**“说个你当时印象深刻的灰度/回滚案例。”

**你：**

“凡新有次把**库存写入缓存策略**改成‘写后淘汰’，金丝雀 5% 时 `STOCK_INCONSISTENT` 错误码抬头且 P95 拉长，我们立即**abort 回滚**，切回旧策略，随后排查发现是**并发回写与淘汰顺序**导致短暂不一致；修复后再灰度就稳了。
麦克尔斯那边，移动端依赖的**图片处理服务**升级了底层库，1% 金丝雀就发现**内存泄漏 + 容器重启**，我们一键回滚并把这条链路拆出**独立队列限速**，再上线就顺滑了。”

---

## DB & Cache

### 索引选型与失效（B+Tree、组合索引、覆盖索引、左前缀）

> **索引选型与失效**：等值在前、范围在后，`(user_id, status, created_at DESC)` 覆盖列表页；避免函数包列与隐式转换；选择度差用**组合索引**提升；热查询必要时**拆专用索引**，不用指望索引合并；用 `EXPLAIN` 盯 `rows/key/filesort`。

**面试官**

“你在（深圳市凡新科技 / 麦克尔斯深圳）做订单与库存查询时，最常见的一条查询长这样：
按 `user_id + status` 过滤，按 `created_at DESC` 排序，取最近 20 条。偶尔还会加上 `channel`（Shopify/WooCommerce）。你会怎么设计**索引**？在什么情况下**索引会失效**，你怎么避免？”

**你**

“我会直接上一个**组合索引**：`(user_id, status, created_at DESC)`。

- 这样 `WHERE user_id=? AND status=?` 命中**最左前缀**，`ORDER BY created_at DESC LIMIT 20` 能做到**顺序读**，几乎不用再做 filesort。
- 如果查询只返回列表页字段（比如 `order_id, total, created_at, status`），我会把这些字段也纳入**覆盖索引**（只要都在二级索引上，查询就**不回表**）。
- 如果经常用到 `channel` 维度，我会根据**查询比例**和**选择度**决定是否把索引改成 `(user_id, channel, status, created_at DESC)`，或者再开一条以 `channel` 为第二位的索引（避免一个索引承担所有模式）。
- 文本模糊搜索我不会指望 B+Tree，`LIKE '%kw%'` 这种我会用**倒排/全文索引**或**ES**，避免全表扫。”

“**可能失效的坑**我会提前规避：

- **函数/计算包住列**：`DATE(created_at)=?` 会让索引失效，我改成 `created_at >= '2025-09-01 00:00:00' AND < '2025-09-02 00:00:00'`。
- **隐式类型转换**：`user_id` 是 `BIGINT`，但参数传字符串，可能走不到索引，我会在 DAL 层**强类型**。
- **范围列放前面**：`WHERE created_at > ? AND status = ?`，如果把 `created_at` 放在索引前面，后面的列就用不上索引了；所以把**等值列在前，范围列靠后**。
- **选择度差**：`status` 只有 3 个值，单列索引没意义，要靠 `user_id + status` 的组合来提高选择度。
- **回表过多**：遇到‘扫 10 万行再回表’的情况，我会把列表页必要字段放进覆盖索引里，或者改成**先查主键再回表**的两段式。”

“上线前我会用 `EXPLAIN` 看 `type`、`rows`、`key` 和 `Using index/Using filesort`；`rows` 过大就说明选择度不行。慢查询里还会看**扫描行数**与**回表次数**，必要时调整索引顺序或再拆一个更贴近热查询的索引。”

追问 1（实战细节）

**面试官：**“如果同样的查询，偶尔要先按 `channel` 过滤再按 `user_id` 呢？你是加一个新的 `(channel, user_id, status, created_at)` 还是用索引合并？”

**你：**

“我一般会**加一条新索引**，不要指望索引合并（AND 合并常常不稳定且代价高）。判断标准是**这条访问模式的占比**是否值得一条新索引。如果比例不高，我会把这类查询**引导到报表库/异步计算**，避免在 OLTP 上堆太多索引。”

追问 2（排序与覆盖）

**面试官：**“`ORDER BY created_at DESC` 一定能用上索引排序吗？什么情况下会退化成 filesort？”

**你：**

“有两个常见退化点：

1. **排序列不在索引的连续前缀里**（或者方向不一致）；
2. **查询列不被索引覆盖**且回表顺序与排序不一致，优化器可能选择 filesort。
   所以我会把 `created_at DESC` 放在组合索引的最后一位，并尽量让列表页**覆盖索引**，这样基本避免 filesort。”

追问 3（真实案例）

**面试官：**“讲一个你线上遇到的‘索引失效’问题。”

**你：**
“有一次活动页报慢，我们发现 SQL 写了 `WHERE DATE(created_at)=CURDATE()`，直接把索引废了；改成半开区间后 P95 从 900ms 掉到 80ms。还有一次是 `user_id` 参数是字符串，发生了**隐式转换**，修掉参数类型后 `rows` 从十几万降到几百。”

### 事务与隔离级别：RC / RR；MVCC 的一致性读 vs 当前读；如何避免幻读与超卖

> **事务与隔离**：读走 **RC + 一致性读** 提并发；写用**条件更新**（`UPDATE … WHERE qty>=?`）或**当前读 + 明确锁**保正确；RR 防幻靠 Next-Key，但更易死锁；唯一约束/插入即占位 > 锁；控制**短事务、统一加锁顺序、失败可重试**。

**面试官**

“你在（深圳市凡新科技 / 麦克尔斯深圳）做下单与库存扣减时，既要抗高并发，又要保证**不超卖**、**不出现脏数据**。MySQL/InnoDB 的默认隔离级别是 **可重复读（RR）**，很多团队又把线上切到 **读已提交（RC）** 提升并发。你会怎么选？具体到 **MVCC 的一致性读** 和 **当前读（锁定读）** 语义，你怎么落地？”

**你（口语化回答示范）**

“我把读写分两类：

- **一致性读（snapshot read）**：普通 `SELECT`，用 MVCC 读历史快照。RR 下‘同一事务多次查到的是同一份视图’，避免**不可重复读**；RC 下‘每条语句读最新已提交’，更‘新鲜’，但可能前后两次结果不同。
- **当前读（locking read）**：`SELECT … FOR UPDATE/ FOR SHARE`、`UPDATE/DELETE`，会加锁。RR 下会用**间隙锁/Next-Key 锁**来防止范围内的**幻读**；RC 下默认不加间隙锁，更多用**记录锁**，并发更高，但范围查询更容易出现幻写/插入竞争。

**我的选择**：

- **读路径**（列表/详情/报表）多用 **RC + 一致性读**，减少锁冲突，实时性也更好；
- **写路径**（下单扣减/配额/优惠券核销）不用纠结隔离级别，直接用 **‘条件更新’原子写法**或**当前读 + 明确锁**来保证正确性。”

**避免超卖的两种写法**

1. **条件更新（推荐）**：

```sql
UPDATE stock
SET qty = qty - :n
WHERE sku = :sku AND qty >= :n;
-- 受影响行数 = 1 才算成功；=0 表示库存不足
```

- 优点：不需要先 `SELECT FOR UPDATE`，天然**原子**，在 RC/RR 下都安全，锁粒度小、冲突少。
- 适用于“数值扣减”类场景（库存、配额、次数）。

2. **当前读 + 再写**：

```sql
START TRANSACTION;
SELECT qty FROM stock WHERE sku=:sku FOR UPDATE; -- 锁住这行（RR 下还会锁间隙）
-- 校验与扣减
UPDATE stock SET qty = qty - :n WHERE sku=:sku;
COMMIT;
```

- 适合需要**读取多个字段**再决定写入的业务；
- RR 下能靠 Next-Key 防**幻写**，但更可能**死锁**，需要**重试机制**与**短事务**控制。

**常见坑与我的做法**

- **幻读/重复写**：RC 下范围条件的 `SELECT … FOR UPDATE` 不锁间隙，可能插入“新纪录”造成幻影 → 改成**条件更新**或**唯一约束 + INSERT IGNORE/ON DUPLICATE KEY** 防重复。
- **长事务**：RR 下长事务会让 MVCC 的 undo 链变长，吞吐变差、历史回收受阻 → 严控**事务边界**，把计算挪到事务外；UI 上避免“开事务等用户操作”。
- **死锁**：并发写相同/相邻键很常见 → 统一**加锁顺序**（按主键/索引顺序）、**缩短事务**、**减少回表**（覆盖索引），并在代码里**捕获死锁并指数退避重试**。
- **隐式类型/函数包列**导致索引失效 → 在写路径尤其要**强类型**和**区间写法**。

**面试官追问 1**

“如果要‘按条件占位’避免同一用户重复下单，你选 RR 还是 RC？”

**你**

“都可以，我更倾向**唯一约束**+**插入即占位**，规避隔离级别差异：

```sql
-- 唯一键 (user_id, order_uuid)
INSERT INTO orders(user_id, order_uuid, status, ...)
VALUES(:u, :oid, 'PENDING', ...)
ON DUPLICATE KEY UPDATE touched_at = NOW(); -- 幂等
```

或者对核销码/优惠券也是**唯一约束**来杜绝重复消费，比锁更干脆。”

**面试官追问 2**

“实际线上你怎么定位隔离级别相关的问题？”

**你**

“我会：

- 打开 `EXPLAIN` 看是否走到**覆盖索引**，减少锁范围；
- 用 `performance_schema`/慢日志看**等待事件**（锁等待/扫描行数）；
- 碰到死锁，抓 `SHOW ENGINE INNODB STATUS`，审计‘谁先锁了谁’→ 调整**加锁顺序**或改成**条件更新**；
- 对高争用表建**合理分区/热点拆分**，降低冲突；
- 定期巡检**长事务**、**历史版本长度**，看到异常就查出是哪个业务把事务拉得太长。”

**真实案例**

“凡新那边高峰期遇到过**优惠券重复核销**风控误判，我们一开始用 `SELECT … FOR UPDATE` 查是否核销过，再写入；在 RC 下因为不锁间隙，边界条件下出现并发插入比赛的窗口。后来改成**唯一索引 + 插入即占位**，问题消失。
在麦克尔斯那边，做**库存预留**时从 RR 改 RC 后，读写冲突少了一半，但我们把关键写全部改为**条件更新模式**，再配幂等与重试，稳定住了。”

### 慢查询定位：执行计划、扫描行数、回表/下推、坏味道清单

> **慢查询定位**：用慢日志与 `EXPLAIN ANALYZE` 定位“扫描多/回表多/排序慢/锁等待”；组合索引对齐过滤+排序并**覆盖**；谓词改写（半开区间、强类型、去函数/OR）；大分页用**seek**，Join 用**小表驱动**。

小清单

- `EXPLAIN ANALYZE` 看真实耗时；盯 `type/key/rows/Extra`
- 组合索引：等值在前、范围在后；排序方向一致；尽量覆盖索引
- 谓词改写：半开区间、强类型、去函数包列/避免前置 `%`
- 分页：大页改**seek**；Join：小表驱动大表
- 复盘：慢日志 + rows_examined + 统计信息更新

**面试官**

“你在（深圳市凡新科技 / 麦克尔斯深圳）遇到一个接口偶发 900ms+：按 `user_id + status` 过滤，`ORDER BY created_at DESC LIMIT 20`。请你**在线上**快速判断瓶颈在哪，并说说你会如何**定位 → 证实 → 修复**？”

**你**

“我有一套**五步法**，基本两三分钟就能把方向定下来：

**① 先用慢日志/性能面板确定是‘DB 内慢’，还是‘网络/应用层慢’**

- 看 `rows_examined`、`query_time`、`lock_time`；如果 `query_time` 高但 `rows_examined` 很低，多半是**锁/等待**；如果 `rows_examined` 很大，就是**扫描多**。

**② 上 `EXPLAIN`（MySQL 8 用 `EXPLAIN ANALYZE`）看真实代价**

- 关注 `type`（`ALL`/`range`/`ref`/`eq_ref` 越靠后越好）、`key` 是否命中正确索引、`rows` 估算值、以及 `Extra`：
  - `Using index`（覆盖）、`Using where`（回表筛）、`Using filesort`、`Using temporary`、`Using index condition`（ICP 下推）。
- `EXPLAIN ANALYZE`还能看到每步实际耗时与行数，能直接证伪“是不是排序/回表慢”。

**③ 检查‘坏味道’**

- `LIKE '%kw%'`、`DATE(created_at)=…`/对列做函数、隐式类型转换（字符串比 BIGINT）、`OR` 把索引打散、**范围列放在组合索引前**、`ORDER BY` 顺序/方向与索引不一致、`SELECT *` 导致**回表**、大 `OFFSET` 分页。

**④ 快速修法**（优先不改业务）

- **索引对齐**：按这个查询我会用 `(user_id, status, created_at DESC)` 并让列表页字段尽量**覆盖索引**；
- **改写谓词**：把 `DATE(created_at)` 换半开区间；把字符串参数转成强类型；
- **排序与分页**：若仍有 `filesort`，用**覆盖+方向一致**避免；超大分页改为**游标/seek**：`WHERE (created_at, id) < (?, ?) LIMIT 20`；
- **Join 顺序**：小表驱动大表，必要时加 hint；
- **统计信息**：更新表/索引统计，避免优化器走岔路。

**⑤ 复核与回归**

- 再跑 `EXPLAIN ANALYZE` 与压测，确认 P95/P99 回到目标；把修复写进‘索引设计规范’和‘SQL 代码评审清单’。”

追问 1（动手演示）

**面试官：**“就拿你这条 `user_id+status` 的查询，EXPLAIN 看到了 `Using filesort`，而且 `rows` 很大，你怎么落地修？”

**你：**

“我先看索引：

- 如果当前只有 `(user_id)`，我会改成**组合索引** `(user_id, status, created_at DESC)`；
- 列表页只用 `order_id, total, status, created_at`，我把这些字段也放进索引，形成**覆盖索引**；
- 再跑 `EXPLAIN ANALYZE`，目标是看到 `type=range/ref`、`Using index`，没有 `Using filesort`。
  必要时把 `LIMIT 20` 的大偏移分页改成**seek 分页**，配合 `(created_at DESC, id DESC)` 复合排序键。”

追问 2（索引失效的真实坑）

**面试官：**“说个你遇到的‘一行代码让索引失效’的例子。”

**你：**

“典型就是 `WHERE DATE(created_at)=CURDATE()`，或者把 `user_id` 当字符串传，直接**全表扫**。线上修法：立刻改半开区间/强类型，P95 直接从百毫秒量级掉回两位数。”

追问 3（回表与下推）

**面试官：**“怎么判断是**回表**拖慢还是**排序**拖慢？”

**你：**

“看 `EXPLAIN ANALYZE` 的每步耗时：

- 如果 `Using index condition` + `rows` 很大但 `table` 回主键花时高，是**回表**多 → 做覆盖索引/减少列；
- 如果 `Using filesort` 的耗时高，说明**排序**是瓶颈 → 调整索引顺序与方向，或改游标分页。”

追问 4（大表 join）

**面试官：**“两张大表 join 变慢呢？”

**你：**

“先确保**连接键都有索引**，让 `type` 到 `ref/eq_ref`；用**小表驱动大表**（或子查询先裁剪大表），避免 `ALL`；必要时做**中间结果落地**或用 **覆盖索引 + 主键回表**两段式。”

### 读写分离的坑：主从延迟、读旧值、强一致读 / 亲和策略

> **读写分离**：读旧值的解法 = **读亲和 + 主读回退 + 延迟感知**；配 `read_token`（GTID/位点）实现 **read-your-writes**，超时回主；延迟异常时关键读走主、非关键读走缓存并提高 TTL；有开关/权重剔除与监控闭环。

**面试官**

“你在（深圳市凡新科技 / 麦克尔斯深圳）做读写分离后，用户刚下单立刻点订单详情，却偶发‘查不到’或看到旧状态。你怎么**稳定地做到 read-after-write**？当复制延迟上来时你会怎么**降级**？”

**你**

“读写分离最大的坑就是**主从延迟**带来的‘读旧值’。我一般从三层下手：**路由策略、读一致性令牌、退化/降级**。”

1. **路由策略（亲和 + 回退）**
   - **读亲和（stickiness）**：同一用户/会话的读请求在短窗口内（如 60–120s）**固定到同一只从库**，减少随机命中落后副本的概率。
   - **主读回退**：写后**关键读**（订单详情、支付状态）直接读主库；或先读从库，若未命中/版本低则**自动回读主库**。
   - **延迟感知**：每只从库上报 `Seconds_Behind_Master` / 复制位点；超过阈值（如 >2s）就**暂时摘出读池**，或降低权重。
2. **读一致性令牌（read token / GTID bookmark）**
   - **写入时打戳**：拿到 **GTID**（或 binlog 位点）作为 `read_token` 填到响应/上下文（也可放 Redis）。
   - **读时阻塞 / 选择副本**：路由层要求副本执行进度 ≥ token；MySQL 8 可用 `WAIT_FOR_EXECUTED_GTID_SET(token, timeout)`；超时则**回主库**。
   - 这样能保证 **read-your-writes**：同一用户在写后的第一次关键读取一定看到自己的最新数据。
3. **退化与降级**
   - **功能退化**：当延迟异常时，把“非关键读”走缓存或展示“已受理，稍后刷新”；**关键读**强制走主库。
   - **缓存配合**：写路径**先写 DB 再删缓存**（或消息通知回源）；延迟高时**提高临时 TTL**，避免击穿把主压垮。
   - **开关与监控**：有‘**主读开关**’与‘**从库剔除**’开关；仪表盘盯 `延迟/主读比例/回主次数/P95`，超阈值自动切策略。”

“在凡新做活动高峰时，我们对**订单/支付**这类链路默认‘**写后首次读→主库**’，且带 **`read_token`**。在麦克尔斯，MakerPlace 的作品发布后立即查看也走同样逻辑；非关键读（列表/统计）则尽量留在副本，保证总体吞吐。”

追问 1（工程细节）

**面试官：**“你在 Java/Spring 中怎么实现 `read_token` 这一套？”

**你：**

“写请求返回时把 `read_token` 放到响应头/Body；BFF 持久化到用户会话。读请求前在路由拦截器里：

1. 如果携带 token → 先挑**进度足够**的从库；
2. 没有副本满足或超时 → **回主库**；
3. 成功后更新‘该用户→该副本’的亲和映射（短 TTL）。

同时在 MyBatis/JPA 的数据源路由里支持 `forcePrimary()` 标记，让关键接口显式声明强一致读。”

追问 2（复制延迟来源与治理）

**面试官：**“延迟经常是谁拖的？你怎么治？”

**你：**

“常见原因：**主库长事务/大批量写**、从库 I/O 慢、网络抖动、从库查询过重。治理：

- 严控**长事务**和大批量 DDL/DML，拆批次；
- 从库只做**只读**业务，不跑重查询；
- 监控位点滞后，自动**摘除落后副本**；
- 跨区/跨地域复制尽量前置**异地只读场景**，关键链路避免跨地域强一致要求。”

追问 3（真实案例）

**面试官：**“说个你线上遇到的‘读旧值’事故。”

**你：**

“凡新某次‘下单后立刻查看’返回了未支付状态，是因为随机打到**落后 3–4 秒**的从库。上线 **`read_token` + 主回退** 后，关键读不再随机；另外把延迟阈值从 2s 调到 1s，并把那只从库自动降权，问题就消失了。
在麦克尔斯，发布作品后列表没及时刷新，我们加了**列表读缓存 + 变更消息**去**主动失效**缓存，同时把**首次查看走主**，体验就稳定了。”

### 缓存一致性：Cache-Aside 双删顺序、消息通知/回源、热键与热点保护

> **缓存一致性**：写库→删缓存（必要时**延时双删**/CDC 消息）；读 miss 用**互斥回源** + **逻辑过期** + **TTL 抖动**；热键用**本地+远端两级缓存**、**限速重建**与**预热**；值带 **version/etag**，Lua 原子“新旧值比较”防回灌，消费者幂等保证最终一致。

**面试官**

“你在（深圳市凡新科技 / 麦克尔斯深圳）做商品与库存读多写少的场景：价格、库存、商品详情都会进 Redis。高峰期经常发生**脏读**或**雪崩式回源**。请你说明**写路径怎么保证一致性**、**读路径怎么防止击穿**，以及**热 Key 如何防‘缓存重建风暴’**？”

**你：**

“我把它拆成三块：**写路径的删序与可靠失效**、**读路径的回源互斥**、**热键保护**。”

1 - 写路径：先写库，再删缓存（必要时‘延时双删’/消息通知）

- **基本顺序（Cache-Aside）**：
  1. **写 DB** 成功；
  2. **删 Cache**（`DEL product:{id}`）；
  > 不先删再写，避免‘删了却写失败’导致**长期空洞**。
- **并发场景的竞态**（T1 更新、T2 读）：T1 写库后刚删缓存，T2 在删之前回源把**旧值**重新塞回缓存。
- **解法 A：延时双删**：
  - 写库 → 立即 `DEL` → **sleep 200–500ms** → 再 `DEL`（或通过**延时队列/MQ**二次失效）。
- **解法 B：消息通知/CDC**：
  - 写库后**可靠地**发布 `productUpdated(id, version)` 到 Kafka/Redis Stream；消费者订阅做 `DEL`/对比 `version` 决定是否覆盖。
  - 我们在凡新把商品价改、库存改都走 **Outbox + 消息**，重放安全、跨服务也能同步；在麦克尔斯，作品发布后用 **Redis Stream** 做失效广播，移动端立刻看到最新。
- **版本化防回灌**：缓存值里带 `version/etag`；写入方只在 `newVersion >= cachedVersion` 时覆盖（Lua 脚本原子判断）。

2 - 读路径：互斥回源 + 逻辑过期 + 抖动 TTL

- **互斥回源（单飞 / mutex）**：Cache miss 时对 `rebuild:{key}` 做 `SET NX PX` 拿**短锁**（如 1–3s），拿到锁的线程**唯一**回源 DB 并回填；其它线程直接读老值或短暂等待。
- **逻辑过期（stale-while-revalidate）**：缓存里存 `data + expireAt`。
  - 读到**过期但未严重过期**时：先返回**旧值**，后台异步刷新；
  - 严重过期或拿到互斥锁：同步刷新。
  - 这样高峰期不会所有请求都打到 DB。
- **TTL 抖动**：给 TTL 加随机（±10–20%），避免**同一时刻**大面积同时失效引发雪崩。
- **旁路回源保护**：对空值做**短 TTL 缓存**（例如 30–60s）防穿透；对“超大对象”分片缓存，避免单值过大回源慢。

3 -  热 Key 与热点保护

- **本地 + 远端两级缓存**：在 BFF/网关层用 **Caffeine** 做本地 50–200ms 的**极短 TTL**，命中率能挡掉一批抖动；远端 Redis 继续做主缓存。
- **热键互斥 + 限速重建**：
  - 热度检测（QPS、失败率）触发时，对该 key 设 **更长 TTL** 并且**只有一个重建者**（互斥锁 + 队列）；
  - 重建频率加**令牌桶**，每秒最多 N 次回源。
- **分片/哈希打散**：对计数类热键（pv/like）用**分片 key** 聚合，避免单槽被打爆。
- **预热**：发布/大促前，对确定的热点 SKU 批量**预热缓存**，并启用**逻辑过期**延长可服务窗口。

追问 1（工程细节）

**面试官：**“Java 里你怎么写这套互斥回源与版本化？”

**你：**

“互斥用 Redis：

```java
String lockKey = "lock:product:" + id;
boolean locked = redis.set(lockKey, nodeId, SetArgs.Builder.nx().px(2000));
if (locked) {
  try {
    Product p = db.load(id);
    // 带版本回写（Lua 保证原子判断 newVer >= oldVer）
    redis.evalsha(updateIfNewerSha, keys=[cacheKey, verKey], args=[pJson, p.version]);
  } finally {
    // 只释放自己加的锁（value 比对）
    releaseLock(lockKey, nodeId);
  }
} else {
  // 返回旧值（逻辑过期也可先回旧）
  return cachedOrFallback();
}
```

版本化我用 Lua 做：`if newVer >= currVer then set value & ver`，避免“旧值回灌”。

追问 2（失败与补偿）

**面试官：**“如果删缓存失败，或者消息消费者短暂挂了会怎样？”

**你：**

“删失败就**重试 + 告警**；消息侧我们用 **Outbox**（DB 事务里落消息）+ **至少一次** 语义，消费者**幂等**（按 key + version 去重）。如果消费者挂了，重启后会把落下的消息**补拉**补删；同时写路径保留**延时双删**，两条腿保证最终一致。”

追问 3（真实案例）

**面试官：**“讲一个你们解决‘缓存重建风暴’的真实例子。”

**你：**

“凡新的一次大促，`/product/{id}` 在 0 点同时失效，直接把 MySQL 顶高。我们加了**TTL 抖动 + 逻辑过期 + 单飞互斥**，并提前**预热热点 SKU**，开售后 QPS 峰值时 DB 仍在可控范围。

麦克尔斯那边，作品详情是**高基数但局部热**，我们在 BFF 加了 **Caffeine 100ms 本地缓存**，并把回源改为**互斥重建 + 限速**，p95 从 ~280ms 降到~90ms。”

### 三座大山：穿透 / 击穿 / 雪崩（识别与治理清单）

> **穿透/击穿/雪崩**：穿透→**空值缓存 + 校验 + 布隆**；击穿→**单飞互斥 + 逻辑过期 + 预热/两级缓存 + 限速重建**；雪崩→**TTL 抖动 + 分级限流/降级 + 渐进放量 + 多级兜底**；全程用**版本化写回**防旧值回灌，并监控 `miss burst / db fallback / hotspot TopN`。

**面试官**

“你在（深圳市凡新科技 / 麦克尔斯深圳）给商品与库存做缓存时，遇到三类经典事故：

1. **穿透**：请求大量不存在的 `productId`，缓存每次都 miss，DB 被打爆；
2. **击穿**：一个**超热点 Key** 恰好过期，瞬间几十万请求同时回源；
3. **雪崩**：大批 Key 在同一窗口过期或 Redis 集群重启，DB 被洪峰淹没。
   你分别如何**识别**与**治理**？”

**你：**

“我给自己准备了一个**小清单**，每类问题都有‘**识别信号 → 快速止血 → 长期治理**’。”

1 - 穿透（请求的 Key 根本不存在）

- **识别信号**：缓存 **Hit 率骤降**，但**每个 Key 的访问频次很低**；DB QPS 升、`NOT FOUND` 比例高。
- **快速止血**：
  - **空值缓存**：把不存在结果以**短 TTL**（30–60s）缓存，挡住重复探测；
  - **前置校验层**：对 `productId/sku` 做**格式/范围校验**，明显非法的直接拦截；
  - **限流**：对同一来源/同一前缀做速率限制。
- **长期治理**：
  - **Bloom Filter / 布隆**：把已存在的 `id` 集合装入布隆，拦截不存在的 Key；
  - **风控名单**：来源异常（爬虫/脚本）进入黑名单或更强校验。
- **真实例子**：凡新双 11 前被爬虫扫空 ID 段，Hit 率跌到 20% 左右；临时启**空值缓存 + 前置校验**立刻止血，后续上线**布隆**恢复到 80%+。

2 - 击穿（超热点 Key 过期瞬间被打穿）

- **识别信号**：Hit 率总体高，但**单 Key QPS 极高**，且在过期点出现**锯齿形** DB 峰值。
- **快速止血**：
  - **单飞互斥（mutex）**：miss 时用 `SETNX PX=2s` 抢锁，仅**一个线程**回源，其它请求要么等锁、要么直接返回旧值；
  - **逻辑过期**：缓存里存 `expireAt`，**轻度过期**时先返回**旧值**并后台刷新；
  - **强制延长 TTL**：临时把热点 Key TTL 拉长（开关可控）。
- **长期治理**：
  - **预热**：大促/上新前对热点 Key 预热；
  - **本地 + 远端两级缓存**（如 BFF Caffeine 50–200ms 超短 TTL）；
  - **重建限速**：对同一 Key 的回源加令牌桶，每秒最多 N 次重建。
- **真实例子**：麦克尔斯作品详情在发布后 1 分钟内 QPS 飙升，改为**逻辑过期 + 单飞互斥 + 两级缓存**后，DB 峰值压回正常区间，p95 从 \~280ms 降到 \~90ms。

3 - 雪崩（大面积同时失效 / 缓存整体不可用）

- **识别信号**：**大量 Key 同时过期或 Redis 重启**后，DB QPS 呈**整段抬升**；服务端 CPU 飙高，排队/超时增多。
- **快速止血**：
  - **入口分级限流 + 降级**：非关键读直接返回旧值或“受理中”，**关键读走主库**；
  - **熔断**：下游 DB 超时/错误拉高时，网关/服务端短时间**快速失败**，避免排队雪崩；
  - **随机 TTL 抖动**：紧急把 TTL 加随机（±20%）并逐步放量。
- **长期治理**：
  - **TTL 打散策略**：生成缓存时随机化 TTL；
  - **多级兜底**：热点用两级缓存；静态或半静态内容加 **CDN/边缘缓存**；
  - **冷启动保护**：Redis 集群重启后，恢复阶段**渐进式放量**（金丝雀），并对回源加**全局重建速率**上限；
  - **数据分层**：把“必需数据”（配置/白名单）做**更长 TTL** 或持久化本地快照，保证最低可用。
- **真实例子**：凡新某晚 Redis 集群滚更，错把一批 Key 设置成同一 TTL，半小时后**同刻失效**引发 DB 洪峰。复盘后统一上线**TTL 抖动**、**冷启动渐进放量**，并给热点接口加本地兜底。

追问（工程细节）

**面试官：**“如果缓存里是**旧值**但你又不想卡住用户体验，你会怎么做？”

**你：**

“走**stale-while-revalidate**：在轻度过期窗口内**先回旧值**，同时后台异步刷新。只有严重过期或加锁成功的请求才同步回源。用户体验稳定，DB 负载也平滑。”

**面试官：**“怎么避免‘旧值回灌’？”

**你：**

“**版本化写回**：缓存结构里带 `version/etag`，Lua 脚本原子地比较 `newVer >= currVer` 再覆盖；写路径也配**延时双删/CDC** 把旧值清干净。”

**面试官：**“你怎么监控这三类问题？”

**你：**

“按 **Key 前缀/接口** 维度看 `hit ratio、miss burst、db fallback qps、回源耗时、互斥锁命中率、热点 TopN、TTL 分布`。出现 miss 突增且 DB 回源同步拉高，基本就能定位是哪一类问题。”

---

## Message and Consistency

### Outbox（事务外箱）& 本地事务边界

> Outbox：单库事务落数据+事件，Publisher 异步发布；至少一次发送 + 幂等消费（`eventId`/`aggregateId+version` 去重）≈ 几乎一次；分区键保证同聚合顺序；失败退避重试，CDC/归档保性能。

**面试官**

“下单成功要**占库存**并**通知**支付/仓库/搜索。如何保证**写数据库**和**发消息**‘要么都成功，要么都不成功’，避免双写不一致？你在（深圳市凡新科技 / 麦克尔斯深圳）是怎么落地的？”

**你：**

“我们用**事务外箱**（Outbox）把‘数据变更’和‘事件生成’放到**同一个本地事务**里：

- **应用事务**里做两步：① `INSERT/UPDATE` 业务表（orders/stock…）② `INSERT outbox(event)`。只要事务提交，**两者一定同时落地**。
- 事务外的**Publisher**轮询/CDC 读取 `outbox`：`SELECT ... FOR UPDATE SKIP LOCKED LIMIT N`，发布到 Kafka/SQS/Redis Stream，成功后把这行标记 `SENT`，失败就**退避重试**并回写 `attempts/next_retry_at`。
- **消费端幂等**：事件里自带 `eventId`（或 `aggregateId+version`），消费者先查 `processed_events`（唯一键），**见过就直接 ACK**，没见过才处理并插入标记。
  总体语义是**至少一次发送 + 幂等消费 = 几乎一次（effectively-once）**。我们在凡新下单→扣减库存→异步出库通知这条链路就是这么做的；在麦克尔斯，作品发布→索引更新→通知订阅者也同理。”

**简化表结构（示意）**

```sql
CREATE TABLE outbox (
  id           CHAR(36) PRIMARY KEY,      -- eventId (UUID)
  aggregate_id CHAR(36) NOT NULL,         -- 订单/商品ID
  aggregate_type VARCHAR(32) NOT NULL,    -- ORDER / STOCK ...
  event_type   VARCHAR(64) NOT NULL,      -- OrderCreated / StockReserved...
  payload      JSON NOT NULL,
  version      INT NOT NULL,              -- 聚合版本，用于顺序/去重
  status       ENUM('NEW','SENT','FAILED') DEFAULT 'NEW',
  attempts     INT DEFAULT 0,
  available_at DATETIME NOT NULL,         -- 下次可重试时间（退避）
  created_at   DATETIME NOT NULL
);

CREATE TABLE processed_events (
  event_id CHAR(36) PRIMARY KEY,          -- 去重键
  processed_at DATETIME NOT NULL
);
```

**应用层伪代码**

```java
// Tx begin
insertOrder(...);                 // 业务写
insertOutbox(event(orderId,...)); // 同库插入消息
// Tx commit  => 原子性成立
```

**Publisher 轮询（要点）**

- 批量拉取 `status=NEW AND available_at<=now()`，用 `FOR UPDATE SKIP LOCKED` 防并发争抢；
- 发布成功 → `status=SENT`；失败 → `attempts++`，按 `min(backoff*2^attempts, cap)` 回写 `available_at`；
- **崩溃边界**：如果“已发布但未标记 SENT”重启后会再发一次，所以**消费者必须幂等**。

**顺序性与分区**

- 需要**同一订单的事件按序**：Kafka 用 `partitionKey=orderId`；SQS 用 **FIFO**，`messageGroupId=orderId`；Redis Stream 按 stream per-aggregate 或在消费者端序列化处理。
- 跨聚合全局顺序一般**不保证**，用**因果字段**（version/timestamp）校正。

**真实落地小例子**

- 凡新：`OrderCreated` 与 `StockReserveRequested` 一起落库；Publisher 发 SQS，消费者是库存服务；出现网络抖动时，**重复消息**被 `processed_events` 吸收，不再重复扣减。
- 麦克尔斯：作品发布事件走 Kafka，搜索索引消费者先做**去重**再写 ES；Publisher 用**退避+抖动**，避免抖动期把队列压爆。

**追问 1：为什么不用 DB 里直接调用消息系统事务？**

“跨资源的**分布式事务**（2PC）复杂且脆弱，Outbox 把一致性**收敛在单库事务**里，外围只需‘至少一次 + 幂等’，工程成本更低、恢复性更好。”

**追问 2：如果消息量很大，轮询会不会很慢？**

“生产里我们更倾向**CDC（如 Debezium）**或**分片轮询**：按时间或主键区间扫描；并用**归档/TTL**压缩 `SENT` 行，保持表小而快。”

### 幂等消费与去重键：表/Redis 实战与失败补偿

> **幂等消费**：`eventId`（或 `aggregateId+version`）做去重键；**同库事务**里先插 `processed_events` 再执行业务写；写法用**条件更新/UPSERT/版本检查**做到可重放；Redis `SETNX+TTL` 快速挡重复，DB 约束兜底；失败走 **DLQ+重放**，顺序用**分区键/FIFO**。

**面试官**

“支付平台和库存系统都会**重复投递**事件（网络抖动/重试）。你在（深圳市凡新科技 / 麦克尔斯深圳）如何保证**消费端幂等**？具体怎么做**去重键**、**落库顺序**、**失败补偿**？”

**你：**

“我的做法是把 ‘**去重 + 业务落库**’ 放进同一个本地事务里，保证 ‘**要么都成功，要么都回滚**’，并且提供**双层去重**（DB 强约束 + Redis 快速挡重复）。

1. **去重键**
   - 统一使用 `eventId`（UUID）或 `aggregateId + version` 作为**全局唯一键**。
   - **DB 层**建 `processed_events(event_id PK)`，天然幂等；
   - **Redis 快速去重**：`SETNX de:{eventId} 1 EX <TTL>` 抢到再处理，没抢到直接 ACK（避免同一时刻并发重复执行）。
2. **同库事务顺序**（Inbox 模式）
    ```sql
    -- 在一次事务内完成两件事：业务变更 & 去重落账
    BEGIN;
    -- ① 先插 processed_events（如果已存在直接报错/返回）
    INSERT INTO processed_events(event_id, processed_at) VALUES(:eid, NOW());
    -- ② 再做业务落库（UPDATE/INSERT ...）
    UPDATE stock SET qty = qty - :n
        WHERE sku=:sku AND qty >= :n;  -- 条件更新，重复执行也不会二次扣
    COMMIT;
    ```
    > 这样如果第二次收到同一事件，`INSERT processed_events` 会**冲突**，直接回滚，业务不会再落一次。
3. **幂等写法**
   - **条件更新**：`UPDATE ... WHERE qty>=:n`；
   - **UPSERT**：`INSERT ... ON DUPLICATE KEY UPDATE ...`（比如“首次创建订单，重复则更新状态/时间”）；
   - **版本检查**：`WHERE version = :old` 成功后 `version=version+1`，重复事件因版本不匹配**零影响**。
4. **失败补偿**
   - **指数退避 + 尝试上限**：消费失败 `attempts++`，超过阈值（如 10 次）投递到 **DLQ/停车场**；
   - **人工/自动重放**：DLQ 可按 `eventId` 批量重放；
   - **顺序性**：按 `aggregateId` 做**分区键/FIFO**，单聚合同一消费者串行处理，避免乱序导致的版本冲突。

在凡新：支付回调/库存事件会**重复 3–5 次**，我们用 `processed_events` 做硬去重，外层再加 Redis TTL 去重，**99% 的重复在入口被挡**；在麦克尔斯：作品索引更新走 Kafka，消费者先插 `processed_events`，再写 ES，重复消息直接被忽略，**不会重复建索引**。”

追问 1（Redis 被逐出或丢失怎么办）

**面试官：**“如果 Redis 因为内存淘汰把 `de:{eventId}` 清掉了，会不会又重复处理？”

**你：**

“不会，因为**DB 层还有硬约束**。Redis 只是**快速挡**；真正的幂等保证靠 `processed_events` 主键或**业务唯一约束**（例如 `user_id+coupon_id` 唯一）。即使 Redis 丢了，DB 也会拒绝重复写。”

追问 2（顺序与并发）

**面试官：**“同一 `orderId` 的事件要按顺序消费，怎么做？”

**你：**

“把 `orderId` 当作**分区键**（Kafka partition / SQS messageGroupId），确保同聚合到同一队列分区，由**单个消费者串行**处理。确需并发就用**乐观版本**：版本不匹配的更新返回 0 行，进入**重试/停靠**。”

追问 3（真实事故）

**面试官：**“讲个你们因为没做幂等导致的事故。”

**你：**

“早期库存扣减没做条件更新，重复消息会**二次扣**。修复后改成‘`processed_events` PK 去重 + `UPDATE … WHERE qty>=`’，重复消息变成幂等重放，工单直接清零。”

### 重试策略与“重试预算”：退避 + 抖动 + 限额；与幂等/熔断的协同

> **重试策略**：仅对可重试错误（5xx/超时/429）+ **幂等写**启用；**指数退避 + 抖动**，并以**重试预算**（≈≤10%）限额；与**熔断/限流/舱壁**协同——熔断开启时停止重试，仅半开探测；前端/ BFF 做**请求合并**与 `Retry-After` 遵循。

**面试官**

“在（深圳市凡新科技 / 麦克尔斯深圳）的大促或第三方网关抖动时，接口偶发超时/503/429。你怎么设计**重试策略**，既提升成功率，又不把依赖打穿？重试和**幂等**、**熔断**怎么配合？”

**你：**

“我把重试做成一个**受控系统**：**只在明确可重试的场景**触发，采用**指数退避 + 抖动**，并受‘**重试预算（Retry Budget）**’约束；同时和**幂等键**、**熔断器**联动，避免雪崩。

1. **什么时候重试 / 不重试**
   - **可重试**：`5xx`、网络异常、**超时**、`429`（配合 `Retry-After`）。
   - **不重试**：`4xx` 业务错误（如验证失败、配额不足）、幂等条件不满足。
   - **写请求**只有在**具备幂等**（`Idempotency-Key` 或“条件更新/版本检测”）时才允许重试。
2. **退避策略（带抖动）**
   - 我常用序列：`200ms → 500ms → 1.2s → 2.5s → 5s`（上限 5 次），**全抖动**或**去相关抖动**避免齐步重试。
   - **请求总截止时间（Deadline）** 优先于次数，前端/交互链路一般 **2–3s** 就该止损反馈。
3. **重试预算（核心）**
   - 定义：单位时间内，**重试数 ≤ min(10% × 成功请求数 + 常数保底, 上限)**。
   - 落地：在 BFF 或客户端维护**预算计数器**；预算用尽，后续**直接快速失败**（或返回缓存/降级），避免**重试风暴**。
   - 观测：暴露 `retry_allowed / retry_exhausted / retry_after_honored` 指标。
4. **与幂等/熔断的协同**
   - **幂等**：写链路统一携带 `Idempotency-Key`；服务端“**原子占位 + 响应快照**”确保重复请求**无副作用**。
   - **熔断**：错误/超时率高到阈值（如 50% 且 QPS≥N）→ **打开熔断**；**熔断打开期间**不再重试（或者仅做**极少探测**），等**半开**再小流量尝试。
   - **限流**：命中 `429` 时尊重 `Retry-After`，并把下一次退避与其对齐；服务端返回**明确可重试/不可重试**信号。
   - **并发舱壁（Bulkhead）**：对外呼并发设上限，防止重试把线程池塞满。
5. **凡新/麦克尔斯的落地口径**
   - 凡新：支付网关抖动时，BFF 只对携带 `Idempotency-Key` 的写请求做**最多 4 次**退避重试，并将“重试预算”设为每分钟 **≤10%**。熔断打开后，直接 **快速失败 + 降级**（受理中/稍后查询）。
   - 麦克尔斯：图片/索引服务偶发慢，客户端把超时从 3s 降到 **1.5s** 并加退避重试；服务端接入 Resilience4j 的 **Retry + TimeLimiter + CircuitBreaker** 组合，`Retry-After` 生效、预算消耗在仪表盘可见。

追问 1（工程细节：Java/Resilience4j）

**面试官：**“你会怎么在 Spring/Resilience4j 里配置？”
**你：**

```java
RetryConfig retry = RetryConfig.custom()
    .maxAttempts(5)                                // 包含首次调用
    .waitDuration(Duration.ofMillis(200))         // 基准退避
    .intervalFunction(IntervalFunction.ofExponentialBackoff(200, 2.0)) // 指数退避
    .retryOnException(ex -> isRetryable(ex))      // 只对可重试异常
    .build();
// 可替换为带随机抖动的 IntervalFunction
TimeLimiterConfig tl = TimeLimiterConfig.custom()
    .timeoutDuration(Duration.ofMillis(1500))     // 单次调用硬超时
    .build();
CircuitBreakerConfig cb = CircuitBreakerConfig.ofDefaults(); // 结合错误率阈值
```

> 额外：在拦截器中维护**重试预算**计数；若预算不足，**不进入重试装饰器**直接返回。

追问 2（请求合并与去抖）

**面试官：**“同一用户 1 秒内点了 3 次提交，怎么避免 3×N 次重试？”

**你：**

“BFF 做**去抖 + 合并**：对同一幂等键的并发请求只保留一个下行，其余‘排队等结果/直接复用响应’。这样重试也只发生在**一条请求**上。”

追问 3（真实复盘）

**面试官：**“讲个因为重试策略不当导致放大的案例。”

**你：**

“早期凡新有一段时间对 503 做**固定间隔**重试，且**无预算**，在第三方 2–3 分钟抖动时把线程池打满。复盘后改为**指数退避 + 抖动 + 10% 预算**，同时把熔断半开探测降到每秒**单次**，问题消失；成功率反而提高，因为我们不再挤占自己资源。”

### DLQ / 停车场与人工处置：可观察、可回放、可审计

> **DLQ/停车场**：重试上限或不可重试错误**停靠**，指标与告警可见；支持**筛选/批量回放**，回放走**慢车道 + 令牌桶**；消息记录 `traceId/eventId/aggregateId/error_code/attempts` 便于审计；回放幂等、设黑名单与冷却期，避免“无限回环”。

**面试官**

“真实生产里，总会有**毒性消息**（数据缺字段、顺序打乱、对方幂等键冲突）怎么也处理不了。你在（深圳市凡新科技 / 麦克尔斯深圳）如何设计 **DLQ（死信队列）/Parking Lot（停车场）**，做到**不阻塞主线**、**可定位**、**可重放**、**可审计**？”

**你：**

“我的原则是：**主线轻装前进，异常集中停靠**。落地分四件事：**准入到 DLQ 的规则**、**可观察与告警**、**可重放机制**、**审计与防二次伤害**。”

1. **什么时候进 DLQ / 停车场**
   - **达到重试上限**（比如 10 次指数退避仍失败）或命中特定错误（不可重试类，如 schema 不兼容、幂等冲突）。
   - **顺序受损**：同一 `aggregateId` 新版本已被处理、旧版本再来 → 直接停靠。
   - 我们把消息打上**失败原因 code**（`VALIDATION_ERROR / VERSION_CONFLICT / UPSTREAM_4XX`）和**最近一次异常栈摘要**，方便后续归因。
2. **可观察（第一时间定位）**
   - 指标：`dlq_size、dlq_in_rate、top_error_code、message_age_p95、redrive_success_rate`。
   - 日志 / APM：每条 DLQ 消息带 `traceId`、`eventId`、`aggregateId`，能一键跳到当时的业务日志/链路。
   - 告警：`dlq_in_rate > 基线` 或 `age_p95 > 阈值` 告警到 on-call；并附上**样本消息链接**。
3. **可重放（安全回放）**
   - **按钮式回放**：控制台支持按 `eventId` / `aggregateId` / 时间窗口**筛选 + 批量 redrive**。
   - **回放到哪**：
     - 修复了数据后，回放到**原主队列**；
     - 依赖仍不稳时，回放到**隔离的“慢车道”队列**，有更严格的速率/并发。
   - **回放幂等**：消费者已经是**幂等**，所以即便重复也不会二次扣减/二次下单。
   - **节流**：redrive 受**令牌桶**保护（例如每秒最多 50 条），避免回放本身造成**二次冲击**。
4. **审计与防二次伤害**
   - DLQ 里保留**处理历史**：`first_seen / last_attempt_at / attempts / last_error_code / operator / redrive_at`。
   - **红/黑名单**：某些 `aggregateId` 连续失败 3 次以上，进入**黑名单**，临时不再回放，等待数据修复。
   - **保留策略**：保留 7–30 天，超期自动归档到对象存储（便于审计与离线排查）。

**凡新**这边：订单出库事件偶发因**上游字段缺失**失败，我们把它们停靠到 DLQ，填补字段后**按聚合 ID 批量回放**，有速率上限，不影响主线。

**麦克尔斯**那边：作品索引更新在 ES 集群滚更时会失败，我们让 DLQ 回放走**慢车道**消费者，等 ES 恢复就自然清空；整个过程 on-call 看到 `dlq_size`、`age`、`redrive_success_rate` 一目了然。

关键表/队列与接口（示意）

```sql
-- 停车场记录（可用 DB，也可映射 DLQ 元数据到 DB 里便于查询）
CREATE TABLE dlq_events (
  event_id      CHAR(36) PRIMARY KEY,
  aggregate_id  CHAR(36),
  error_code    VARCHAR(64),
  last_error    TEXT,
  attempts      INT,
  first_seen    DATETIME,
  last_attempt  DATETIME,
  last_operator VARCHAR(64),    -- 谁点的回放/忽略
  status        ENUM('PENDING','REDRIVEN','IGNORED') DEFAULT 'PENDING'
);
```

```text
POST /ops/dlq/redrive?eventId=...|aggregateId=...|from=...&to=...
- 校验：黑名单/白名单；幂等：二次点击不重复回放
- 限速：令牌桶 N/sec
```

**队列侧实践**

- **Kafka**：DLQ 用独立主题 `topic.DLQ`，消息里保留 `headers`（`traceId、error_code、attempts`）；回放时把 `original_topic/partition/offset` 也带上便于追溯。
- **SQS**：配置 `redrive policy`（`maxReceiveCount` 达到即进 DLQ），另建一个**人工/批量 redrive**的 Lambda/Job；需要顺序时用 **FIFO + messageGroupId** 的“慢车道”回放。

追问 1（为什么要“停车场”而不是只用 DLQ？）

**你：**

“DLQ 是‘**被动**死信’，停车场是‘**主动**停靠’：当我们识别为**数据质量问题**或**顺序冲突**时，**不等重试上限**就直接停靠，避免无意义的打桩刷屏；修复后再**人工确认**回放，风险更可控。”

追问 2（如何避免“无限回放→再失败”的回环）

**你：**

“回放有**次数上限**与**冷却期**：同一 `eventId` 失败≥N 次后标记 `IGNORED`，需**人工解除**；同时在回放通道加**速率与并发上限**。另外我们会在回放前跑**干跑验证（dry-run）**，例如先校验 payload/schema 与依赖状态。”

追问 3（真实复盘）

**你：**

“凡新一次大促，支付回执 Schema 升级，部分字段名变化导致消费者报 `VALIDATION_ERROR`；我们 5 分钟内把错误集中到 DLQ、修正映射、**分批回放**，排队清空且用户侧无感知。
麦克尔斯那边，ES 扩容期间出现 `UPSTREAM_5XX`，我们把回放切到**慢车道**并降低速率，指标稳定后再全量回放。”

### 顺序性与分区键：按“聚合维度”保序，吞吐与热点的权衡

> **顺序与分区**：用 `aggregateId` 做分区键（同聚合同分区）实现**局部有序**；消费者维护 `last_version`，重复/过期丢弃，缺口停靠；热点聚合接受串行或拆流/分片；不追求全局 FIFO；跨聚合因果用 `version/causal` 描述并在源头收敛。

**面试官**

“订单→库存→出库通知这一链路里，**同一订单**的事件必须按顺序处理，但全局不需要严格顺序。你在（深圳市凡新科技 / 麦克尔斯深圳）怎么设计**分区键/队列模型**来既保证**局部有序**又能**水平扩展**？遇到**热点聚合**时怎么破？”

**你：**

“我坚持**按聚合（Aggregate）保序**，按整体吞吐做分区扩展：

- **分区键** = `aggregateId`（如 `orderId`）。Kafka 就是 `partition = hash(orderId) % N`；SQS/FIFO 就是 `messageGroupId = orderId`。这样**同一订单的事件在同一分区**，**单消费者串行**，天然有序；不同订单可并行处理、横向扩展靠**分区数 N**。
- **顺序语义不跨聚合**：我们不追求全局顺序，避免吞吐被锁死。
- **幂等 + 版本**兜底：每条事件带 `aggregateVersion`（或 `seq`）。消费者维护 `last_version(orderId)`：
  - `version <= last` → **重复/过期**，直接忽略（幂等）；
  - `version == last + 1` → 正常处理并 `last = version`；
  - `version > last + 1` → **发现缺口**（乱序/丢消息），**停靠到停车场**或**短暂缓存等待**后再处理。
    这套在凡新和麦克尔斯都实践过：Kafka/SQS 提供**分区内有序**，版本字段让我们在极端情况下**检测/修复顺序**。”

**消费端事务示意（确保“处理 + 推进版本”一致）**

```sql
BEGIN;
  -- 去重/保序：每个聚合维护一个版本表
  SELECT last_version FROM agg_progress WHERE aggregate_id=:aid FOR UPDATE;
  -- 若无记录先插入 last_version = 0
  CASE
    WHEN :version <= last_version THEN ROLLBACK; -- 重复/过期，忽略
    WHEN :version = last_version + 1 THEN
      -- 执行业务落库（幂等写法/条件更新）
      UPDATE ...;
      UPDATE agg_progress SET last_version=:version WHERE aggregate_id=:aid;
      COMMIT;
    ELSE
      -- version 跳跃：缺口，停靠到 DLQ/停车场，稍后回放
      ROLLBACK;
  END CASE;
```

**热点聚合（Hot Key）处理**

- **接受串行**：如果某个 `orderId` 本来就要求严格顺序，就接受它在单分区**串行**，吞吐上限 = 单消费者能力。
- **降热/分片聚合键**：如果热点来自**商品/库存**这类“可拆分”的聚合，就把分区键改成 `hash(skuId, bucket)`，在消费端做**按 sku 聚合后的有序合并**（只在确需极高吞吐时用，复杂度高）。
- **拆流**：把“强保序”的事件（订单状态流）与“弱保序”的事件（库存读模型刷新）分不同主题/队列，避免强约束拖累整体吞吐。
- **限速与背压**：热点聚合触发**令牌桶**，必要时降级为“受理中”。

**为什么不用“全局 FIFO/单分区”**

- 全局 FIFO 会把吞吐卡死在**单消费者**；重新分区/扩分区又会引入复杂迁移。**按聚合保序**是大多数电商链路的工程平衡点。

**真实落地**

- 在凡新，`OrderCreated/OrderPaid/OrderShipped` 以 **`orderId` 为分区键**；库存相关的 `StockReserved/Released` 也跟订单同分区，所以**订单内严格按序**。碰到活动单品成热点，我们把**读模型刷新**拆到独立主题，写链路保持串行稳定。
- 在麦克尔斯，作品发布事件用 **Kafka 分区** + `workId`，搜索索引消费者用“**版本推进**”表防乱序；偶发跳序进入**停车场**，修复后批量回放即可。

追问 1（跨聚合的“因果”如何表达）

**面试官：**“库存事件要落后于订单创建，你怎么表达这种因果？”

**你：**

“事件里带 `causal` 字段（如 `orderVersion`），消费者可在发现依赖未就绪时**短暂停靠**或**重试**；更稳妥是由**同一服务**在本地事务里产生‘订单创建’与‘库存预留请求’（Outbox），从源头保障时序。”

追问 2（扩分区与重平衡的影响）

**面试官：**“Kafka 扩分区后哈希变化，顺序还在吗？”

**你：**

“**分区内顺序仍成立**，但**键到分区的映射可能变化**，导致同一键后续去到新分区。为避免扰动，**提前估算足够的分区数**；或用**一致性哈希**/自定义分区器在扩容时减少抖动。”

追问 3（Exactly-once 的取舍）

**面试官：**“你们追求 exactly-once 吗？”

**你：**

“我们更偏向‘**几乎一次（effectively-once）**’：生产端至少一次 + **幂等消费者 + 版本推进**。Kafka 的 EOS（事务性生产/消费）对堆栈/运维要求更高，只有在**单主题内计算**且强一致计费等场景才考虑。”

### Exactly-once 的工程化取舍：追求 “effectively-once” 而非执念 EOS

> **Exactly-once 取舍**：端到端 EO 成本高；工程上以 **Outbox + 至少一次传输 + 幂等消费（去重键/条件更新/UPSERT/版本推进）+ DLQ 回放** 达到 **effectively-once**；Kafka EOS 仅限**拓扑内**且无外部副作用场景，涉及外部系统仍靠幂等落地。

**面试官**

“你在（深圳市凡新科技 / 麦克尔斯深圳）做‘下单→扣库存→出库通知→索引刷新’这条链路时，怎么保证**不多扣、不漏扣**？你会不会上**Exactly-once**？如果不用，怎么做到 **几乎一次（effectively-once）** 的工程效果？”

**你：**

“我的原则是：**跨系统端到端的 Exactly-once 很难、代价高**；我们在生产里更推崇 **‘至少一次 + 幂等 + 去重 + 版本推进 = effectively-once’**。

- **源头**（订单服务）用 **Outbox**：数据变更与事件同库同事务落地，**保证不丢**。
- **传输**（Kafka/SQS）默认**至少一次**，接受**可能重复**。
- **落地**（库存/搜索）把**幂等**做到数据模型里：
  - **去重键**：`eventId` 或 `aggregateId+version`；
  - **条件更新 / UPSERT**：`UPDATE stock SET qty=qty-:n WHERE sku=:sku AND qty>=:n`；
  - **版本推进**：维护 `last_version(aggregateId)`，只接收 `= last+1`；小于等于是重放，**零影响**；大于则**停靠/缓冲**处理顺序。
- **失败与顺序异常**进 **DLQ/停车场**，修复后**限速回放**。
  这套在凡新的订单库存、以及麦克尔斯的作品索引链路都跑得很稳：**不靠分布式 2PC**，靠**局部原子 + 全链路幂等**把效果做到位。”

**那 Kafka 的 EOS（事务性生产/消费）要不要用？**

“**只在局部计算拓扑内**（Kafka→Kafka 的流式作业、无外部 DB 副作用），并且**团队熟练/运维可控**时才考虑：比如用 idempotent producer + transactional producer/consumer 保证 **‘每条消息要么处理一次并写入目标主题，要么不处理’**。

一旦涉及 **外部系统（数据库/ES/第三方）**，仍然回到 **幂等写 + 版本推进**。EOS 会带来**事务协调开销、故障恢复复杂度**，不适合所有链路。”

**如何验证你真的达到了 effectively-once？**

“我做两个验证：

1. **对账/幂等监控**：落库端统计 `duplicate_drop_rate`、`version_gap_count`；库存余额做**定时对账**（DB 与事件总量核对）。
2. **混沌/重放演练**：把一批事件**重复投递**、**乱序投递**、甚至**模拟丢失后回放**，确认业务端只产生**一次**净效应且可收敛。”

**真实取舍例子**

“凡新：我们评估过‘订单→库存’用分布式事务，复杂度和脆弱点太多，最终选 **Outbox + 幂等消费**，重复率有但**净效果只一次**；

麦克尔斯：搜索索引刷新不要求强事务一致，就用 **at-least-once + 幂等 upsert**；需要‘感知因果’时靠 `aggregateVersion` 和‘版本推进表’保证**先后关系**。”

**小片段（消费端事务伪码）**

```sql
BEGIN;
  -- 去重 + 保序
  INSERT INTO processed_events(event_id) VALUES(:eid);  -- 若重复则失败回滚
  SELECT last_version FROM agg_progress WHERE id=:aid FOR UPDATE;
  IF :ver = last_version + 1 THEN
      -- 业务写，幂等/条件更新
      UPDATE stock SET qty = qty - :n WHERE sku=:sku AND qty >= :n;
      UPDATE agg_progress SET last_version=:ver WHERE id=:aid;
      COMMIT;
  ELSEIF :ver <= last_version THEN ROLLBACK; -- 重放，忽略
  ELSE ROLLBACK; -- 发现缺口，停靠等待
END;
```

---

## Java Concurrency

### 内存模型 & 可见性：happens-before / `volatile` 的边界

> **可见性 vs 原子性**：`volatile` 保障**可见/有序**不保障**复合原子**；计数/聚合用 `LongAdder/Atomic*` 或串行队列；**安全发布**用“不可变对象 + volatile 引用”；DCL 单例需 `volatile`；跨线程顺序靠 `start/join`、`synchronized unlock/lock`、`volatile write/read` 的 happens-before。

**面试官：**

“你们在（深圳市凡新科技 / 麦克尔斯深圳）有个**库存曝光服务**：多个线程持续累加访问量并在 1s 定时刷到 Redis。偶发地，页面 PV/库存快照会**落后**或**计数丢失**。请你解释 Java 内存模型里的 **happens-before** 关系、`volatile` 能/不能解决什么，并给出代码级修正。”

**你：**

“我先分两件事：**可见性/有序性** 和 **原子性**。

- `volatile` 只保证**写→读**的可见性和一定的**指令有序**（建立 happens-before：对同一变量 `volatile write` 先于后续线程的 `volatile read`），**不保证复合操作的原子性**。
- 对‘计数丢失’这种++操作，我会用 **原子类**（`AtomicLong`/热点高时用 `LongAdder`）或把更新放进**单线程/串行化**的队列；
- 对‘配置刷新/开关不生效’这种 **发布-订阅/开关切换**，我会用 `volatile` 或 **不可变对象 + volatile 引用**做**安全发布**。”

**反例 & 修正**

```java
// 反例：可见性不可靠 + 原子性缺失
class CounterBad {
  private long pv = 0;                 // 非 volatile，且 ++ 非原子
  void inc() { pv++; }                 // 读-改-写竞争
  long get() { return pv; }            // 可能读到旧值（CPU 缓存未刷回）
}

// 修正 1：高并发计数 —— LongAdder 更抗热点
class CounterGood {
  private final LongAdder pv = new LongAdder();
  void inc() { pv.increment(); }       // 分段计数，聚合时冲突少
  long get() { return pv.sum(); }
}

// 修正 2：配置热更新 —— 不可变对象 + volatile 引用
class Config { final int ttl; final boolean enable; Config(int t, boolean e){ttl=t;enable=e;} }
class ConfHolder {
  private volatile Config cfg = new Config(60, true); // 安全发布
  Config get(){ return cfg; }
  void reload(){ cfg = loadFromDb(); }                // 对所有线程立即可见
}
```

**happens-before 你要说得出的 4 条常用规则**

1. 程序次序规则：同一线程内的代码顺序。
2. **监视器**：对同一锁，`unlock` 先于后续线程的 `lock`（`synchronized`）。
3. **volatile**：对同一变量的 `write` 先于后续线程的 `read`。
4. **线程启动/终止**：`Thread.start()` 先于子线程内操作；子线程内操作先于 `Thread.join()` 返回。

**易错场景 & 取舍**

- **双重检查单例（DCL）**必须把实例引用设为 `volatile`，否则可能看到**半初始化对象**。
- **组合操作**（如 `check-then-act`、`put-if-absent`）用 `ConcurrentHashMap.computeIfAbsent` 或锁/CAS 保护；仅靠 `volatile` 不够。
- **计数/埋点**：热点极高时 `AtomicLong` 会退化自旋，**LongAdder**冲突更小；但**要快照**（求准确值）时用 `sum()`，它与上一刻的 `inc()` 有极小窗口差。
- **有序性**：`volatile` 可阻止相关指令重排，但**不能**代替锁来“临界区互斥”。

**项目实话实说**

“凡新那边促销统计我们一开始用 `AtomicLong`，峰值下自旋热点明显；换成 `LongAdder` 后 CPU 降了不少。Michaels 的开关配置用的是**不可变配置 + volatile 引用**，热更新后前端请求马上生效，不需要重启。”

### `synchronized` vs `ReentrantLock`：可中断 / 定时 / 公平 / 条件队列

> **锁的选择**：短小互斥→`synchronized`；需要**可中断/定时/多条件/可观测**→`ReentrantLock`。公平锁减吞吐；`unlock` 一定放 `finally`；条件队列要**循环检查**；读多写少可上 `ReentrantReadWriteLock`（只允许**降级**）；热点用**锁分段**，长等待用 **tryLock(timeout)+重试/降级**。

**面试官：**
“在（深圳市凡新科技 / 麦克尔斯深圳）的大促高峰，你们有个**库存预留**的热点段：偶发下游抖动时，线程在等待锁期间**堆积**，无法快速取消，导致**线程池被占死**。你会选择 `synchronized` 还是 `ReentrantLock`？为什么？”

**你：**

“我会把选择标准说清楚：

- **简单互斥、临界区短、无需可中断/定时/多条件队列** → `synchronized` 就够，JIT 对偏向/轻量级锁已经很优化了；
- **需要更细的控制**（例如**等待可中断**、**超时放弃**、**多条件变量**、或**可选公平性**）→ 用 `ReentrantLock`（基于 AQS）。库存预留这种‘热点且可能长等待’就该用 `ReentrantLock`，这样**等待线程可取消**，避免把线程池卡死。”

**关键差异**

- **可中断获取**：`lockInterruptibly()` 只有 `ReentrantLock` 支持 —— 等锁时能响应 `Thread.interrupt()`。
- **定时获取**：`tryLock(timeout, unit)` 允许**超时退避**，可与**重试预算**协同；`synchronized` 没法。
- **条件队列**：`newCondition()` 可建多个条件（如 `notEmpty/notFull`），`wait/notify` 只能一个条件队列且易误用。
- **公平性**：`new ReentrantLock(true)` 可公平，但吞吐下降；大多数场景用**非公平**提升吞吐。
- **可观测/灵活性**：`isLocked/hasQueuedThreads/getQueueLength` 等有助于指标化与排障。
- **语义**：两者都**可重入**；`synchronized` 通过监视器，`ReentrantLock` 通过 AQS 队列。

**等待可中断 + 超时退避（库存热点）**

```java
class StockGuard {
  private final ReentrantLock lock = new ReentrantLock(); // 非公平更高吞吐
  boolean runWithLock(Duration d, Runnable task) throws InterruptedException {
    if (lock.tryLock(d.toMillis(), TimeUnit.MILLISECONDS)) { // 定时获取
      try { task.run(); return true; }
      finally { lock.unlock(); }
    }
    return false; // 超时放弃，交给上层重试/降级
  }
}
// 调用处：失败则触发指数退避 + 幂等重试，避免长等待压死线程池
```

**多条件队列（有界缓存/异步出库）**

```java
class BoundedBuffer<T> {
  private final ReentrantLock lock = new ReentrantLock();
  private final Condition notEmpty = lock.newCondition();
  private final Condition notFull  = lock.newCondition();
  private final Deque<T> q = new ArrayDeque<>();
  private final int cap;

  void put(T x) throws InterruptedException {
    lock.lockInterruptibly();
    try {
      while (q.size() == cap) notFull.await(); // 可中断等待
      q.addLast(x);
      notEmpty.signal(); // 唤醒一个取者
    } finally { lock.unlock(); }
  }
  T take() throws InterruptedException {
    lock.lockInterruptibly();
    try {
      while (q.isEmpty()) notEmpty.await();
      T v = q.removeFirst();
      notFull.signal();
      return v;
    } finally { lock.unlock(); }
  }
}
```

**读多写少补充 - 读写锁与降级**

- `ReentrantReadWriteLock`：读多写少可提升吞吐；**可降级**（持有写锁时获取读锁再释放写锁），**不可升级**（先读后写会死锁/饥饿）。
- 极端读场景可考虑 `StampedLock`（乐观读），但 API 更复杂，注意**中断与可重入限制**。

**工程取舍与坑**

- **一定 `unlock()` 放在 `finally`**；`Condition.await()` 需在**持锁**前提下调用，醒来要**循环检查条件（防虚假唤醒）**。
- 公平锁**减少饥饿**但吞吐更低（队列严格 FIFO，缓存局部性变差）。
- **锁粗化**（把多次短锁合并）能省切换，但要警惕临界区过大造成争用；
- **锁分段/分片**（例如用 `stripe = hash(key) % N` 选择不同锁）能摊薄热点；
- **避免双重检查单例未加 `volatile`** 导致半初始化可见；
- `synchronized` 适合**非常短小**且无中断需求的路径（JIT 可内联/消除），否则用 `ReentrantLock` 获得**超时/可中断/多条件**这些工程特性。
- 指标化：导出**活跃线程数、队列长度、等待时长分位、拒绝数**；看到长等待优先**缩小临界区/分段**，其次再调线程数。

**项目口径**

“凡新那边库存预留把热点 SKU 的临界区改成 `ReentrantLock.tryLock(100ms)`，拿不到就**快速失败 + 幂等重试**，线程池再也没被‘长等待’拖死。麦克尔斯那边有个有界队列用两个 `Condition` 做 `notEmpty/notFull`，比 `wait/notify` 可读且不容易误唤醒。”

### `ThreadPoolExecutor` 七参数、队列取舍与拒绝策略

> **线程池 = 背压阀**：**有界队列 + 合理 core/max + CallerRuns/Abort 推回上游**；按**依赖分池**（Bulkhead），外呼**强制超时**，命中拒绝→**降级+退避**；拒绝把 `LinkedBlockingQueue` 用成**无界**；SynchronousQueue 需配强限流；观测 `active/queue/rejected/p95` 做动态调度。

**面试官：**

“大促瞬时高峰涌进来，你们的**下游限速**，结果你这边线程池一路涨、队列越积越多，延迟飙升甚至 OOM。你会怎么**选型与调参**，既**吸收突发**又不把下游打穿？”

**你：**

“我把线程池当成**背压阀**来设计：**有界队列 + 合理的 core/max + 拒绝策略推回上游**，再配超时/重试预算/熔断。”

1 - 七参数怎么选

`ThreadPoolExecutor(core, max, keepAlive, unit, workQueue, threadFactory, handler)`

- **corePoolSize**
  - **CPU 密集**：≈ `CPU核数`（或 `核数 * 1.0`）。
  - **IO 阻塞**：按 `核数 * (1 + 阻塞比)` 估算；例如阻塞≈3 倍计算时长，可把 `max` 提高到 `~ 核数 * 4`。
- **maximumPoolSize**：允许短时**扩张吸收突发**，但要**有限度**，避免把下游压穿。
- **keepAliveTime**：突发型业务把**非核心线程** 30–120s 回收；也可 `allowCoreThreadTimeOut(true)` 让 core 也缩。
- **workQueue（关键）**：强烈建议**有界**，容量体现你的**等待预算**。
- **threadFactory**：起**可读名**（含依赖名/用途），便于诊断；可带 MDC/traceId。
- **handler（拒绝策略）**：用来**施加背压**或**快速失败**，比“无限排队”更健康。

2 - 队列选型（取舍）

- **`ArrayBlockingQueue(cap)`（推荐）**：**有界 FIFO**，简单可控，最符合“背压”。
- **`LinkedBlockingQueue`**：默认**无界**（反模式，可能 OOM）；如用务必**指定上限**。
- **`SynchronousQueue`**：**零容量**直传；适合**低延迟 + 可弹性扩线程**，但**极易打穿下游**，除非有**强兜底**（限流/熔断）。
- **`PriorityBlockingQueue`**：有优先级但**可能饿死**低优先任务；仅在确需优先级时用。

3- 拒绝策略（语义与场景）

- **`CallerRunsPolicy`（首选）**：把任务在**调用线程**执行 → **自然限速**，把压力**传回上游**；适合 BFF/同步链路。
- **`AbortPolicy`**：抛 `RejectedExecutionException` → **快速失败**，让上层走**降级/重试**。
- **`DiscardPolicy/DiscardOldestPolicy`**：静默丢弃/丢最老任务，**不建议**用于关键业务（难排障）。

4 - 反模式 RISK 清单

- **无界队列 + 巨大 max**：看似稳，其实**无限排队 → 高尾延迟/OOM**。
- **一个池干所有事**：CPU 任务与 IO 任务**混用** → 互相拖垮（饥饿/死锁）。
- **阻塞任务塞进 `ForkJoinPool`/默认 `CompletableFuture` 池** → 线程**饥饿**。
- **任务内再 `submit()` 并同步等待**（嵌套提交）→ **线程耗尽**死锁。
- **无限等待的下游调用**（无超时）→ 线程长期占用。

> 修复：**按依赖分池（Bulkhead）**；阻塞任务用**自定义 Executor**；所有外呼**必须有超时**；避免同步等待嵌套。

5 - 一套“突发但要保护下游”的实用模板

```java
int cores = Runtime.getRuntime().availableProcessors();
int queueCap = 2000; // 等待预算：能接受在本层堆多少
ThreadPoolExecutor pool = new ThreadPoolExecutor(
    Math.max(cores, 8),          // core：基础吞吐
    Math.max(cores * 4, 32),     // max：吸收突发，但有限度
    60, TimeUnit.SECONDS,        // 非核心回收
    new ArrayBlockingQueue<>(queueCap), // 有界FIFO = 背压阀
    r -> { Thread t = new Thread(r, "outbound-stock-%d".formatted(r.hashCode())); t.setDaemon(true); return t; },
    new ThreadPoolExecutor.CallerRunsPolicy() // 调用方背压
);
pool.allowCoreThreadTimeOut(true); // 突发过后及时收缩
```

**配套策略**

- 任务里**强制超时**（`TimeLimiter`/`CompletableFuture.orTimeout`）；
- 命中拒绝策略时：返回**清晰错误**或**降级**，并按**重试预算**退避；
- 指标：`active/queue/rejected/p95`，看到**队列持续高位**优先**降入口 RPS/放大下游限额/扩容**，不是“盲目加线程”。

6 - 与限流/重试/熔断的协同

- **限流**：入口**令牌桶**控制进入线程池的速率；`429 + Retry-After` 指导上游退避。
- **重试**：只对**幂等**请求；遵守**预算**，避免把队列堆满。
- **熔断**：下游异常率/超时升高时**打开熔断**，线程池**不再接活**（或改走降级）。
- **Bulkhead（分仓壁）**：为每个关键下游**单独线程池**（或信号量池），互不牵连。

7 - 诊断与观测（上线就要有）

- 导出：`poolSize/activeCount/queueSize/completedTaskCount/rejectedCount`；
- 采样**任务耗时分位**，分依赖/分接口看；
- **线程命名**可读（带依赖名），异常栈里一眼定位；
- `rejectedCount` 异常抬头 = **背压生效**，不是“坏事”，配合**降级开关**即可。

**项目口径**

“凡新我们给每个外呼（支付、库存）各一组**有界池 + CallerRuns**，配上**重试预算**，高峰期把压力稳稳**卡在调用方**，尾延迟大幅收敛；

麦克尔斯把原来单池改成**按依赖分池**，并给 `CompletableFuture` 指定自定义 Executor，解决了**默认池被阻塞任务吃光**的问题。”

### `CompletableFuture` 任务编排：并行、超时、取消与自定义 Executor

> **CF 编排**：总是用**自定义有界线程池**；子任务 `orTimeout/completeOnTimeout + 降级`，总体 `allOf` 加 **deadline**，超时**取消 siblings**；竞速用 `anyOf`；避免在 commonPool 跑阻塞 IO；通过装饰器传递 **MDC/traceId**；别在任务里嵌套阻塞等待。

**面试官：**

“在（深圳市凡新科技 / 麦克尔斯深圳）的下单页，你需要**并行**查询：价格、库存、优惠、地址校验。要求**总体超时 1.2s**，任何子调用超时要**快速降级**，并且**不要把公共线程池卡死**。你怎么用 `CompletableFuture` 编排？”

**你：**

“我有四个原则：**自定义 Executor**、**fail-fast 超时**、**可取消**、**清晰降级**。默认 `commonPool` 是 `ForkJoinPool`，**不能塞阻塞 IO**，所以我总是传入**专用线程池**（按依赖分池）。整体用 `allOf` 聚合，子任务统一 `orTimeout` + `exceptionally` 降级；一旦总体超时或关键任务失败，**取消其余**。”

**代码骨架（并行 + 超时 + 降级 + 取消）**

```java
Executor ioPool = outboundPool("checkout-io"); // 你的自定义、有界线程池

CompletableFuture<Price> fPrice = CompletableFuture
    .supplyAsync(() -> priceApi.get(sku), ioPool)
    .orTimeout(400, TimeUnit.MILLISECONDS)                // 子调用超时
    .exceptionally(e -> Price.fallback());                // 明确降级

CompletableFuture<Stock> fStock = CompletableFuture
    .supplyAsync(() -> stockApi.get(sku), ioPool)
    .orTimeout(300, TimeUnit.MILLISECONDS)
    .exceptionally(e -> Stock.unknown());                 // 可返回未知/保守值

CompletableFuture<Coupon> fCoupon = CompletableFuture
    .supplyAsync(() -> couponApi.check(user), ioPool)
    .orTimeout(300, TimeUnit.MILLISECONDS)
    .exceptionally(e -> Coupon.empty());

CompletableFuture<AddressCheck> fAddr = CompletableFuture
    .supplyAsync(() -> addrApi.validate(addr), ioPool)
    .orTimeout(250, TimeUnit.MILLISECONDS)
    .exceptionally(e -> AddressCheck.skip());

CompletableFuture<Void> all = CompletableFuture.allOf(fPrice, fStock, fCoupon, fAddr);

// 总体超时 + 取消其余
try {
    all.orTimeout(1200, TimeUnit.MILLISECONDS).join();
} catch (CompletionException e) {
    // overall timeout/fail-fast，取消还在跑的子任务（若 API 支持中断/超时，会尽快返回）
    fPrice.cancel(true); fStock.cancel(true); fCoupon.cancel(true); fAddr.cancel(true);
}
// 聚合结果（子任务已各自降级，getNow 不会抛异常）
Result r = new Result(
    fPrice.getNow(Price.fallback()),
    fStock.getNow(Stock.unknown()),
    fCoupon.getNow(Coupon.empty()),
    fAddr.getNow(AddressCheck.skip())
);
```

**常用模式**

- **Fail-fast 聚合**：关键依赖失败就**立即返回**（而不是等其它都完成）。做法：给关键依赖单独 `orTimeout/exceptionally`，并在 `handle/whenComplete` 里触发**取消 siblings**。
- **Fastest-wins（竞速）**：`anyOf(f1, f2, ...)` 选最快结果（如多机房并发读），其余任务在 `thenAccept` 里**best effort cancel**。
- **串并混排**：`thenCompose` 串行依赖（拿到 user 再查优惠）、`thenCombine` 汇合独立分支。
- **超时占位**：`completeOnTimeout(fallback, 300, ms)` —— 超时直接返回**默认值**，不抛异常。

**异常与上下文**

- **不要 `get()`/`join()` 早取**，统一在末端聚合；
- 用 `exceptionally/handle` 明确每个子任务的**降级逻辑**；
- **MDC/traceId 传递**：`CompletableFuture` 不会自动继承 MDC，包一层 `Supplier` 复制/恢复 MDC（或用装饰器 Executor）；
- **中断语义**：取消只是一种信号，确保下游客户端**尊重超时/中断**（HTTP 客户端配置 read/connect timeout），否则线程仍被占。

**反模式（常见坑）**

- 把阻塞 IO 跑在 `ForkJoin commonPool`；
- `LinkedBlockingQueue` 无界池接 CF 任务 → **无限排队**；
- 在任务内部**同步等待另一个 CF**（嵌套 `get()`）→ **线程互等**；
- 没有 `orTimeout/completeOnTimeout`，导致**整体任务无上限**；
- 降级不明确，异常被吞掉，排障困难。

**项目口径**

“凡新那边我们把 checkout 的四个外呼**并行**起来，子任务 `250–400ms` 超时，各自有 fallback；总体 **1.2s deadline** 到就**取消其余**并回前端‘受理中’。麦克尔斯那边把默认 `commonPool` 全部替换成**分池**，p95 立即下降，且**拒绝数**成了清晰的背压信号。”

### 并发诊断与排障：死锁、线程池饱和、阻塞点定位，5 分钟 SOP

> **并发排障 SOP**：指标判型（active/queue/rejected、CPU/GC、依赖 P95）→ 采样线程栈（`Thread.print` 连打 2–3 次）定位 **I/O/锁/CPU** → 当场止血（超时/降级/背压/熔断/收紧并发）→ 根因修复（有界池+超时、分池、加锁顺序、`tryLock`、降对象膨胀、重试预算）。出现 “deadlock” 即统一**锁顺序**或使用超时锁。

**面试官：**

“促销高峰里接口 P95 飙升、线程池不出活。你如何 **5 分钟内** 判断是**CPU 打满**、**下游阻塞**、还是**锁竞争/死锁**，并给出**止血**与**根因**？”

**你：**

“我有一套 **SOP**：**看指标 → 采样线程栈 → 对症止血 → 复盘修复**。”

① 先看指标（1 分钟：判型）

- **线程池四件套**：`activeCount / poolSize / queueSize / rejectedCount`
  - `queueSize` 持续攀升 + `rejectedCount` 抬头 → **池饱和/背压生效**；
  - `active≈max` 且出活慢 → 任务**被阻塞**（I/O、锁）。
- **CPU/GC**：CPU 100% + GC 次数/停顿上升 → **计算/分配压力**或**对象膨胀**（大队列）。
- **依赖红灯**：下游 P95、超时率、熔断状态；若依赖同时抬头，优先判断**下游阻塞**。

> 结论模板：
> - CPU 高 + 线程大多 RUNNABLE → **CPU 绑定/自旋/热点**；
> - CPU 不高 + 队列涨/超时多 → **外部 I/O 或锁等待**。

② 线程栈取证（2 分钟：定位）

**命令**（连打 2–3 次，间隔 5s）：

```
jcmd <pid> Thread.print -l > tdump1.txt
jcmd <pid> Thread.print -l > tdump2.txt
```

**怎么看**

- 大量线程 `TIMED_WAITING on java.net...` / `WAITING on java.util.concurrent.CompletableFuture$Signaller` → **I/O/下游慢**或**无超时**。
- `BLOCKED (on object monitor)` / `parking to wait for <...AQS>` → **锁竞争**；若 dump 顶部出现
  `Found one Java-level deadlock` → **死锁**。
- 线程名可读（建议命名如 `outbound-stock-*`），一眼看出**哪条依赖**卡住。
- **热点类目**：
  - `AbstractQueuedSynchronizer`：`synchronized/ReentrantLock/CountDownLatch` 等等待；
  - `ForkJoinPool.commonPool-worker-*`：阻塞任务误入 **commonPool**；
  - `SynchronousQueue`：直传队列 + 下游慢 ⇒ **堆线程/打穿**。

**补刀**

```
jcmd <pid> GC.class_histogram > histo.txt   # 看大对象/队列膨胀
jfr start duration=60s filename=profile.jfr # 1 分钟 JFR 看锁/IO热点
```

③ 当场止血（1 分钟：控损）

- **强制超时/限速**：临时把外呼客户端 read/connect timeout 降到 300–800ms；尊重 `Retry-After`；将**重试预算**降为 ≤5%。
- **打开降级/熔断**：对“非关键读”直接旧值/占位；关键写**受理中**。
- **收紧并发**：把问题依赖的线程池 **max/queue** 下调，命中 `CallerRuns/Abort` 让**上游减速**（背压）；或直接**临时摘除**问题下游。
- **杀环路**：发现死锁或嵌套等待，**关闭相关入口**（灰度下线）并重启单实例释放锁。

④ 根因与修复（+ 事后规范）

**常见根因 → 修复清单**

1. **无超时/大超时** → 所有外呼**必须**配置超时（请求 deadline 统一 ≤1.5–2s）；`CompletableFuture.orTimeout / TimeLimiter` 落地。
2. **池饱和（无界队列）** → 改为**有界** `ArrayBlockingQueue`；拒绝策略用 `CallerRuns/Abort`；建立**背压指标**。
3. **阻塞任务跑在 commonPool** → 给 CF 指定**自定义 Executor**（按依赖分池）。
4. **锁竞争/死锁** →
   - 固化**加锁顺序**；
   - 长等待改 `tryLock(timeout)+重试/降级`；
   - 缩小临界区/**锁分段**；
   - 用 `ReentrantReadWriteLock`/`LongAdder` 降低写冲突。
5. **对象膨胀/GC**（大队列/大集合） → **控队列上限**、分批处理、减少中间对象。
6. **重试风暴** → **指数退避 + 抖动 + 预算**；熔断打开期间不重试，仅半开探测。

代码小工具（两段，线上非常好用）

**A. 线程池探针（日志 + 指标）**

```java
class InstrumentedExecutor extends ThreadPoolExecutor {
  InstrumentedExecutor(int core, int max, int q) {
    super(core, max, 60, TimeUnit.SECONDS, new ArrayBlockingQueue<>(q),
      r -> new Thread(r, "outbound-stock-" + r.hashCode()),
      new CallerRunsPolicy());
    allowCoreThreadTimeOut(true);
  }
  @Override protected void beforeExecute(Thread t, Runnable r) {
    // 采样记录排队时长/开始时间（可放到 MDC）
  }
  @Override protected void afterExecute(Runnable r, Throwable t) {
    // 上报耗时/异常，若 t!=null 记录
  }
}
```

**B. 锁看门狗（持锁超时报警）**

```java
class TimedLock implements AutoCloseable {
  private final ReentrantLock lock; private final long start=System.nanoTime();
  TimedLock(ReentrantLock l, long warnMs) {
    this.lock = l;
    try { if(!l.tryLock(warnMs, TimeUnit.MILLISECONDS))
            throw new RuntimeException("lock timeout"); }
    catch (InterruptedException e) { Thread.currentThread().interrupt(); }
  }
  public void close() {
    long costMs=(System.nanoTime()-start)/1_000_000;
    if (costMs > 200) log.warn("Lock held {} ms", costMs);
    lock.unlock();
  }
}
// 用法：try (var g=new TimedLock(lock,100)) { /*临界区*/ }
```

死锁“一句话排查”

- 看 `Thread.print` 顶部是否有 “**Found one Java-level deadlock**”；
- 找到两个（或多）线程互相 `BLOCKED`，标出**锁对象**与**获取顺序**；
- **修复**：统一获取顺序；或拆一把锁；或把其中一个改为 `tryLock` + 超时退避。

### 突发流量 + 下游限速，线程池怎么“吸收不作死”？

**面试官：**

“促销 5 分钟峰值打过来，你们要并发调用库存与优惠服务。现在的线程池**队列越积越多**、尾延迟飙升，偶尔还 OOM。请你说说你会怎么设计 `ThreadPoolExecutor`（参数、队列、拒绝策略），以及怎么和**超时/重试/熔断**配合，既吃下突发又不把下游打穿？最好结合你在（深圳市凡新科技 / 麦克尔斯深圳）的经历给一组**可落地**的参数。”

**你：**

“我把线程池当**背压阀**来用，不当仓库。落地分四步：

1. **按依赖分池（Bulkhead）**
   支付、库存、优惠**各一组**线程池，互不拖累；`CompletableFuture` 一律用**自定义池**，不占 `commonPool`。
2. **有界队列 + 适度弹性**
   我选 `ArrayBlockingQueue`，把队列容量当作**等待预算**而不是垃圾桶。凡新那次活动我用过一组参数：
   - `core = max(8, CPU核)`，`max ≈ 核数 * 4`（IO 阻塞型业务）；
   - `queueCap = 2000`（能接受在本层最多囤 2k 个请求，再多就宁可拒）；
   - `keepAlive = 60s`，`allowCoreThreadTimeOut(true)`（峰值后尽快回收）；
   - 队列**坚决不用无界 `LinkedBlockingQueue`**，也不会在这种场景用 `SynchronousQueue`（太容易把下游打穿）。
3. **拒绝策略 = 把压力推回去**
   用 `CallerRunsPolicy` 或 `AbortPolicy`：
   - **CallerRuns**：调用线程自己跑，**自然限速**（BFF 线程被占就降速了）；
   - **Abort**：直接抛异常，BFF 把它转成**清晰的降级/受理中**。
     命中拒绝时我会**记录指标**（`rejectedCount`）并触发**灰度降入口 RPS**。
4. **协同：超时 / 重试预算 / 熔断**
   - 外呼**硬超时**（300–800ms 一档），总体**deadline**（比如 1.2s）；
   - **只对幂等**请求重试，**指数退避 + 抖动**，**预算 ≤10%**；
   - 下游错误/超时率越线**打开熔断**，熔断期**不再重试**，仅半开探测；
   - 命中 `429` 尊重 `Retry-After`，把下一次退避对齐。

**实战口径**

- 在**凡新**，库存外呼池我们配了：`core=12, max=48, queue=2000, CallerRuns`，外呼 HTTP 客户端 `read/connect=500ms`，命中拒绝就**快速失败 + 幂等重试**（预算 10%）。峰值时**rejected**会上来，但尾延迟收敛，下游也没有被打穿。
- 在**麦克尔斯**，图片索引原本用无界队列，直接把 GC 顶爆。改成**有界 + Abort** 后，前端拿到“受理中”占位；等消费链路恢复我们**异步补齐**，整体体验反而更稳。”

**一段可复用代码（骨架）**

```java
int cores = Runtime.getRuntime().availableProcessors();
ThreadPoolExecutor pool = new ThreadPoolExecutor(
    Math.max(cores, 8),
    Math.max(cores * 4, 32),
    60, TimeUnit.SECONDS,
    new ArrayBlockingQueue<>(2000),
    r -> { Thread t = new Thread(r, "outbound-stock-" + r.hashCode()); t.setDaemon(true); return t; },
    new ThreadPoolExecutor.CallerRunsPolicy() // 或 AbortPolicy
);
pool.allowCoreThreadTimeOut(true);
```

**面试官可能追问 & 我会补充**

- **为什么不用无界队列？** → 它把延迟藏起来，最后以 OOM/长尾爆雷。
- **为什么不用 `SynchronousQueue`？** → 直传 + 下游慢会疯狂拉起线程或大量阻塞，风险更大。
- **如何判定“队列该多大”？** → 结合**SLA × 目标吞吐**估算最大在途数；超出就拒绝/降级，而不是堆。
- **监控看什么？** → `active/queue/rejected/p95`；看到队列高位稳定 5 分钟，就**降入口/扩下游**而不是加线程。

### `CompletableFuture` 并行编排要做到：fail-fast + 可取消 + 明确降级

**面试官：**

“下单页要并行查：价格、库存、优惠、地址校验。要求**1.2s 总超时**；任何**关键依赖失败要立刻返回（fail-fast）**；其余子任务要**可取消**；每个子任务都要有**明确降级**。你会怎么写？（结合你在凡新/麦克尔斯的做法说说）”

**你：**

“我的套路是：**自定义有界线程池** + **每个子任务 orTimeout + 降级**，再加一段‘**首个失败即取消其他**’的钩子，整体再套一层**deadline**。”

```java
// 1) 专用有界线程池（别用 commonPool）
Executor ioPool = outboundPool("checkout-io"); // 有界 + CallerRuns/Abort

// 2) 子任务：各自 timeout + 明确降级（fallback）
var fPrice = CompletableFuture.supplyAsync(() -> priceApi.get(sku), ioPool)
    .orTimeout(400, MILLISECONDS)
    .exceptionally(e -> Price.fallback());

var fStock = CompletableFuture.supplyAsync(() -> stockApi.get(sku), ioPool)
    .orTimeout(300, MILLISECONDS)
    .exceptionally(e -> Stock.unknown());

var fCoupon = CompletableFuture.supplyAsync(() -> couponApi.check(user), ioPool)
    .orTimeout(300, MILLISECONDS)
    .exceptionally(e -> Coupon.empty());

var fAddr = CompletableFuture.supplyAsync(() -> addrApi.validate(addr), ioPool)
    .orTimeout(250, MILLISECONDS)
    .exceptionally(e -> AddressCheck.skip());

// 3) fail-fast：任一“关键依赖”异常 → 取消其余（best-effort）
List<CompletableFuture<?>> all = List.of(fPrice, fStock, fCoupon, fAddr);
all.forEach(f -> f.whenComplete((r, ex) -> {
    if (ex != null) all.forEach(o -> { if (o != f) o.cancel(true); });
}));
// 关键依赖你可以只挂在价格/库存上；非关键（比如优惠、地址）失败不触发取消

// 4) 总体 deadline（1.2s）
try {
    CompletableFuture.allOf(fPrice, fStock, fCoupon, fAddr)
        .orTimeout(1200, MILLISECONDS).join();
} catch (CompletionException e) {
    // 总体超时或关键失败：日志打点 + 统一降级响应（受理中/稍后刷新）
    fPrice.cancel(true); fStock.cancel(true); fCoupon.cancel(true); fAddr.cancel(true);
}

// 5) 聚合结果（getNow 避免再抛异常）
var result = new CheckoutView(
    fPrice.getNow(Price.fallback()),
    fStock.getNow(Stock.unknown()),
    fCoupon.getNow(Coupon.empty()),
    fAddr.getNow(AddressCheck.skip())
);
```

**工程要点**

- 在线上，**HTTP 客户端也要配置 connect/read 超时**，不然 CF 取消不了真正的阻塞。
- **关键依赖失败即刻取消**能把平均资源占用降下来，凡新促销时我们就是这么做的；麦克尔斯把图片/地址校验标成**非关键**，失败只降级，不触发 fail-fast。
- 观测：按接口维度暴露 `timeout_count / cancelled_count / fallback_ratio / pool_rejected`，一眼看出是**下游慢**还是**线程池饱和**。

**常见坑 & 一句话纠偏**

- 把阻塞 IO 跑在 `commonPool` → **改自定义有界池**；
- 没 `orTimeout/completeOnTimeout` → **加子任务超时**，避免整体被拖死；
- 在子任务里 `get()` 等另外一个 CF → **用组合算子 (`thenCombine/anyOf`)**，别嵌套阻塞；
- 只取消 CF 不设置客户端超时 → **取消信号传不到下游**；
- 降级不明确 → **每个分支都 `.exceptionally(e -> fallback)`**。

### 锁竞争 / 死锁如何 5 分钟内定位并修复？

**面试官：**

“高峰期你们的下单接口 P95 飙升，线程池 `active≈max` 但出活很慢。线程栈里很多 `BLOCKED`/`parking to wait for <...AQS>`。请你说说**如何快速确认是不是锁竞争/死锁**，以及**当场止血 + 代码层修复**？”

**你：**

“我会按 **SOP** 来：**看指标→打线程栈→当场止血→代码修复**。

**① 判型（1 分钟）**

- 线程池：`active≈max`、`queue↑`、`rejected↑`；CPU 不高（<60%），说明多半是**等待**而不是算力。
- 依赖侧 P95 正常 → 更像**锁竞争**；若依赖也慢，则优先处理下游。

**② 线程栈取证（2 分钟）**

```
jcmd <pid> Thread.print -l > tdump1.txt
jcmd <pid> Thread.print -l > tdump2.txt
```

- 看到大量 `BLOCKED (on object monitor)`（`synchronized`）或 `parking to wait for <...AbstractQueuedSynchronizer>`（`ReentrantLock`），并且 dump 顶部若出现
  `Found one Java-level deadlock` → **确定死锁**。
- 记录**锁对象地址**与**调用栈**，确认是否存在**交叉获取顺序**。

**③ 当场止血（1 分钟）**

- 暂时把热点路径换成 **`tryLock(timeout)` + 快速降级/重试**，避免长时间占用线程；
- **缩小临界区**（把远程调用/IO 移出锁内）；
- 对热点 key 做**分段锁**（`stripe = hash(key)%N`），立刻摊薄争用；
- 如果确认死锁且可灰度，**摘流/重启**单实例解除僵持，同时拉低入口 RPS。

**④ 代码修复（复盘落地）**

- **统一加锁顺序**（最根本）：所有线程都按同一顺序获取 `lockA → lockB`；
- **避免嵌套等待**：临界区只做数据结构操作；
- **可中断/超时**：用 `ReentrantLock.lockInterruptibly()` / `tryLock(t,unit)`；
- 读多写少改 **`ReentrantReadWriteLock`**；计数热点用 **`LongAdder`** 替代全局锁。

**反例（易死锁写法）**

```java
// T1 顺序：A -> B，T2 顺序：B -> A  => 经典死锁
synchronized (lockA) {
  // ...
  synchronized (lockB) { /* ... */ }
}
synchronized (lockB) {
  // ...
  synchronized (lockA) { /* ... */ }
}
```

**修正（统一顺序 / 超时退避）**

```java
// 统一获取顺序：按 hash 比较决定 A/B 的先后
Object first = System.identityHashCode(a) < System.identityHashCode(b) ? a : b;
Object second = first == a ? b : a;
synchronized (first) {
  synchronized (second) { /* 临界区仅做内存操作 */ }
}

// 或改 ReentrantLock + 超时，避免长等待卡死线程池
boolean ok = lock.tryLock(80, TimeUnit.MILLISECONDS);
if (!ok) return fallback(); // 快速降级/重试（具幂等）
try { /* 临界区 */ }
finally { lock.unlock(); }
```

**真实口径（结合经历）**

- **凡新**：活动页库存热 key 竞争，早期把**HTTP 调用放在锁内**导致队列暴涨；修复为**缩小临界区 + tryLock(100ms)**，拿不到就降级成“受理中”，线程池不再被拖死。
- **麦克尔斯**：图片处理链路出现互相持有两把锁的交叉顺序，`Thread.print` 直接报 deadlock；统一锁顺序后问题根除，并加了**锁看门狗**（持锁>200ms报警）。

**一句话总结**

> 判断：`BLOCKED/AQS` + CPU 不高 → 锁问题。止血：`tryLock(timeout)`/缩小临界区/分段锁/降级。修复：**统一锁顺序**、可中断/超时获取、读写分离与热点规避。”

### 线程池饱和 + 重试风暴，如何协同治理？

**面试官：**

“高峰期库存接口开始超时，你们的调用方按老规矩**固定间隔重试 3 次**，结果线程池**队列暴涨**、**拒绝数飙升**，还把下游**彻底打穿**。你在（深圳市凡新科技 / 麦克尔斯深圳）会怎么**同时**处理线程池饱和和重试风暴？”

**你：**

“我把它当一个**闭环控制问题**：线程池充当**背压阀**，重试遵守**预算**，熔断在异常时**切断正反馈**。落地四件事：**限流前置 → 有界池背压 → 重试预算 + 退避抖动 → 熔断/降级**。”

1 - 前置限流（入口滴灌）

- BFF/网关用**令牌桶**控进入下游线程池的速率；命中 `429` 明确返回 `Retry-After`。
- 预算估算：在途上限 ≈ `active + queueCap`，例如池 `max=48`、队列 `2000` → **最多 2048** 在途，网关把**每秒入池速率**限制在 `2048 / SLA秒` 左右。

2 - 线程池 = 背压阀（而不是仓库）

- **有界队列**（`ArrayBlockingQueue`），`CallerRuns` 或 `Abort`。
- 一旦 `queueSize` 高位稳定 ≥ 60s，就**主动降入口 RPS**或**扩容**，而不是“继续加线程”。
- 观测四指标：`active / queue / rejected / p95`。

3 - 重试要有“预算”（≤10%）+ 指数退避 + 抖动

- **只对幂等**请求重试；**固定间隔** → 全改 **指数退避 + 抖动**。
- **重试预算**：单位时间内 `retry_count ≤ 0.1 * success_count + 50`（保底 50）。用**原子计数**实现；**预算耗尽就快速失败**。
- 退避序列：`200ms, 500ms, 1.2s, 2.5s, 5s`，并加 full jitter。

**Java 伪码（带预算 + 抖动 + Retry-After）**

```java
boolean allowed = retryBudget.tryConsume(); // 原子扣减，返回是否还有预算
if (!allowed) throw new RetryBudgetExhausted();

Duration wait = nextBackoffWithJitter(attempt); // 指数退避+抖动
if (resp.is429() && resp.retryAfter() != null) {
  wait = max(wait, resp.retryAfter());          // 与 Retry-After 对齐
}
sleep(wait);
```

4 - 熔断 + 半开探测（切断风暴）

- 错误率/超时率越线（例如**错误率 >50% & 请求数>每秒 N**）→ **打开熔断**；
- 熔断打开期间**停止重试**；只在**半开**状态用少量探针请求；失败再回开路。

5 - 实战口径（我会这样说）

- **凡新**：我们把库存外呼池（`core=12,max=48,queue=2000,CallerRuns`）配了**重试预算 10%**；抖动后的退避 + 熔断后**不重试**，峰值时 `rejected` 上来但尾延迟稳定，下游没有被打穿。
- **麦克尔斯**：图片索引链路把“固定 3 次重试”改成**预算 + 抖动 + Retry-After**，并把失败任务挪到**慢车道队列**；风暴当场止住，回收后再**限速回放**。

6 - 一键止血（值班 SOP）

- 立刻把客户端**read/connect timeout**降至 `300–800ms`；
- 将线程池 `max` 和 `queueCap` **下调**，命中 `CallerRuns/Abort` 把压力推回；
- 开启**降级**（返回旧值/受理中）；
- 打开/收紧**熔断阈值**；
- 查询并临时禁止**固定间隔重试**的调用方（脚本批量下发配置）。

7 - 反模式（必须一口气指出）

- **无界队列 + 固定间隔重试** → 高尾延迟 + OOM + 打穿下游；
- **熔断打开仍在重试** → 负反馈；
- **忽略 `Retry-After`** → 和对方节拍打架；
- **写请求无幂等键还重试** → 双扣/乱序副作用。

**一句话总结**

> **有界池 + 背压（CallerRuns/Abort）** 吸收突发，**重试=预算化+退避抖动**，**熔断**在异常期切断正反馈，入口**令牌桶**稳节奏——四管齐下才能把风暴压成可控波动。
